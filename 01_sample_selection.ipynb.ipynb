{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading EPF database...\n",
      "✓ EPF loaded: 958,410 rows, 26 columns\n",
      "Step 2: Loading NOVA mapping...\n",
      "✓ NOVA mapping loaded: 342 rows\n",
      "Step 3: Merging EPF with NOVA on CCIF codes...\n",
      "✓ Merge completed:\n",
      "  - Total records: 958,410\n",
      "  - With NOVA classification: 900,301\n",
      "  - Match rate: 93.9%\n",
      "  - NOVA distribution:\n",
      "    NOVA 1.0: 325,663 records\n",
      "    NOVA 2.0: 32,523 records\n",
      "    NOVA 3.0: 232,346 records\n",
      "    NOVA 4.0: 309,769 records\n",
      "✓ Merged dataset ready in 'df_merged' variable\n"
     ]
    }
   ],
   "source": [
    "# EPF-NOVA Database Merger - LSE Master's Capstone Project\n",
    "# Simple 3-step process as requested\n",
    "\n",
    "# ============================================================================\n",
    "# 0. LOAD THE LIBRARIES REQUIRED\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# 1. OPEN THE DATABASE OF THE IX HOUSEHOLD BUDGET SURVEY\n",
    "# ============================================================================\n",
    "print(\"Step 1: Loading EPF database...\")\n",
    "df_epf = pd.read_stata(\"./data/base-cantidades-quintilizada-ix-epf-(stata).dta\", \n",
    "                      convert_categoricals=False)\n",
    "print(f\"✓ EPF loaded: {df_epf.shape[0]:,} rows, {df_epf.shape[1]} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. MERGE/JOIN THE NOVA AND COICOP INFORMATION - JOIN WITH CCIF CODE\n",
    "# ============================================================================\n",
    "print(\"Step 2: Loading NOVA mapping...\")\n",
    "df_nova = pd.read_csv(\"./data/ccif_nova.txt\")\n",
    "print(f\"✓ NOVA mapping loaded: {df_nova.shape[0]} rows\")\n",
    "\n",
    "print(\"Step 3: Merging EPF with NOVA on CCIF codes...\")\n",
    "# Clean CCIF codes for consistent matching\n",
    "df_epf['ccif'] = df_epf['ccif'].astype(str).str.strip()\n",
    "df_nova['CCIF'] = df_nova['CCIF'].astype(str).str.strip()\n",
    "\n",
    "# Perform the merge\n",
    "df_merged = df_epf.merge(df_nova, left_on='ccif', right_on='CCIF', how='left', indicator=True)\n",
    "\n",
    "# Show results\n",
    "merge_stats = df_merged['_merge'].value_counts()\n",
    "matched = merge_stats.get('both', 0)\n",
    "total = len(df_merged)\n",
    "match_rate = (matched / total) * 100\n",
    "\n",
    "print(f\"✓ Merge completed:\")\n",
    "print(f\"  - Total records: {total:,}\")\n",
    "print(f\"  - With NOVA classification: {matched:,}\")\n",
    "print(f\"  - Match rate: {match_rate:.1f}%\")\n",
    "\n",
    "# Show NOVA distribution\n",
    "if matched > 0:\n",
    "    nova_dist = df_merged[df_merged['_merge'] == 'both']['NOVA'].value_counts().sort_index()\n",
    "    print(f\"  - NOVA distribution:\")\n",
    "    for nova_level, count in nova_dist.items():\n",
    "        print(f\"    NOVA {nova_level}: {count:,} records\")\n",
    "\n",
    "print(\"✓ Merged dataset ready in 'df_merged' variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking unmatched CCIF codes...\n",
      "Unmatched CCIF codes: 30\n",
      "\n",
      "Top unmatched codes by frequency:\n",
      "--------------------------------------------------------------------------------\n",
      "CCIF Code       | Description                              | Records\n",
      "--------------------------------------------------------------------------------\n",
      "01.4.1.01.01    | GASTOS NO DESGLOSADOS EN ALIMENTOS Y BEBIDAS NO ALCOHÓLICAS |   22072\n",
      "02.3.1.01.01    | CIGARRILLOS Y CIGARROS                   |   11043\n",
      "02.5.1.01.01    | GASTOS NO DESGLOSADOS EN BEBIDAS ALCOHÓLICAS, TABACO Y ESTUPEFACIENTES |    9626\n",
      "02.1.3.01.01    | CERVEZAS CON ALCOHOL                     |    6039\n",
      "02.1.2.01.01    | VINO DE UVAS                             |    4335\n",
      "11.1.1.01.09    | OTRAS BEBIDAS ALCOHÓLICAS FERMENTADAS O DESTILADAS (PURAS O COMBINADAS), ADQUIRIDAS EN RESTAURANTES, CAFÉS Y SIMILARES, CON SERVICIO COMPLETO |     767\n",
      "11.1.1.01.08    | CERVEZAS ADQUIRIDAS EN RESTAURANTES, CAFÉS Y SIMILARES, CON SERVICIO COMPLETO |     611\n",
      "02.1.9.01.01    | CÓCTELES Y CREMAS DE LICOR CON ALCOHOL   |     567\n",
      "02.1.2.01.02    | VINO ESPUMOSO DE UVAS                    |     566\n",
      "02.1.1.01.01    | PISCO                                    |     491\n",
      "02.1.1.01.03    | WHISKY                                   |     319\n",
      "02.1.1.01.99    | OTROS DESTILADOS Y LICORES N.C.P.        |     275\n",
      "02.3.1.09.01    | OTROS PRODUCTOS DE TABACO                |     248\n",
      "02.1.3.01.02    | CERVEZAS CON BAJO CONTENIDO DE ALCOHOL O SIN ALCOHOL |     219\n",
      "11.1.1.02.20    | BEBIDAS ALCOHÓLICAS LISTAS PARA SU CONSUMO, ADQUIRIDAS EN COMERCIO AMBULANTE, CARRITOS DE COMIDA Y SIMILARES |     138\n",
      "\n",
      "✓ Full list saved to: unmatched_ccif_codes.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK CCIF CODES THAT DID NOT MERGE\n",
    "# ============================================================================\n",
    "print(\"\\nChecking unmatched CCIF codes...\")\n",
    "\n",
    "# Get unmatched records\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "unmatched_codes = unmatched[['ccif', 'glosa_ccif']].drop_duplicates().sort_values('ccif')\n",
    "\n",
    "print(f\"Unmatched CCIF codes: {len(unmatched_codes)}\")\n",
    "\n",
    "if len(unmatched_codes) > 0:\n",
    "    # Count frequency of each unmatched code\n",
    "    unmatched_counts = unmatched['ccif'].value_counts()\n",
    "    unmatched_codes['record_count'] = unmatched_codes['ccif'].map(unmatched_counts)\n",
    "    unmatched_codes = unmatched_codes.sort_values('record_count', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop unmatched codes by frequency:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'CCIF Code':<15} | {'Description':<40} | {'Records'}\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in unmatched_codes.head(15).iterrows():\n",
    "        print(f\"{row['ccif']:<15} | {row['glosa_ccif']:<40} | {row['record_count']:>7}\")\n",
    "    \n",
    "    # Save full list\n",
    "    unmatched_codes.to_csv(\"unmatched_ccif_codes.csv\", index=False)\n",
    "    print(f\"\\n✓ Full list saved to: unmatched_ccif_codes.csv\")\n",
    "else:\n",
    "    print(\"✓ All CCIF codes matched successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving merged dataset...\n",
      "✓ Merged dataset saved as: epf_nova_merged_dataset.csv\n",
      "  - File contains 958,410 records\n",
      "  - Ready for stratified sampling and BETO training preparation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE THE MERGED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\nSaving merged dataset...\")\n",
    "output_filename = \"epf_nova_merged_dataset.csv\"\n",
    "df_merged.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "print(f\"✓ Merged dataset saved as: {output_filename}\")\n",
    "print(f\"  - File contains {len(df_merged):,} records\")\n",
    "print(f\"  - Ready for stratified sampling and BETO training preparation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged dataset...\n",
      "✓ Dataset loaded: 958,410 records\n",
      "✓ Records with NOVA: 900,301\n",
      "\n",
      "Quintile column analysis:\n",
      "Using column: quintil\n",
      "Quintile distribution:\n",
      "  I: 206,153\n",
      "  II: 197,591\n",
      "  III: 183,490\n",
      "  IV: 169,107\n",
      "  V: 143,960\n",
      "\n",
      "Confidence distribution:\n",
      "  HIGH: 782,564\n",
      "  LOW: 60,446\n",
      "  MEDIUM: 57,291\n",
      "\n",
      "Starting stratified sampling...\n",
      "\n",
      "Processing Quintil I:\n",
      "  Available records: 206,153\n",
      "  Medium/Low confidence: 19,805\n",
      "  High confidence: 186,348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gr1kzxzs5nj68r2ry0ykrmzc0000gp/T/ipykernel_7215/2894076520.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_conf['token_count'] = high_conf['glosa_ccif'].str.split().str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final sample size: 1000\n",
      "\n",
      "Processing Quintil IV:\n",
      "  Available records: 169,107\n",
      "  Medium/Low confidence: 25,196\n",
      "  High confidence: 143,911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gr1kzxzs5nj68r2ry0ykrmzc0000gp/T/ipykernel_7215/2894076520.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_conf['token_count'] = high_conf['glosa_ccif'].str.split().str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final sample size: 1000\n",
      "\n",
      "Processing Quintil II:\n",
      "  Available records: 197,591\n",
      "  Medium/Low confidence: 20,835\n",
      "  High confidence: 176,756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gr1kzxzs5nj68r2ry0ykrmzc0000gp/T/ipykernel_7215/2894076520.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_conf['token_count'] = high_conf['glosa_ccif'].str.split().str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final sample size: 1000\n",
      "\n",
      "Processing Quintil III:\n",
      "  Available records: 183,490\n",
      "  Medium/Low confidence: 22,055\n",
      "  High confidence: 161,435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gr1kzxzs5nj68r2ry0ykrmzc0000gp/T/ipykernel_7215/2894076520.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_conf['token_count'] = high_conf['glosa_ccif'].str.split().str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final sample size: 1000\n",
      "\n",
      "Processing Quintil V:\n",
      "  Available records: 143,960\n",
      "  Medium/Low confidence: 29,846\n",
      "  High confidence: 114,114\n",
      "  Final sample size: 1000\n",
      "\n",
      "Combining samples...\n",
      "✓ Stratified sample size: 5,000\n",
      "\n",
      "Selecting required variables...\n",
      "✓ Variables selected and renamed:\n",
      "  1. id_gasto\n",
      "  2. folio\n",
      "  3. ccif\n",
      "  4. glosa\n",
      "  5. descripcion (SUPER IMPORTANT - original descriptions for relabeling)\n",
      "  6. NOVA_preliminar\n",
      "  7. confidence\n",
      "  8. quintil\n",
      "\n",
      "Final Sample Statistics:\n",
      "  Total records: 5,000\n",
      "\n",
      "  NOVA Distribution:\n",
      "    NOVA 1.0: 462 (9.2%)\n",
      "    NOVA 2.0: 35 (0.7%)\n",
      "    NOVA 3.0: 1,274 (25.5%)\n",
      "    NOVA 4.0: 3,229 (64.6%)\n",
      "\n",
      "  Confidence Distribution:\n",
      "    MEDIUM: 2,138 (42.8%)\n",
      "    LOW: 2,112 (42.2%)\n",
      "    HIGH: 750 (15.0%)\n",
      "\n",
      "✓ 5,000 sample saved as: beto_training_sample_5000.csv\n",
      "✓ Encoding: UTF-8 with BOM (fixes √â/√ç character issues)\n",
      "✓ Ready for manual NOVA labeling and BETO fine-tuning!\n",
      "\n",
      "Sample descriptions (to verify encoding):\n",
      "  1. HELADO\n",
      "  2. DESAYUNO\n",
      "  3. ALMUERZO, PLATO DE FONDO, LISTO PARA CONSUMIR\n",
      "\n",
      "Next steps:\n",
      "  1. Manual review/correction using 'descripcion' (original descriptions)\n",
      "  2. Use 'glosa' for BETO training (standardized descriptions)\n",
      "  3. Target: Medium/Low confidence items for maximum impact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gr1kzxzs5nj68r2ry0ykrmzc0000gp/T/ipykernel_7215/2894076520.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_conf['token_count'] = high_conf['glosa_ccif'].str.split().str.len()\n"
     ]
    }
   ],
   "source": [
    "# 5,000 STRATIFIED SAMPLE FOR BETO TRAINING\n",
    "# Following your agreed strategy: quintile balance + confidence distribution\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the merged dataset\n",
    "print(\"Loading merged dataset...\")\n",
    "df_merged = pd.read_csv(\"epf_nova_merged_dataset.csv\")\n",
    "print(f\"✓ Dataset loaded: {len(df_merged):,} records\")\n",
    "\n",
    "# Filter to only records with NOVA classification\n",
    "df_nova = df_merged[df_merged['_merge'] == 'both'].copy()\n",
    "print(f\"✓ Records with NOVA: {len(df_nova):,}\")\n",
    "\n",
    "# Check quintile column and its values\n",
    "print(f\"\\nQuintile column analysis:\")\n",
    "if 'quintil' in df_nova.columns:\n",
    "    quintile_col = 'quintil'\n",
    "elif 'quintile' in df_nova.columns:\n",
    "    quintile_col = 'quintile'\n",
    "else:\n",
    "    print(\"Available columns:\")\n",
    "    print(df_nova.columns.tolist())\n",
    "    raise ValueError(\"No quintile column found!\")\n",
    "\n",
    "print(f\"Using column: {quintile_col}\")\n",
    "quintile_values = df_nova[quintile_col].value_counts().sort_index()\n",
    "print(f\"Quintile distribution:\")\n",
    "for q, count in quintile_values.items():\n",
    "    print(f\"  {q}: {count:,}\")\n",
    "\n",
    "# Create confidence categories (HIGH/MEDIUM/LOW based on your mapping)\n",
    "df_nova['confidence_category'] = df_nova['CONFIDENCE'].fillna('UNKNOWN')\n",
    "\n",
    "print(f\"\\nConfidence distribution:\")\n",
    "conf_dist = df_nova['confidence_category'].value_counts()\n",
    "for conf, count in conf_dist.items():\n",
    "    print(f\"  {conf}: {count:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STRATIFIED SAMPLING: 1,000 PER QUINTILE\n",
    "# ============================================================================\n",
    "print(f\"\\nStarting stratified sampling...\")\n",
    "\n",
    "sample_rows = []\n",
    "\n",
    "# Use actual quintile values from the data\n",
    "for quintil in df_nova[quintile_col].unique():\n",
    "    if pd.isna(quintil):\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nProcessing Quintil {quintil}:\")\n",
    "    \n",
    "    quintil_data = df_nova[df_nova[quintile_col] == quintil].copy()\n",
    "    print(f\"  Available records: {len(quintil_data):,}\")\n",
    "    \n",
    "    if len(quintil_data) == 0:\n",
    "        print(f\"  ⚠️ No data for quintil {quintil}\")\n",
    "        continue\n",
    "    \n",
    "    # CONFIDENCE DISTRIBUTION (85% Medium/Low, 15% High)\n",
    "    medium_low = quintil_data[quintil_data['confidence_category'].isin(['MEDIUM', 'LOW'])]\n",
    "    high_conf = quintil_data[quintil_data['confidence_category'] == 'HIGH']\n",
    "    \n",
    "    print(f\"  Medium/Low confidence: {len(medium_low):,}\")\n",
    "    print(f\"  High confidence: {len(high_conf):,}\")\n",
    "    \n",
    "    # Sample 850 from Medium/Low confidence (where manual labeling adds most value)\n",
    "    if len(medium_low) >= 850:\n",
    "        sample_medium_low = medium_low.sample(n=850, random_state=42)\n",
    "    else:\n",
    "        sample_medium_low = medium_low.copy()\n",
    "        print(f\"  ⚠️ Only {len(medium_low)} Medium/Low available, taking all\")\n",
    "    \n",
    "    # Sample 150 from High confidence (reliable training foundation)\n",
    "    if len(high_conf) >= 150:\n",
    "        # Apply selection heuristics for High confidence diversity\n",
    "        high_conf['token_count'] = high_conf['glosa_ccif'].str.split().str.len()\n",
    "        \n",
    "        # Prefer descriptions ≥3 tokens\n",
    "        high_long = high_conf[high_conf['token_count'] >= 3]\n",
    "        high_short = high_conf[high_conf['token_count'] < 3]\n",
    "        \n",
    "        # Try to get diverse high confidence samples\n",
    "        if len(high_long) >= 100:\n",
    "            sample_high_long = high_long.sample(n=100, random_state=42)\n",
    "            remaining_high = 50\n",
    "        else:\n",
    "            sample_high_long = high_long.copy()\n",
    "            remaining_high = 150 - len(high_long)\n",
    "        \n",
    "        if remaining_high > 0 and len(high_short) > 0:\n",
    "            sample_high_short = high_short.sample(n=min(remaining_high, len(high_short)), random_state=42)\n",
    "            sample_high = pd.concat([sample_high_long, sample_high_short])\n",
    "        else:\n",
    "            sample_high = sample_high_long.copy()\n",
    "            \n",
    "    else:\n",
    "        sample_high = high_conf.copy()\n",
    "        print(f\"  ⚠️ Only {len(high_conf)} High confidence available, taking all\")\n",
    "    \n",
    "    # Combine samples for this quintil\n",
    "    quintil_sample = pd.concat([sample_medium_low, sample_high])\n",
    "    \n",
    "    # If we don't have enough, fill from remaining data\n",
    "    target_size = 1000\n",
    "    if len(quintil_sample) < target_size:\n",
    "        remaining_needed = target_size - len(quintil_sample)\n",
    "        already_sampled = set(quintil_sample.index)\n",
    "        remaining_data = quintil_data[~quintil_data.index.isin(already_sampled)]\n",
    "        \n",
    "        if len(remaining_data) >= remaining_needed:\n",
    "            additional_sample = remaining_data.sample(n=remaining_needed, random_state=42)\n",
    "            quintil_sample = pd.concat([quintil_sample, additional_sample])\n",
    "        else:\n",
    "            print(f\"  ⚠️ Only {len(quintil_data)} total records available for quintil {quintil}\")\n",
    "            quintil_sample = quintil_data.sample(n=min(target_size, len(quintil_data)), random_state=42)\n",
    "    \n",
    "    print(f\"  Final sample size: {len(quintil_sample)}\")\n",
    "    \n",
    "    # Add to overall sample\n",
    "    sample_rows.append(quintil_sample)\n",
    "\n",
    "# Combine all quintil samples\n",
    "print(f\"\\nCombining samples...\")\n",
    "\n",
    "if len(sample_rows) == 0:\n",
    "    print(\"❌ No samples collected! Check quintile values and data availability.\")\n",
    "    print(\"Falling back to simple random sampling...\")\n",
    "    \n",
    "    # Simple fallback: random sample of 5000 from available NOVA-classified records\n",
    "    if len(df_nova) >= 5000:\n",
    "        final_sample = df_nova.sample(n=5000, random_state=42)\n",
    "        print(f\"✓ Random sample of 5,000 records selected\")\n",
    "    else:\n",
    "        final_sample = df_nova.copy()\n",
    "        print(f\"✓ Using all {len(final_sample):,} available records\")\n",
    "else:\n",
    "    final_sample = pd.concat(sample_rows, ignore_index=True)\n",
    "    print(f\"✓ Stratified sample size: {len(final_sample):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT AND RENAME REQUIRED VARIABLES\n",
    "# ============================================================================\n",
    "print(f\"\\nSelecting required variables...\")\n",
    "\n",
    "# Select the required variables (including the new ones)\n",
    "required_vars = ['id_gasto', 'folio', 'ccif', 'glosa_ccif', 'establecimiento', 'descripcion_gasto', 'NOVA', 'CONFIDENCE', quintile_col]\n",
    "sample_final = final_sample[required_vars].copy()\n",
    "\n",
    "# Rename variables as requested\n",
    "sample_final = sample_final.rename(columns={\n",
    "    'glosa_ccif': 'glosa',\n",
    "    'descripcion_gasto': 'descripcion',  # SUPER IMPORTANT for relabeling\n",
    "    'NOVA': 'NOVA_preliminar',\n",
    "    'CONFIDENCE': 'confidence',\n",
    "    quintile_col: 'quintil'  # Ensure consistent naming\n",
    "})\n",
    "\n",
    "print(f\"✓ Variables selected and renamed:\")\n",
    "print(f\"  1. id_gasto\")\n",
    "print(f\"  2. folio\") \n",
    "print(f\"  3. ccif\")\n",
    "print(f\"  4. glosa\")\n",
    "print(f\"  5. descripcion (SUPER IMPORTANT - original descriptions for relabeling)\")\n",
    "print(f\"  6. NOVA_preliminar\")\n",
    "print(f\"  7. confidence\")\n",
    "print(f\"  8. quintil\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SAMPLE STATISTICS\n",
    "# ============================================================================\n",
    "print(f\"\\nFinal Sample Statistics:\")\n",
    "print(f\"  Total records: {len(sample_final):,}\")\n",
    "\n",
    "# NOVA distribution\n",
    "nova_dist = sample_final['NOVA_preliminar'].value_counts().sort_index()\n",
    "print(f\"\\n  NOVA Distribution:\")\n",
    "for nova, count in nova_dist.items():\n",
    "    pct = (count/len(sample_final)*100)\n",
    "    print(f\"    NOVA {nova}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Confidence distribution  \n",
    "conf_dist = sample_final['confidence'].value_counts()\n",
    "print(f\"\\n  Confidence Distribution:\")\n",
    "for conf, count in conf_dist.items():\n",
    "    pct = (count/len(sample_final)*100)\n",
    "    print(f\"    {conf}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE SAMPLE FOR BETO TRAINING (WITH PROPER ENCODING)\n",
    "# ============================================================================\n",
    "output_filename = \"beto_training_sample_5000.csv\"\n",
    "sample_final.to_csv(output_filename, index=False, encoding='utf-8-sig')  # UTF-8 with BOM for Excel compatibility\n",
    "\n",
    "print(f\"\\n✓ 5,000 sample saved as: {output_filename}\")\n",
    "print(f\"✓ Encoding: UTF-8 with BOM (fixes √â/√ç character issues)\")\n",
    "print(f\"✓ Ready for manual NOVA labeling and BETO fine-tuning!\")\n",
    "\n",
    "# Show sample of descriptions to verify encoding\n",
    "print(f\"\\nSample descriptions (to verify encoding):\")\n",
    "for i, desc in enumerate(sample_final['descripcion'].dropna().head(3)):\n",
    "    print(f\"  {i+1}. {desc}\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Manual review/correction using 'descripcion' (original descriptions)\")\n",
    "print(f\"  2. Use 'glosa' for BETO training (standardized descriptions)\")\n",
    "print(f\"  3. Target: Medium/Low confidence items for maximum impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: (5000, 9)\n",
      "\n",
      "Quintiles: {'I': 1000, 'II': 1000, 'III': 1000, 'IV': 1000, 'V': 1000}\n",
      "NOVA: {1.0: 462, 2.0: 35, 3.0: 1274, 4.0: 3229}\n",
      "Confidence: {'MEDIUM': 2138, 'LOW': 2112, 'HIGH': 750}\n",
      "\n",
      "NOVA × Confidence by Quintile (%):\n",
      "\n",
      "Quintil I:\n",
      "confidence        HIGH   LOW  MEDIUM\n",
      "NOVA_preliminar                     \n",
      "1.0               75.3  24.7     0.0\n",
      "2.0              100.0   0.0     0.0\n",
      "3.0               12.5  83.1     4.4\n",
      "4.0                6.9  26.4    66.7\n",
      "\n",
      "Quintil II:\n",
      "confidence        HIGH   LOW  MEDIUM\n",
      "NOVA_preliminar                     \n",
      "1.0               83.3  16.7     0.0\n",
      "2.0              100.0   0.0     0.0\n",
      "3.0               13.1  81.5     5.4\n",
      "4.0                6.3  25.2    68.5\n",
      "\n",
      "Quintil III:\n",
      "confidence        HIGH   LOW  MEDIUM\n",
      "NOVA_preliminar                     \n",
      "1.0               72.4  27.6     0.0\n",
      "2.0              100.0   0.0     0.0\n",
      "3.0               18.5  77.2     4.3\n",
      "4.0                5.9  25.1    69.0\n",
      "\n",
      "Quintil IV:\n",
      "confidence        HIGH   LOW  MEDIUM\n",
      "NOVA_preliminar                     \n",
      "1.0               66.7  33.3     0.0\n",
      "2.0              100.0   0.0     0.0\n",
      "3.0               10.8  83.0     6.2\n",
      "4.0                7.9  33.7    58.3\n",
      "\n",
      "Quintil V:\n",
      "confidence        HIGH   LOW  MEDIUM\n",
      "NOVA_preliminar                     \n",
      "1.0               60.2  39.8     0.0\n",
      "2.0              100.0   0.0     0.0\n",
      "3.0                7.1  87.1     5.8\n",
      "4.0                9.4  33.6    57.0\n",
      "\n",
      "Food Categories (CCIF first 2 digits):\n",
      "  01: 2888 (57.8%)\n",
      "  11: 2112 (42.2%)\n",
      "\n",
      "NOVA Distribution by Food Category (%):\n",
      "NOVA_preliminar   1.0  2.0   3.0   4.0\n",
      "food_category                         \n",
      "01               11.3  1.2   7.7  79.8\n",
      "11                6.4  0.0  49.8  43.8\n",
      "\n",
      "Confidence Distribution by Food Category (%):\n",
      "confidence     HIGH    LOW  MEDIUM\n",
      "food_category                     \n",
      "01             26.0    0.0    74.0\n",
      "11              0.0  100.0     0.0\n"
     ]
    }
   ],
   "source": [
    "# ENHANCED SAMPLE EXPLORATION WITH MORE STATISTICS\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"beto_training_sample_5000.csv\", encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Sample: {df.shape}\")\n",
    "\n",
    "# Basic distributions\n",
    "print(f\"\\nQuintiles: {df['quintil'].value_counts().sort_index().to_dict()}\")\n",
    "print(f\"NOVA: {df['NOVA_preliminar'].value_counts().sort_index().to_dict()}\")\n",
    "print(f\"Confidence: {df['confidence'].value_counts().to_dict()}\")\n",
    "\n",
    "# NOVA × Confidence by Quintile\n",
    "print(f\"\\nNOVA × Confidence by Quintile (%):\")\n",
    "for q in ['I', 'II', 'III', 'IV', 'V']:\n",
    "    qdata = df[df['quintil'] == q]\n",
    "    cross = pd.crosstab(qdata['NOVA_preliminar'], qdata['confidence'], normalize='index') * 100\n",
    "    print(f\"\\nQuintil {q}:\")\n",
    "    print(cross.round(1))\n",
    "\n",
    "# Food categories (first 2 digits of CCIF)\n",
    "df['food_category'] = df['ccif'].str[:2]\n",
    "print(f\"\\nFood Categories (CCIF first 2 digits):\")\n",
    "cat_dist = df['food_category'].value_counts()\n",
    "for cat, count in cat_dist.items():\n",
    "    pct = count/len(df)*100\n",
    "    print(f\"  {cat}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# NOVA by Food Category\n",
    "print(f\"\\nNOVA Distribution by Food Category (%):\")\n",
    "nova_by_cat = pd.crosstab(df['food_category'], df['NOVA_preliminar'], normalize='index') * 100\n",
    "print(nova_by_cat.round(1))\n",
    "\n",
    "# Confidence by Food Category\n",
    "print(f\"\\nConfidence Distribution by Food Category (%):\")\n",
    "conf_by_cat = pd.crosstab(df['food_category'], df['confidence'], normalize='index') * 100\n",
    "print(conf_by_cat.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOVA 2 Confidence in Original Data:\n",
      "Total NOVA 2 records: 32,523\n",
      "Confidence distribution:\n",
      "  HIGH: 32,523 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Check NOVA 2 confidence distribution in original merged data\n",
    "import pandas as pd\n",
    "\n",
    "df_merged = pd.read_csv(\"epf_nova_merged_dataset.csv\")\n",
    "df_nova = df_merged[df_merged['_merge'] == 'both']\n",
    "\n",
    "print(\"NOVA 2 Confidence in Original Data:\")\n",
    "nova2_data = df_nova[df_nova['NOVA'] == 2.0]\n",
    "print(f\"Total NOVA 2 records: {len(nova2_data):,}\")\n",
    "\n",
    "if len(nova2_data) > 0:\n",
    "    conf_dist = nova2_data['CONFIDENCE'].value_counts()\n",
    "    print(\"Confidence distribution:\")\n",
    "    for conf, count in conf_dist.items():\n",
    "        pct = count/len(nova2_data)*100\n",
    "        print(f\"  {conf}: {count:,} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"No NOVA 2 records found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGED DATABASE DESCRIPTIVES\n",
      "========================================\n",
      "Total EPF records: 958,410\n",
      "Records with NOVA: 900,301\n",
      "Match rate: 93.9%\n",
      "\n",
      "NOVA Distribution in Chilean Households:\n",
      "  NOVA 1.0: 325,663 (36.2%)\n",
      "  NOVA 2.0: 32,523 (3.6%)\n",
      "  NOVA 3.0: 232,346 (25.8%)\n",
      "  NOVA 4.0: 309,769 (34.4%)\n",
      "\n",
      "Confidence Distribution:\n",
      "  HIGH: 782,564 (86.9%)\n",
      "  LOW: 60,446 (6.7%)\n",
      "  MEDIUM: 57,291 (6.4%)\n",
      "\n",
      "Quintile Distribution:\n",
      "  Quintil I: 206,153 (22.9%)\n",
      "  Quintil II: 197,591 (21.9%)\n",
      "  Quintil III: 183,490 (20.4%)\n",
      "  Quintil IV: 169,107 (18.8%)\n",
      "  Quintil V: 143,960 (16.0%)\n",
      "\n",
      "Food Categories:\n",
      "  Category 01: 839,855 (93.3%)\n",
      "  Category 11: 60,446 (6.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gr1kzxzs5nj68r2ry0ykrmzc0000gp/T/ipykernel_7215/1864969375.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nova['food_category'] = df_nova['ccif'].str[:2]\n"
     ]
    }
   ],
   "source": [
    "# MERGED DATABASE DESCRIPTIVES\n",
    "import pandas as pd\n",
    "\n",
    "df_merged = pd.read_csv(\"epf_nova_merged_dataset.csv\")\n",
    "df_nova = df_merged[df_merged['_merge'] == 'both']\n",
    "\n",
    "print(\"MERGED DATABASE DESCRIPTIVES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Total EPF records: {len(df_merged):,}\")\n",
    "print(f\"Records with NOVA: {len(df_nova):,}\")\n",
    "print(f\"Match rate: {len(df_nova)/len(df_merged)*100:.1f}%\")\n",
    "\n",
    "# NOVA Distribution in Population\n",
    "print(f\"\\nNOVA Distribution in Chilean Households:\")\n",
    "nova_dist = df_nova['NOVA'].value_counts().sort_index()\n",
    "for nova, count in nova_dist.items():\n",
    "    pct = count/len(df_nova)*100\n",
    "    print(f\"  NOVA {nova}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Confidence Distribution in Population  \n",
    "print(f\"\\nConfidence Distribution:\")\n",
    "conf_dist = df_nova['CONFIDENCE'].value_counts()\n",
    "for conf, count in conf_dist.items():\n",
    "    pct = count/len(df_nova)*100\n",
    "    print(f\"  {conf}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Quintile Distribution\n",
    "print(f\"\\nQuintile Distribution:\")\n",
    "quintil_dist = df_nova['quintil'].value_counts().sort_index()\n",
    "for q, count in quintil_dist.items():\n",
    "    pct = count/len(df_nova)*100\n",
    "    print(f\"  Quintil {q}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Food Categories\n",
    "df_nova['food_category'] = df_nova['ccif'].str[:2]\n",
    "print(f\"\\nFood Categories:\")\n",
    "cat_dist = df_nova['food_category'].value_counts()\n",
    "for cat, count in cat_dist.items():\n",
    "    pct = count/len(df_nova)*100\n",
    "    print(f\"  Category {cat}: {count:,} ({pct:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
