{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOW7san3ePB6yLgw8OyTNLR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"dKBZ4-sEt1WX","executionInfo":{"status":"ok","timestamp":1753618240146,"user_tz":-60,"elapsed":22523,"user":{"displayName":"Alfredo Heufemann Peña","userId":"01230962574111998829"}},"outputId":"b9d3878a-cd2a-45f8-a08c-1b53cd38edc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","✓ Cleanup complete!\n","\n","Remaining files:\n","['.config', 'beto-nova-final', 'cv_fold_0', 'cv_fold_4', 'cross_validation_results.png', 'validation_vs_test_performance.png', 'beto-nova-best', 'uncertainty_distribution.png', 'test_set_confusion_matrix.png', 'cv_fold_1', 'validation_confusion_matrix_final.png', 'reliability_diagram.png', 'cv_results.json', 'cv_fold_2', 'cv_fold_3', 'TEST_SET_FINAL_SUMMARY.json', 'sample_data']\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-9e977dec-d21f-459f-8ae7-927e66da3841\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-9e977dec-d21f-459f-8ae7-927e66da3841\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving training_dataset_6000.csv to training_dataset_6000.csv\n"]}],"source":["# CAREFUL: This removes ALL CSV, XLSX, and Parquet files\n","# Clean up ALL data files\n","import os\n","import glob\n","\n","# Remove all CSV, XLSX, and Parquet files\n","for pattern in ['*.csv', '*.xlsx', '*.parquet']:\n","    files = glob.glob(pattern)\n","    for f in files:\n","        print(f\"Removing: {f}\")\n","        os.remove(f)\n","\n","print(\"\\n✓ Cleanup complete!\")\n","\n","# Check what's left\n","print(\"\\nRemaining files:\")\n","print(os.listdir())\n","\n","# Upload training dataset\n","from google.colab import files\n","uploaded = files.upload()  # Upload 'training_dataset_6000.csv'"]},{"cell_type":"code","source":["import torch\n","print(f\"GPU available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0G25qW8uD_F","executionInfo":{"status":"ok","timestamp":1753618240169,"user_tz":-60,"elapsed":5,"user":{"displayName":"Alfredo Heufemann Peña","userId":"01230962574111998829"}},"outputId":"652abfd9-dcca-4020-a4f9-44cde001eb1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU available: True\n","GPU: NVIDIA L4\n"]}]},{"cell_type":"code","source":["# BETO Food Classification Pipeline - WITH CLASSIFICATION MARKERS VERSION\n","# Text preprocessing with domain-specific markers for NOVA classification\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import Counter\n","import re\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# ==============================================================================\n","# DATA LOADING AND VALIDATION FUNCTIONS\n","# ==============================================================================\n","def load_and_validate_data(filepath):\n","    \"\"\"\n","    Load data and perform basic validation checks\n","    \"\"\"\n","    print(f\"Loading data from {filepath}...\")\n","\n","    try:\n","        df = pd.read_csv(filepath)\n","        print(f\"✓ Successfully loaded {len(df)} samples\")\n","    except Exception as e:\n","        print(f\"✗ Error loading file: {e}\")\n","        return None\n","\n","    # Check required columns\n","    required_cols = ['descripcion', 'establecimiento', 'NOVA']\n","    missing_cols = set(required_cols) - set(df.columns)\n","    if missing_cols:\n","        print(f\"✗ Missing required columns: {missing_cols}\")\n","        return None\n","    else:\n","        print(f\"✓ All required columns present\")\n","\n","    # Check for nulls in critical columns\n","    null_counts = df[required_cols].isnull().sum()\n","    if null_counts.any():\n","        print(\"\\n⚠️  Warning: Found null values:\")\n","        print(null_counts[null_counts > 0])\n","\n","    # Validate NOVA values\n","    valid_nova = {0, 1, 2, 3, 4}\n","    valid_nova = {0, 1, 2, 3, 4}\n","    unique_nova = set(df['NOVA'].dropna().unique())\n","    invalid_nova = unique_nova - valid_nova\n","    if invalid_nova:\n","        print(f\"✗ Invalid NOVA values found: {invalid_nova}\")\n","        return None\n","    else:\n","        print(f\"✓ All NOVA values valid: {sorted(unique_nova)}\")\n","\n","    # Check for duplicates based on id_gasto if it exists\n","    if 'id_gasto' in df.columns:\n","        duplicates = df['id_gasto'].duplicated().sum()\n","        if duplicates > 0:\n","            print(f\"⚠️  Warning: Found {duplicates} duplicate id_gasto entries\")\n","\n","    return df\n","\n","# ==============================================================================\n","# TEXT PREPROCESSING FUNCTIONS\n","# ==============================================================================\n","def clean_text(text):\n","    \"\"\"\n","    Basic text cleaning while preserving Spanish characters\n","    \"\"\"\n","    if pd.isna(text):\n","        return \"\"\n","\n","    text = str(text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Remove extra whitespaces\n","    text = ' '.join(text.split())\n","\n","    # Keep letters, numbers, spaces, and common Spanish characters\n","    # Don't remove accents or ñ as they're important in Spanish\n","    text = re.sub(r'[^\\w\\sáéíóúñü\\-\\.]', ' ', text)\n","\n","    # Remove extra spaces again\n","    text = ' '.join(text.split())\n","\n","    return text\n","\n","# ==============================================================================\n","# NOVA CLASSIFICATION MARKERS - CORRECTED VERSION\n","# ==============================================================================\n","def add_nova_markers(descripcion, establecimiento):\n","    \"\"\"\n","    MINIMAL MARKERS - Only the most reliable classification signals\n","    CORRECTED VERSION with proper NOVA 0 handling\n","    \"\"\"\n","    markers = []\n","    desc_clean = clean_text(descripcion)\n","    est_clean = clean_text(establecimiento)\n","\n","# 0. NON-FOOD ITEMS - NOVA 0 marker (ULTRA-CONSERVATIVE + REFINED)\n","# Only mark as NOVA 0 if we're ABSOLUTELY CERTAIN it's not food\n","\n","    # PROPINA - only if it's clearly just the tip\n","    if (desc_clean == 'propina' or\n","      desc_clean.startswith('propina restaurant') or\n","      desc_clean.startswith('propina bar') or\n","      desc_clean.startswith('propina en servicio') or\n","      desc_clean.startswith('propina cafe')) and \\\n","      not any(food in desc_clean for food in ['almuerzo', 'comida', 'sandwich', 'cafe', 'bebida']):\n","      markers.append('[NON_FOOD_NOVA0]')\n","      return markers\n","\n","# EXCLUDE CUOTA if it contains food/meal references\n","    if 'cuota' in desc_clean and not any(food_term in desc_clean for food_term in\n","      ['almuerzo', 'comida', 'pollo', 'carne', 'bebida', 'cuenta compartida', 'menu', 'plato']):\n","      markers.append('[NON_FOOD_NOVA0]')\n","      return markers\n","\n","# Other non-food items - use exact phrases to avoid false positives\n","    nova0_phrases = [\n","    'diferencia ticket',\n","    'diferencia boleta',\n","    'ticket de casino',\n","    'celebracion cumpleaños',\n","    'celebración cumpleaños',\n","    'decoracion fiesta',\n","    'pañales bebe',\n","    'bolsa plastica',\n","    'entrada cine',\n","    'ticket estacionamiento',\n","    'ticket diferencia'\n","    ]\n","\n","    if any(phrase in desc_clean for phrase in nova0_phrases):\n","        markers.append('[NON_FOOD_NOVA0]')\n","        return markers\n","\n","    if any(phrase in desc_clean for phrase in nova0_phrases):\n","        markers.append('[NON_FOOD_NOVA0]')\n","        return markers\n","\n","    # 1. COMPLETO - More specific for hot dog, not \"complete meal\"\n","    if 'completo' in desc_clean and \\\n","       not any(menu_word in desc_clean for menu_word in ['menu', 'plato', 'entrada', 'fondo', 'ensalada']):\n","        markers.append('[COMPLETO_NOVA4]')\n","\n","    # 2. COFFEE WITH MILK - NOVA 4\n","    if any(coffee_milk in desc_clean for coffee_milk in ['cafe cortado', 'cafe con leche', 'cappuccino', 'capuccino', 'latte', 'mocha', 'frappuccino']):\n","        markers.append('[COFFEE_MILK_NOVA4]')\n","\n","    # 3. FAST FOOD CHAINS - NOVA 4 (but exclude simple coffee/tea)\n","    if any(chain in est_clean for chain in ['mc donalds', 'mcdonald', 'kfc', 'burger king', 'subway', 'doggis']) and \\\n","       not any(simple_drink in desc_clean for simple_drink in ['cafe', 'te']):\n","        markers.append('[FAST_FOOD_NOVA4]')\n","\n","\n","  # 4. FRESH BREAD - NOVA 3 marker (FINAL VERSION)\n","    if any(bread in desc_clean for bread in ['marraqueta', 'hallulla', 'pan amasado', 'pan italiano', 'pan frances', 'pan francés', 'pan corriente', 'amasado', 'dobladita', 'dobladitas', 'especial']) and \\\n","      any(venue in est_clean for venue in ['almacen', 'panaderia', 'panadería', 'particular', 'calle', 'cafeteria', 'supermercado', 'comercial', 'negocio', 'lider']) and \\\n","      not any(industrial in desc_clean for industrial in ['hamburguesa','blan','rayado','rallado','panko','molde','multigrano', 'blanco','completo', 'precocida', 'precocido', 'precosidas', 'precosidos', 'pre cocida', 'envasado', 'envasadas', 'molde', 'hot dog', 'bolsa', 'bolsas', 'congelada', 'preparado', 'env', 'prehorneadas']):\n","        markers.append('[FRESH_BREAD_NOVA3]')\n","\n","    # 5. ARTISANAL ICE CREAM - NOVA 3\n","    if 'helado' in desc_clean and 'artesanal' in desc_clean and 'heladeria' in est_clean:\n","      markers.append('[ARTISANAL_NOVA3]')\n","\n","    # 6. TRADITIONAL CHILEAN DRINKS - NOVA 3\n","    if 'mote con huesillo' in desc_clean or 'mote co huesillo' in desc_clean:\n","      markers.append('[TRADITIONAL_DRINK_NOVA3]')\n","\n","    # 7. Restaurant dishes - NOT NOVA 1 (let BERT choose between NOVA 3/4)\n","    if any(dish in desc_clean for dish in ['pollo', 'lomo', 'cerdo', 'vacuno', 'cazuela', 'tallarin', 'tallarines', 'seco', 'arroz', 'erizo']) and \\\n","        any(rest in est_clean for rest in ['restaurant', 'restaurante', 'particular']):\n","        markers.append('[RESTAURANT_DISH]')\n","\n","    # 8. COLACION WITH TRADITIONAL MEAL COMPONENTS - NOVA 3\n","    if 'colacion' in desc_clean and \\\n","      ('plato de fondo' in desc_clean or 'plato principal' in desc_clean or\n","      ('plato + ensalada' in desc_clean)):\n","      markers.append('[COLACION_TRADITIONAL_NOVA3]')\n","\n","    return markers\n","\n","# ==============================================================================\n","# ENHANCED PREPROCESSING WITH MARKERS\n","# ==============================================================================\n","def preprocess_dataframe(df):\n","    \"\"\"\n","    Apply text preprocessing WITH classification markers\n","    Markers help BERT learn NOVA patterns more effectively\n","    \"\"\"\n","    print(\"\\n=== Text Preprocessing with NOVA Classification Markers ===\")\n","\n","    # Create a copy\n","    df_processed = df.copy()\n","\n","    # Clean text fields\n","    print(\"\\nCleaning text fields...\")\n","    df_processed['descripcion_clean'] = df_processed['descripcion'].apply(clean_text)\n","    df_processed['establecimiento_clean'] = df_processed['establecimiento'].apply(clean_text)\n","\n","    # Add classification markers\n","    print(\"Adding NOVA classification markers...\")\n","    df_processed['markers'] = df_processed.apply(\n","        lambda row: add_nova_markers(row['descripcion'], row['establecimiento']),\n","        axis=1\n","    )\n","\n","    # Convert markers to string\n","    df_processed['markers_str'] = df_processed['markers'].apply(lambda x: ' '.join(x) if x else '')\n","\n","    # Create combined text with markers\n","    print(\"Creating combined text with markers...\")\n","    df_processed['combined_text'] = df_processed.apply(\n","        lambda row: f\"{row['markers_str']} {row['descripcion_clean']} [SEP] {row['establecimiento_clean']}\".strip(),\n","        axis=1\n","    )\n","\n","    # Add text statistics for analysis\n","    df_processed['desc_length'] = df_processed['descripcion_clean'].str.len()\n","    df_processed['desc_words'] = df_processed['descripcion_clean'].str.split().str.len()\n","    df_processed['est_words'] = df_processed['establecimiento_clean'].str.split().str.len()\n","    df_processed['marker_count'] = df_processed['markers'].apply(len)\n","    df_processed['total_words'] = df_processed['desc_words'] + df_processed['est_words'] + df_processed['marker_count']\n","\n","    print(\"✓ Text preprocessing with markers complete\")\n","\n","    # Show marker statistics\n","    print(\"\\n=== Marker Statistics ===\")\n","    marker_counts = df_processed['marker_count'].value_counts().sort_index()\n","    print(\"Number of markers per item:\")\n","    for count, freq in marker_counts.items():\n","        print(f\"  {count} markers: {freq} items ({freq/len(df_processed)*100:.1f}%)\")\n","\n","    # Show most common markers\n","    all_markers = []\n","    for markers in df_processed['markers']:\n","        all_markers.extend(markers)\n","\n","    if all_markers:\n","        marker_freq = Counter(all_markers)\n","        print(f\"\\nMost common markers:\")\n","        for marker, count in marker_freq.most_common(10):\n","            print(f\"  {marker}: {count} times\")\n","\n","    # Show preprocessing examples\n","    print(\"\\n=== Preprocessing Examples ===\")\n","    samples = df_processed.sample(min(5, len(df_processed)), random_state=42)\n","    for idx, row in samples.iterrows():\n","        print(f\"\\nOriginal: {df.loc[idx, 'descripcion']} | {df.loc[idx, 'establecimiento']}\")\n","        print(f\"Markers:  {row['markers_str']}\")\n","        print(f\"Final:    {row['combined_text']}\")\n","        print(f\"NOVA:     {int(row['NOVA'])}\")\n","\n","    return df_processed\n","\n","# ==============================================================================\n","# ENHANCED ANALYSIS FUNCTIONS\n","# ==============================================================================\n","def analyze_class_distribution(df):\n","    \"\"\"\n","    Analyze NOVA class distribution\n","    \"\"\"\n","    print(\"\\n=== NOVA Class Distribution ===\")\n","\n","    nova_counts = df['NOVA'].value_counts().sort_index()\n","    print(nova_counts)\n","    print(f\"\\nClass proportions:\")\n","    for nova, count in nova_counts.items():\n","        print(f\"NOVA {int(nova)}: {count} ({count/len(df)*100:.1f}%)\")\n","\n","    # Class imbalance ratio\n","    max_class = nova_counts.max()\n","    min_class = nova_counts.min()\n","    print(f\"\\nClass imbalance ratio: {max_class/min_class:.1f}:1\")\n","\n","    # Visualize\n","    plt.figure(figsize=(10, 6))\n","    ax = nova_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n","    plt.title('NOVA Category Distribution', fontsize=14)\n","    plt.xlabel('NOVA Category', fontsize=12)\n","    plt.ylabel('Count', fontsize=12)\n","    plt.xticks(rotation=0)\n","\n","    # Add value labels on bars\n","    for i, v in enumerate(nova_counts):\n","        ax.text(i, v + 10, str(v), ha='center', va='bottom')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return nova_counts\n","\n","def analyze_markers_by_nova(df):\n","    \"\"\"\n","    Analyze how markers distribute across NOVA categories\n","    Focus on identifying misalignments and potential mislabeling\n","    \"\"\"\n","    print(\"\\n=== Marker Distribution by NOVA Category ===\")\n","\n","    # Get all unique markers\n","    all_markers = set()\n","    for markers in df['markers']:\n","        all_markers.update(markers)\n","\n","    if not all_markers:\n","        print(\"No markers found!\")\n","        return\n","\n","    # Analyze each marker\n","    for marker in sorted(all_markers):\n","        print(f\"\\n{marker}:\")\n","        marker_mask = df['markers'].apply(lambda x: marker in x)\n","        marker_dist = df[marker_mask]['NOVA'].value_counts().sort_index()\n","\n","        total_with_marker = marker_dist.sum()\n","        expected_nova = None\n","\n","        # Determine expected NOVA based on marker name\n","        if 'NOVA1' in marker:\n","            expected_nova = 1\n","        elif 'NOVA2' in marker:\n","            expected_nova = 2\n","        elif 'NOVA3' in marker:\n","            expected_nova = 3\n","        elif 'NOVA4' in marker:\n","            expected_nova = 4\n","\n","        for nova, count in marker_dist.items():\n","            percentage_of_marker = (count / total_with_marker) * 100\n","            is_expected = (nova == expected_nova)\n","            status = \"✓\" if is_expected else \"⚠️\"\n","\n","            print(f\"  {status} NOVA {int(nova)}: {count} items ({percentage_of_marker:.1f}% of this marker)\")\n","\n","        # Show problematic examples\n","        if expected_nova:\n","            wrong_items = df[marker_mask & (df['NOVA'] != expected_nova)]\n","            if len(wrong_items) > 0:\n","                print(f\"  🚨 MISALIGNED ITEMS ({len(wrong_items)} items):\")\n","                for _, row in wrong_items.head(3).iterrows():\n","                    print(f\"    • {row['descripcion']} | {row['establecimiento']} → NOVA {int(row['NOVA'])}\")\n","                if len(wrong_items) > 3:\n","                    print(f\"    ... and {len(wrong_items) - 3} more\")\n","\n","        # Calculate alignment score\n","        if expected_nova and expected_nova in marker_dist:\n","            correct_items = marker_dist[expected_nova]\n","            alignment_score = (correct_items / total_with_marker) * 100\n","            print(f\"  📊 Alignment Score: {alignment_score:.1f}% (higher is better)\")\n","\n","        print()\n","\n","def analyze_marker_quality(df):\n","    \"\"\"\n","    NEW FUNCTION: Analyze marker quality and identify potential issues\n","    \"\"\"\n","    print(\"\\n=== MARKER QUALITY ANALYSIS ===\")\n","\n","    # Get all unique markers\n","    all_markers = set()\n","    for markers in df['markers']:\n","        all_markers.update(markers)\n","\n","    if not all_markers:\n","        print(\"No markers found!\")\n","        return\n","\n","    marker_quality = {}\n","\n","    for marker in sorted(all_markers):\n","        marker_mask = df['markers'].apply(lambda x: marker in x)\n","        marker_data = df[marker_mask]\n","\n","        # Determine expected NOVA\n","        expected_nova = None\n","        if 'NOVA1' in marker:\n","            expected_nova = 1\n","        elif 'NOVA2' in marker:\n","            expected_nova = 2\n","        elif 'NOVA3' in marker:\n","            expected_nova = 3\n","        elif 'NOVA4' in marker:\n","            expected_nova = 4\n","\n","        if expected_nova:\n","            total_items = len(marker_data)\n","            correct_items = len(marker_data[marker_data['NOVA'] == expected_nova])\n","            accuracy = (correct_items / total_items) * 100 if total_items > 0 else 0\n","\n","            marker_quality[marker] = {\n","                'total': total_items,\n","                'correct': correct_items,\n","                'accuracy': accuracy,\n","                'expected_nova': expected_nova\n","            }\n","\n","    # Sort by accuracy (worst first)\n","    sorted_markers = sorted(marker_quality.items(), key=lambda x: x[1]['accuracy'])\n","\n","    print(\"Marker Quality Summary (worst first):\")\n","    print(\"Marker | Total | Correct | Accuracy | Status\")\n","    print(\"-\" * 60)\n","\n","    for marker, stats in sorted_markers:\n","        status = \"🔴 POOR\" if stats['accuracy'] < 80 else \"🟡 OK\" if stats['accuracy'] < 95 else \"🟢 GOOD\"\n","        print(f\"{marker[:30]:<30} | {stats['total']:5d} | {stats['correct']:7d} | {stats['accuracy']:8.1f}% | {status}\")\n","\n","    # Show detailed analysis for problematic markers\n","    print(f\"\\n=== DETAILED ANALYSIS FOR PROBLEMATIC MARKERS ===\")\n","    for marker, stats in sorted_markers:\n","        if stats['accuracy'] < 95:  # Focus on markers with <95% accuracy\n","            print(f\"\\n🔍 {marker} (Accuracy: {stats['accuracy']:.1f}%)\")\n","            marker_mask = df['markers'].apply(lambda x: marker in x)\n","            wrong_items = df[marker_mask & (df['NOVA'] != stats['expected_nova'])]\n","\n","            if len(wrong_items) > 0:\n","                print(f\"  Wrong classifications:\")\n","                for _, row in wrong_items.head(5).iterrows():\n","                    print(f\"    • {row['descripcion']} | {row['establecimiento']} → NOVA {int(row['NOVA'])}\")\n","\n","                # Analyze why it's wrong\n","                nova_dist = wrong_items['NOVA'].value_counts()\n","                print(f\"  Most common wrong classifications:\")\n","                for nova, count in nova_dist.items():\n","                    print(f\"    NOVA {int(nova)}: {count} items\")\n","\n","    return marker_quality\n","\n","def analyze_text_characteristics(df):\n","    \"\"\"\n","    Analyze text length and characteristics by NOVA category\n","    \"\"\"\n","    print(\"\\n=== Text Characteristics Analysis ===\")\n","\n","    # Overall statistics\n","    print(\"\\nOverall text statistics:\")\n","    print(f\"Average description length: {df['desc_length'].mean():.1f} characters\")\n","    print(f\"Average description words: {df['desc_words'].mean():.1f}\")\n","    print(f\"Average establishment words: {df['est_words'].mean():.1f}\")\n","    print(f\"Average markers per item: {df['marker_count'].mean():.1f}\")\n","    print(f\"Average total words: {df['total_words'].mean():.1f}\")\n","\n","    # By NOVA category\n","    print(\"\\n=== Text Characteristics by NOVA Category ===\")\n","    print(\"NOVA | Avg Desc Words | Avg Est Words | Avg Markers | Avg Total Words\")\n","    print(\"-\" * 70)\n","    for nova in sorted(df['NOVA'].unique()):\n","        nova_data = df[df['NOVA'] == nova]\n","        print(f\"  {int(nova)}  |     {nova_data['desc_words'].mean():.1f}      |     {nova_data['est_words'].mean():.1f}      |     {nova_data['marker_count'].mean():.1f}     |      {nova_data['total_words'].mean():.1f}\")\n","\n","def show_nova_examples_with_markers(df, n_examples=2):\n","    \"\"\"\n","    Show example descriptions for each NOVA category with markers\n","    \"\"\"\n","    print(\"\\n=== Sample Descriptions per NOVA Category (with markers) ===\")\n","\n","    for nova in sorted(df['NOVA'].unique()):\n","        print(f\"\\n--- NOVA {int(nova)} Examples ---\")\n","        nova_data = df[df['NOVA'] == nova]\n","        samples = nova_data.sample(min(n_examples, len(nova_data)), random_state=42)\n","\n","        for _, row in samples.iterrows():\n","            print(f\"• Original: {row['descripcion']} | {row['establecimiento']}\")\n","            print(f\"  Markers: {row['markers_str']}\")\n","            print(f\"  Final: {row['combined_text']}\")\n","            print()\n","\n","# ==============================================================================\n","# NEW FUNCTION: DETAILED ERROR ANALYSIS FOR ALL MARKERS\n","# ==============================================================================\n","def check_all_marker_errors(df):\n","    \"\"\"\n","    DETAILED ERROR ANALYSIS - Show ALL misclassified cases for every marker\n","    This function shows the exact items that are causing marker errors\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"COMPLETE MARKER ERROR ANALYSIS - ALL MISCLASSIFIED CASES\")\n","    print(\"=\"*80)\n","\n","    # Get all unique markers\n","    all_markers = set()\n","    for markers in df['markers']:\n","        all_markers.update(markers)\n","\n","    if not all_markers:\n","        print(\"No markers found!\")\n","        return\n","\n","    total_errors = 0\n","\n","    for marker in sorted(all_markers):\n","        print(f\"\\n{'='*60}\")\n","        print(f\"ANALYZING MARKER: {marker}\")\n","        print(f\"{'='*60}\")\n","\n","        # Find all items with this marker\n","        marker_mask = df['markers'].apply(lambda x: marker in x)\n","        marker_items = df[marker_mask]\n","\n","        if len(marker_items) == 0:\n","            print(\"No items found with this marker\")\n","            continue\n","\n","        # Determine expected NOVA\n","        expected_nova = None\n","        if 'NOVA0' in marker:\n","            expected_nova = 0\n","        elif 'NOVA1' in marker:\n","            expected_nova = 1\n","        elif 'NOVA2' in marker:\n","            expected_nova = 2\n","        elif 'NOVA3' in marker:\n","            expected_nova = 3\n","        elif 'NOVA4' in marker:\n","            expected_nova = 4\n","\n","        if expected_nova is None:\n","            print(\"Cannot determine expected NOVA for this marker\")\n","            continue\n","\n","        # Find errors\n","        error_items = marker_items[marker_items['NOVA'] != expected_nova]\n","        correct_items = marker_items[marker_items['NOVA'] == expected_nova]\n","\n","        print(f\"Expected NOVA: {expected_nova}\")\n","        print(f\"Total items with marker: {len(marker_items)}\")\n","        print(f\"Correct classifications: {len(correct_items)}\")\n","        print(f\"ERROR classifications: {len(error_items)}\")\n","        print(f\"Accuracy: {len(correct_items)/len(marker_items)*100:.1f}%\")\n","\n","        if len(error_items) > 0:\n","            total_errors += len(error_items)\n","            print(f\"\\n🚨 ALL ERROR CASES ({len(error_items)} items):\")\n","            print(\"-\" * 50)\n","\n","            for idx, (_, row) in enumerate(error_items.iterrows(), 1):\n","                print(f\"{idx}. ERROR CASE:\")\n","                print(f\"   Original Description: {row['descripcion']}\")\n","                print(f\"   Establishment: {row['establecimiento']}\")\n","                print(f\"   Expected NOVA: {expected_nova}\")\n","                print(f\"   Actual NOVA: {int(row['NOVA'])}\")\n","                print(f\"   Markers Applied: {row['markers_str']}\")\n","                print(f\"   Combined Text: {row['combined_text']}\")\n","                if 'id_gasto' in row:\n","                    print(f\"   ID: {row['id_gasto']}\")\n","                print()\n","\n","            # Summary of error patterns\n","            error_nova_dist = error_items['NOVA'].value_counts().sort_index()\n","            print(f\"ERROR PATTERN SUMMARY:\")\n","            for nova, count in error_nova_dist.items():\n","                print(f\"   → Incorrectly labeled as NOVA {int(nova)}: {count} cases\")\n","\n","        else:\n","            print(f\"\\n✅ NO ERRORS - Perfect accuracy!\")\n","\n","        print(f\"\\n{'='*60}\")\n","\n","    print(f\"\\n\" + \"=\"*80)\n","    print(f\"SUMMARY: TOTAL MARKER ERRORS ACROSS ALL MARKERS: {total_errors}\")\n","    print(f\"=\"*80)\n","\n","    return total_errors\n","\n","# ==============================================================================\n","# MAIN FUNCTION\n","# ==============================================================================\n","def explore_data(filepath):\n","    \"\"\"\n","    Complete data exploration pipeline - WITH MARKERS VERSION\n","    Enhanced text preprocessing with NOVA classification markers\n","    \"\"\"\n","    print(\"=\"*70)\n","    print(\"BETO FOOD CLASSIFICATION - WITH NOVA MARKERS PREPROCESSING\")\n","    print(\"=\"*70)\n","    print(\"\\nThis version adds classification markers to help BERT learn NOVA patterns.\")\n","    print(\"Markers are based on your labeling rules and domain knowledge.\")\n","\n","    # Load data\n","    df = load_and_validate_data(filepath)\n","    if df is None:\n","        return None\n","\n","    # Basic info\n","    print(f\"\\nDataset shape: {df.shape}\")\n","    print(f\"Columns: {df.columns.tolist()}\")\n","\n","    # Apply preprocessing with markers\n","    df_processed = preprocess_dataframe(df)\n","\n","    # Comprehensive analysis\n","    nova_counts = analyze_class_distribution(df_processed)\n","    analyze_markers_by_nova(df_processed)\n","    marker_quality = analyze_marker_quality(df_processed)\n","    analyze_text_characteristics(df_processed)\n","    show_nova_examples_with_markers(df_processed)\n","\n","    # Summary\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"PREPROCESSING SUMMARY - WITH NOVA MARKERS\")\n","    print(\"=\"*70)\n","    print(f\"✓ Total samples: {len(df_processed)}\")\n","    print(f\"✓ Text format: '[MARKERS] descripcion [SEP] establecimiento'\")\n","    print(f\"✓ Classification markers added based on domain rules\")\n","    print(f\"✓ Average markers per item: {df_processed['marker_count'].mean():.1f}\")\n","    print(f\"\\n📊 Class distribution:\")\n","    for nova, count in nova_counts.items():\n","        print(f\"   NOVA {int(nova)}: {count} ({count/len(df)*100:.1f}%)\")\n","    print(f\"\\n✓ Ready for BETO training with enhanced features!\")\n","\n","    return df_processed\n","\n","# ==============================================================================\n","# SAVE FUNCTION\n","# ==============================================================================\n","def save_preprocessed_data(df):\n","    \"\"\"\n","    Save the preprocessed data with markers\n","    \"\"\"\n","    # Select columns to save\n","    columns_to_save = [\n","        'id_gasto',  # if exists\n","        'descripcion', 'establecimiento',  # original\n","        'descripcion_clean', 'establecimiento_clean',  # cleaned\n","        'markers', 'markers_str',  # markers\n","        'combined_text',  # this is what we'll use for training\n","        'NOVA',  # target\n","        'desc_words', 'est_words', 'marker_count', 'total_words'  # statistics\n","    ]\n","\n","    # Only save existing columns\n","    columns_to_save = [col for col in columns_to_save if col in df.columns]\n","\n","    # Save\n","    output_file = 'training_dataset_preprocessed.csv'\n","    df[columns_to_save].to_csv(output_file, index=False)\n","    print(f\"\\n✓ Saved to '{output_file}'\")\n","\n","    return output_file\n","\n","# ==============================================================================\n","# MAIN EXECUTION\n","# ==============================================================================\n","if __name__ == \"__main__\":\n","    # Run preprocessing\n","    df_ready = explore_data('training_dataset_6000.csv')\n","\n","    if df_ready is not None:\n","        # Save the preprocessed data\n","        output_file = save_preprocessed_data(df_ready)\n","\n","        # NEW: Check all marker errors in detail\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"RUNNING DETAILED ERROR ANALYSIS...\")\n","        print(\"=\"*70)\n","        check_all_marker_errors(df_ready)\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"NEXT STEPS:\")\n","        print(\"=\"*70)\n","        print(\"1. Use 'combined_text' column for BETO training\")\n","        print(\"2. Markers should help with NOVA 3/4 boundary issues\")\n","        print(\"3. Compare results with your no-markers approach\")\n","        print(\"4. Expected improvements in:\")\n","        print(\"   - Completo items → NOVA 4 (excluding menu completo)\")\n","        print(\"   - Coffee with milk → NOVA 4\")\n","        print(\"   - Fast food chains → NOVA 4 (excluding simple drinks)\")\n","        print(\"   - Homemade small vendor → NOVA 3\")\n","        print(\"\\n💡 TIP: Monitor if markers reduce NOVA 3/4 confusion errors!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"k1XhW7z3uGDy","executionInfo":{"status":"ok","timestamp":1753618243509,"user_tz":-60,"elapsed":1005,"user":{"displayName":"Alfredo Heufemann Peña","userId":"01230962574111998829"}},"outputId":"de1e1c4b-a14c-41d9-fa4f-825b2d18134d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","BETO FOOD CLASSIFICATION - WITH NOVA MARKERS PREPROCESSING\n","======================================================================\n","\n","This version adds classification markers to help BERT learn NOVA patterns.\n","Markers are based on your labeling rules and domain knowledge.\n","Loading data from training_dataset_6000.csv...\n","✓ Successfully loaded 6000 samples\n","✓ All required columns present\n","✓ All NOVA values valid: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n","\n","Dataset shape: (6000, 7)\n","Columns: ['id_gasto', 'folio', 'descripcion', 'establecimiento', 'ccif', 'NOVA', 'glosa']\n","\n","=== Text Preprocessing with NOVA Classification Markers ===\n","\n","Cleaning text fields...\n","Adding NOVA classification markers...\n","Creating combined text with markers...\n","✓ Text preprocessing with markers complete\n","\n","=== Marker Statistics ===\n","Number of markers per item:\n","  0 markers: 5571 items (92.8%)\n","  1 markers: 426 items (7.1%)\n","  2 markers: 3 items (0.1%)\n","\n","Most common markers:\n","  [RESTAURANT_DISH]: 113 times\n","  [FRESH_BREAD_NOVA3]: 94 times\n","  [COMPLETO_NOVA4]: 94 times\n","  [FAST_FOOD_NOVA4]: 56 times\n","  [NON_FOOD_NOVA0]: 30 times\n","  [COFFEE_MILK_NOVA4]: 25 times\n","  [COLACION_TRADITIONAL_NOVA3]: 12 times\n","  [TRADITIONAL_DRINK_NOVA3]: 6 times\n","  [ARTISANAL_NOVA3]: 2 times\n","\n","=== Preprocessing Examples ===\n","\n","Original: HELADO CASATTA DE VAINILLA 1 LT | SUPERMERCADO LIDER\n","Markers:  \n","Final:    helado casatta de vainilla 1 lt [SEP] supermercado lider\n","NOVA:     4\n","\n","Original: THICK WESTERN BACON | CARLS JR RESTORAN)\n","Markers:  \n","Final:    thick western bacon [SEP] carls jr restoran\n","NOVA:     4\n","\n","Original: PAN HOT DOG 2 BOLSAS DE BAS | LIDER\n","Markers:  \n","Final:    pan hot dog 2 bolsas de bas [SEP] lider\n","NOVA:     4\n","\n","Original: SALSA BBQ GOURMET | SUPERMERCADO\n","Markers:  \n","Final:    salsa bbq gourmet [SEP] supermercado\n","NOVA:     4\n","\n","Original: NARANJAS FRESCAS | FERIA\n","Markers:  \n","Final:    naranjas frescas [SEP] feria\n","NOVA:     1\n","\n","=== NOVA Class Distribution ===\n","NOVA\n","0      44\n","1    1248\n","2     243\n","3     976\n","4    3489\n","Name: count, dtype: int64\n","\n","Class proportions:\n","NOVA 0: 44 (0.7%)\n","NOVA 1: 1248 (20.8%)\n","NOVA 2: 243 (4.0%)\n","NOVA 3: 976 (16.3%)\n","NOVA 4: 3489 (58.1%)\n","\n","Class imbalance ratio: 79.3:1\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","=== Marker Distribution by NOVA Category ===\n","\n","[ARTISANAL_NOVA3]:\n","  ✓ NOVA 3: 2 items (100.0% of this marker)\n","  📊 Alignment Score: 100.0% (higher is better)\n","\n","\n","[COFFEE_MILK_NOVA4]:\n","  ✓ NOVA 4: 25 items (100.0% of this marker)\n","  📊 Alignment Score: 100.0% (higher is better)\n","\n","\n","[COLACION_TRADITIONAL_NOVA3]:\n","  ✓ NOVA 3: 12 items (100.0% of this marker)\n","  📊 Alignment Score: 100.0% (higher is better)\n","\n","\n","[COMPLETO_NOVA4]:\n","  ✓ NOVA 4: 94 items (100.0% of this marker)\n","  📊 Alignment Score: 100.0% (higher is better)\n","\n","\n","[FAST_FOOD_NOVA4]:\n","  ✓ NOVA 4: 56 items (100.0% of this marker)\n","  📊 Alignment Score: 100.0% (higher is better)\n","\n","\n","[FRESH_BREAD_NOVA3]:\n","  ✓ NOVA 3: 94 items (100.0% of this marker)\n","  📊 Alignment Score: 100.0% (higher is better)\n","\n","\n","[NON_FOOD_NOVA0]:\n","  ⚠️ NOVA 0: 30 items (100.0% of this marker)\n","\n","\n","[RESTAURANT_DISH]:\n","  ⚠️ NOVA 3: 86 items (76.1% of this marker)\n","  ⚠️ NOVA 4: 27 items (23.9% of this marker)\n","\n","\n","[TRADITIONAL_DRINK_NOVA3]:\n","  ✓ NOVA 3: 6 items (100.0% of this marker)\n","  📊 Alignment Score: 100.0% (higher is better)\n","\n","\n","=== MARKER QUALITY ANALYSIS ===\n","Marker Quality Summary (worst first):\n","Marker | Total | Correct | Accuracy | Status\n","------------------------------------------------------------\n","[ARTISANAL_NOVA3]              |     2 |       2 |    100.0% | 🟢 GOOD\n","[COFFEE_MILK_NOVA4]            |    25 |      25 |    100.0% | 🟢 GOOD\n","[COLACION_TRADITIONAL_NOVA3]   |    12 |      12 |    100.0% | 🟢 GOOD\n","[COMPLETO_NOVA4]               |    94 |      94 |    100.0% | 🟢 GOOD\n","[FAST_FOOD_NOVA4]              |    56 |      56 |    100.0% | 🟢 GOOD\n","[FRESH_BREAD_NOVA3]            |    94 |      94 |    100.0% | 🟢 GOOD\n","[TRADITIONAL_DRINK_NOVA3]      |     6 |       6 |    100.0% | 🟢 GOOD\n","\n","=== DETAILED ANALYSIS FOR PROBLEMATIC MARKERS ===\n","\n","=== Text Characteristics Analysis ===\n","\n","Overall text statistics:\n","Average description length: 21.7 characters\n","Average description words: 3.5\n","Average establishment words: 1.7\n","Average markers per item: 0.1\n","Average total words: 5.3\n","\n","=== Text Characteristics by NOVA Category ===\n","NOVA | Avg Desc Words | Avg Est Words | Avg Markers | Avg Total Words\n","----------------------------------------------------------------------\n","  0  |     1.9      |     2.3      |     0.7     |      4.9\n","  1  |     2.8      |     1.6      |     0.0     |      4.4\n","  2  |     2.6      |     1.4      |     0.0     |      4.0\n","  3  |     4.2      |     1.9      |     0.2     |      6.3\n","  4  |     3.7      |     1.7      |     0.1     |      5.4\n","\n","=== Sample Descriptions per NOVA Category (with markers) ===\n","\n","--- NOVA 0 Examples ---\n","• Original: ENVASE PARA LLEVAR | RESTAURANTE GALINDO\n","  Markers: \n","  Final: envase para llevar [SEP] restaurante galindo\n","\n","• Original: PROPINA | RESTAURANTE KATSU MISURA\n","  Markers: [NON_FOOD_NOVA0]\n","  Final: [NON_FOOD_NOVA0] propina [SEP] restaurante katsu misura\n","\n","\n","--- NOVA 1 Examples ---\n","• Original: PAPAS FRESCAS | FERIA\n","  Markers: \n","  Final: papas frescas [SEP] feria\n","\n","• Original: NARANJAS | FERIA LIBRE\n","  Markers: \n","  Final: naranjas [SEP] feria libre\n","\n","\n","--- NOVA 2 Examples ---\n","• Original: PAN CHANCACA DELICIOSA | UNIMARC\n","  Markers: \n","  Final: pan chancaca deliciosa [SEP] unimarc\n","\n","• Original: ACEITE MARAVILLA | UNIMARC\n","  Markers: \n","  Final: aceite maravilla [SEP] unimarc\n","\n","\n","--- NOVA 3 Examples ---\n","• Original: MOTES CON HUESILLO LISTOS PARA SERVIR | LOCAL COMIDA RAPIDA\n","  Markers: \n","  Final: motes con huesillo listos para servir [SEP] local comida rapida\n","\n","• Original: MANI 10 PERSONAS CHAPSUI CARNE MONGOLIANA+ARROZ CHAUFAN | MONTAÑA CHINA\n","  Markers: \n","  Final: mani 10 personas chapsui carne mongoliana arroz chaufan [SEP] montaña china\n","\n","\n","--- NOVA 4 Examples ---\n","• Original: ALMUERZO (PORCION PAPAS FRITAS - BEBIDA EN LATA SPRITE - MICHELADA HEINEKEN) | SERVICENTRO\n","  Markers: \n","  Final: almuerzo porcion papas fritas - bebida en lata sprite - michelada heineken [SEP] servicentro\n","\n","• Original: SANDWICH + TAZA MILO | PERSONA PARTICULAR\n","  Markers: \n","  Final: sandwich taza milo [SEP] persona particular\n","\n","\n","======================================================================\n","PREPROCESSING SUMMARY - WITH NOVA MARKERS\n","======================================================================\n","✓ Total samples: 6000\n","✓ Text format: '[MARKERS] descripcion [SEP] establecimiento'\n","✓ Classification markers added based on domain rules\n","✓ Average markers per item: 0.1\n","\n","📊 Class distribution:\n","   NOVA 0: 44 (0.7%)\n","   NOVA 1: 1248 (20.8%)\n","   NOVA 2: 243 (4.0%)\n","   NOVA 3: 976 (16.3%)\n","   NOVA 4: 3489 (58.1%)\n","\n","✓ Ready for BETO training with enhanced features!\n","\n","✓ Saved to 'training_dataset_preprocessed.csv'\n","\n","======================================================================\n","RUNNING DETAILED ERROR ANALYSIS...\n","======================================================================\n","\n","================================================================================\n","COMPLETE MARKER ERROR ANALYSIS - ALL MISCLASSIFIED CASES\n","================================================================================\n","\n","============================================================\n","ANALYZING MARKER: [ARTISANAL_NOVA3]\n","============================================================\n","Expected NOVA: 3\n","Total items with marker: 2\n","Correct classifications: 2\n","ERROR classifications: 0\n","Accuracy: 100.0%\n","\n","✅ NO ERRORS - Perfect accuracy!\n","\n","============================================================\n","\n","============================================================\n","ANALYZING MARKER: [COFFEE_MILK_NOVA4]\n","============================================================\n","Expected NOVA: 4\n","Total items with marker: 25\n","Correct classifications: 25\n","ERROR classifications: 0\n","Accuracy: 100.0%\n","\n","✅ NO ERRORS - Perfect accuracy!\n","\n","============================================================\n","\n","============================================================\n","ANALYZING MARKER: [COLACION_TRADITIONAL_NOVA3]\n","============================================================\n","Expected NOVA: 3\n","Total items with marker: 12\n","Correct classifications: 12\n","ERROR classifications: 0\n","Accuracy: 100.0%\n","\n","✅ NO ERRORS - Perfect accuracy!\n","\n","============================================================\n","\n","============================================================\n","ANALYZING MARKER: [COMPLETO_NOVA4]\n","============================================================\n","Expected NOVA: 4\n","Total items with marker: 94\n","Correct classifications: 94\n","ERROR classifications: 0\n","Accuracy: 100.0%\n","\n","✅ NO ERRORS - Perfect accuracy!\n","\n","============================================================\n","\n","============================================================\n","ANALYZING MARKER: [FAST_FOOD_NOVA4]\n","============================================================\n","Expected NOVA: 4\n","Total items with marker: 56\n","Correct classifications: 56\n","ERROR classifications: 0\n","Accuracy: 100.0%\n","\n","✅ NO ERRORS - Perfect accuracy!\n","\n","============================================================\n","\n","============================================================\n","ANALYZING MARKER: [FRESH_BREAD_NOVA3]\n","============================================================\n","Expected NOVA: 3\n","Total items with marker: 94\n","Correct classifications: 94\n","ERROR classifications: 0\n","Accuracy: 100.0%\n","\n","✅ NO ERRORS - Perfect accuracy!\n","\n","============================================================\n","\n","============================================================\n","ANALYZING MARKER: [NON_FOOD_NOVA0]\n","============================================================\n","Expected NOVA: 0\n","Total items with marker: 30\n","Correct classifications: 30\n","ERROR classifications: 0\n","Accuracy: 100.0%\n","\n","✅ NO ERRORS - Perfect accuracy!\n","\n","============================================================\n","\n","============================================================\n","ANALYZING MARKER: [RESTAURANT_DISH]\n","============================================================\n","Cannot determine expected NOVA for this marker\n","\n","============================================================\n","ANALYZING MARKER: [TRADITIONAL_DRINK_NOVA3]\n","============================================================\n","Expected NOVA: 3\n","Total items with marker: 6\n","Correct classifications: 6\n","ERROR classifications: 0\n","Accuracy: 100.0%\n","\n","✅ NO ERRORS - Perfect accuracy!\n","\n","============================================================\n","\n","================================================================================\n","SUMMARY: TOTAL MARKER ERRORS ACROSS ALL MARKERS: 0\n","================================================================================\n","\n","======================================================================\n","NEXT STEPS:\n","======================================================================\n","1. Use 'combined_text' column for BETO training\n","2. Markers should help with NOVA 3/4 boundary issues\n","3. Compare results with your no-markers approach\n","4. Expected improvements in:\n","   - Completo items → NOVA 4 (excluding menu completo)\n","   - Coffee with milk → NOVA 4\n","   - Fast food chains → NOVA 4 (excluding simple drinks)\n","   - Homemade small vendor → NOVA 3\n","\n","💡 TIP: Monitor if markers reduce NOVA 3/4 confusion errors!\n"]}]},{"cell_type":"code","source":["\"\"\"\n","=============================================================================\n","CHILEAN HOUSEHOLD FOOD CLASSIFICATION PROJECT\n","BERT/BETO Implementation for Ultra-Processed Food Detection\n","\n","Author: Alfredo Heufemann\n","Project: Master's thesis at LSE - Household structure and ultra-processed food consumption\n","Dataset: Chile's 2022 Household Budget Survey (EPF) - 900,000 food purchase records\n","\n","Objective: Fine-tune Spanish BERT (BETO) to classify food items into NOVA categories (0-4)\n","          based on processing levels, overcoming limitations of traditional COICOP codes.\n","=============================================================================\n","\"\"\"\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import accelerate\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import set_seed\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","import warnings\n","import json\n","import os\n","from copy import deepcopy\n","warnings.filterwarnings('ignore')\n","\n","\n","# =============================================================================\n","# SET GLOBAL RANDOM SEED FOR REPRODUCIBILITY\n","# =============================================================================\n","RANDOM_SEED = 2025\n","set_seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","if torch.cuda.is_available():\n","   torch.cuda.manual_seed_all(RANDOM_SEED)\n","\n","# =============================================================================\n","# CUSTOM DATASET CLASS FOR NOVA CLASSIFICATION\n","# =============================================================================\n","class NOVADataset(Dataset):\n","   \"\"\"\n","   PyTorch Dataset for NOVA food classification.\n","\n","   Handles tokenization of Spanish text descriptions using BETO tokenizer.\n","   Input format: \"descripcion [SEP] establecimiento\"\n","   \"\"\"\n","   def __init__(self, texts, labels, tokenizer, max_length=128):\n","       self.texts = texts.values if hasattr(texts, 'values') else texts\n","       self.labels = labels.values if hasattr(labels, 'values') else labels\n","       self.tokenizer = tokenizer\n","       self.max_length = max_length\n","\n","   def __len__(self):\n","       return len(self.texts)\n","\n","   def __getitem__(self, idx):\n","       text = str(self.texts[idx])\n","       label = self.labels[idx]\n","\n","       # Tokenize text with BETO tokenizer\n","       encoding = self.tokenizer(\n","           text,\n","           truncation=True,\n","           padding='max_length',\n","           max_length=self.max_length,\n","           return_tensors='pt'\n","       )\n","\n","       return {\n","           'input_ids': encoding['input_ids'].flatten(),\n","           'attention_mask': encoding['attention_mask'].flatten(),\n","           'labels': torch.tensor(label, dtype=torch.long)\n","       }\n","\n","# =============================================================================\n","# CUSTOM TRAINER WITH CLASS WEIGHTS FOR IMBALANCED DATA\n","# =============================================================================\n","class WeightedTrainer(Trainer):\n","   \"\"\"\n","   Custom Trainer that applies class weights to handle imbalanced dataset.\n","   \"\"\"\n","   def __init__(self, class_weights, *args, **kwargs):\n","       super().__init__(*args, **kwargs)\n","       self.class_weights = class_weights\n","\n","   def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n","       # Move class weights to same device as inputs\n","       if self.class_weights.device != inputs[\"labels\"].device:\n","           self.class_weights = self.class_weights.to(inputs[\"labels\"].device)\n","\n","       labels = inputs.get(\"labels\")\n","       outputs = model(**inputs)\n","       logits = outputs.get(\"logits\")\n","\n","       # Apply weighted cross-entropy loss\n","       loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n","       loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","\n","       return (loss, outputs) if return_outputs else loss\n","\n","# =============================================================================\n","# CALIBRATION METRICS\n","# =============================================================================\n","def calculate_calibration_metrics(y_true, y_pred_proba, n_bins=10):\n","   \"\"\"\n","   Calculate Expected Calibration Error (ECE) and reliability diagram data.\n","\n","   ECE measures the difference between predicted confidence and actual accuracy.\n","   Lower ECE indicates better calibration.\n","   \"\"\"\n","   # Get predicted classes and confidences\n","   y_pred = np.argmax(y_pred_proba, axis=1)\n","   confidences = np.max(y_pred_proba, axis=1)\n","   accuracies = (y_pred == y_true).astype(float)\n","\n","   # Calculate ECE\n","   bin_boundaries = np.linspace(0, 1, n_bins + 1)\n","   bin_lowers = bin_boundaries[:-1]\n","   bin_uppers = bin_boundaries[1:]\n","\n","   ece = 0\n","   bin_accuracies = []\n","   bin_confidences = []\n","   bin_counts = []\n","\n","   for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n","       in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n","       prop_in_bin = in_bin.mean()\n","\n","       if prop_in_bin > 0:\n","           accuracy_in_bin = accuracies[in_bin].mean()\n","           avg_confidence_in_bin = confidences[in_bin].mean()\n","           ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n","\n","           bin_accuracies.append(accuracy_in_bin)\n","           bin_confidences.append(avg_confidence_in_bin)\n","           bin_counts.append(in_bin.sum())\n","       else:\n","           bin_accuracies.append(0)\n","           bin_confidences.append(0)\n","           bin_counts.append(0)\n","\n","   return ece, bin_accuracies, bin_confidences, bin_counts\n","\n","# =============================================================================\n","# EVALUATION METRICS FOR TRAINING\n","# =============================================================================\n","def compute_metrics(eval_pred):\n","   \"\"\"\n","   Compute metrics during training for model selection.\n","   Tracks overall accuracy, weighted F1, and per-class F1 scores.\n","   \"\"\"\n","   predictions, labels = eval_pred\n","   predictions = np.argmax(predictions, axis=1)\n","\n","   # Overall metrics\n","   accuracy = accuracy_score(labels, predictions)\n","   precision, recall, f1, _ = precision_recall_fscore_support(\n","       labels, predictions, average='weighted', zero_division=0\n","   )\n","\n","   # Per-class F1 scores for monitoring class-specific performance\n","   f1_per_class = precision_recall_fscore_support(\n","       labels, predictions, average=None, zero_division=0\n","   )[2]\n","\n","   metrics = {\n","       'accuracy': accuracy,\n","       'f1_weighted': f1,\n","       'precision_weighted': precision,\n","       'recall_weighted': recall,\n","   }\n","\n","   # Add per-class F1 scores\n","   for i, f1_score in enumerate(f1_per_class):\n","       metrics[f'f1_nova_{i}'] = f1_score\n","\n","   return metrics\n","\n","# =============================================================================\n","# TEMPERATURE SCALING FOR CALIBRATION\n","# =============================================================================\n","class TemperatureScaling:\n","   \"\"\"\n","   Temperature scaling for model calibration.\n","   Learns a single parameter T to adjust model confidence.\n","\n","   Reference: Guo et al., \"On Calibration of Modern Neural Networks\"\n","   \"\"\"\n","   def __init__(self):\n","       self.temperature = 1.0\n","\n","   def fit(self, logits, labels):\n","       \"\"\"\n","       Find optimal temperature using validation set.\n","       \"\"\"\n","       from scipy.optimize import minimize\n","\n","       def nll_loss(T):\n","           # Apply temperature scaling\n","           scaled_logits = logits / T\n","           # Calculate negative log-likelihood\n","           probs = torch.softmax(torch.tensor(scaled_logits), dim=-1).numpy()\n","           # Compute cross-entropy\n","           ce = 0\n","           for i in range(len(labels)):\n","               ce -= np.log(probs[i, labels[i]] + 1e-10)\n","           return ce / len(labels)\n","\n","       # Optimize temperature\n","       result = minimize(nll_loss, x0=1.0, bounds=[(0.1, 10.0)], method='L-BFGS-B')\n","       self.temperature = result.x[0]\n","       return self\n","\n","   def transform(self, logits):\n","       \"\"\"Apply temperature scaling to logits.\"\"\"\n","       return logits / self.temperature\n","\n","# =============================================================================\n","# CROSS-VALIDATION FUNCTION\n","# =============================================================================\n","def run_cross_validation(df, n_folds=5):\n","   \"\"\"\n","   Run k-fold cross-validation to assess model stability.\n","\n","   Returns:\n","       cv_results: Dictionary containing metrics for each fold\n","       error_analysis: Consistent errors across folds\n","   \"\"\"\n","   print(\"\\n\" + \"=\"*70)\n","   print(f\"STARTING {n_folds}-FOLD CROSS-VALIDATION\")\n","   print(\"=\"*70)\n","\n","   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","   model_name = 'dccuchile/bert-base-spanish-wwm-uncased'\n","   tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","   # Prepare data\n","   X = df[['combined_text', 'descripcion', 'establecimiento', 'id_gasto']]\n","   y = df['NOVA']\n","\n","   # Initialize cross-validation\n","   skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n","\n","   cv_results = {\n","       'fold_accuracies': [],\n","       'fold_f1_scores': [],\n","       'fold_ece_scores': [],\n","       'fold_nova3_4_errors': [],\n","       'per_class_f1': {i: [] for i in range(5)},\n","       'confusion_matrices': []\n","   }\n","\n","   # Store all predictions for error analysis\n","   all_predictions = pd.DataFrame()\n","\n","   for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n","       print(f\"\\n[FOLD {fold+1}/{n_folds}]\")\n","\n","       # Split data\n","       X_train_fold = X.iloc[train_idx]\n","       y_train_fold = y.iloc[train_idx]\n","       X_val_fold = X.iloc[val_idx]\n","       y_val_fold = y.iloc[val_idx]\n","\n","       print(f\"  Train: {len(X_train_fold)} samples\")\n","       print(f\"  Val:   {len(X_val_fold)} samples\")\n","\n","       # Calculate class weights for this fold\n","       class_weights = compute_class_weight(\n","           'balanced',\n","           classes=np.unique(y_train_fold),\n","           y=y_train_fold\n","       )\n","       class_weights = torch.tensor(class_weights, dtype=torch.float)\n","\n","       # Create datasets\n","       train_dataset = NOVADataset(X_train_fold['combined_text'], y_train_fold, tokenizer)\n","       val_dataset = NOVADataset(X_val_fold['combined_text'], y_val_fold, tokenizer)\n","\n","       # Initialize model (fresh for each fold)\n","       model = AutoModelForSequenceClassification.from_pretrained(\n","           model_name,\n","           num_labels=5,\n","           problem_type=\"single_label_classification\"\n","       )\n","       model.to(device)\n","\n","       # Training configuration\n","       training_args_dict = {\n","           'output_dir': f'./cv_fold_{fold}',\n","           'num_train_epochs': 5,\n","           'per_device_train_batch_size': 32,\n","           'per_device_eval_batch_size': 64,\n","           'warmup_steps': 100,\n","           'logging_steps': 50,\n","           'eval_steps': 100,\n","           'save_strategy': \"no\",  # Don't save intermediate models\n","           'report_to': \"none\",\n","           'seed': RANDOM_SEED + fold,  # Different seed per fold\n","       }\n","\n","       # Handle different transformers versions\n","       if hasattr(TrainingArguments, 'eval_strategy'):\n","           training_args_dict['eval_strategy'] = \"steps\"\n","       else:\n","           training_args_dict['evaluation_strategy'] = \"steps\"\n","\n","       training_args = TrainingArguments(**training_args_dict)\n","\n","       # Train model\n","       trainer = WeightedTrainer(\n","           class_weights=class_weights,\n","           model=model,\n","           args=training_args,\n","           train_dataset=train_dataset,\n","           eval_dataset=val_dataset,\n","           compute_metrics=compute_metrics,\n","           tokenizer=tokenizer,\n","       )\n","\n","       print(f\"  Training fold {fold+1}...\")\n","       trainer.train()\n","\n","       # Evaluate\n","       val_predictions = trainer.predict(val_dataset)\n","       logits = val_predictions.predictions\n","       y_pred = np.argmax(logits, axis=1)\n","       y_true = y_val_fold.values\n","\n","       # Calculate metrics\n","       accuracy = accuracy_score(y_true, y_pred)\n","       f1_weighted = precision_recall_fscore_support(y_true, y_pred, average='weighted')[2]\n","\n","       # Calculate ECE\n","       probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n","       ece, _, _, _ = calculate_calibration_metrics(y_true, probs)\n","\n","       # Per-class F1\n","       f1_per_class = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)[2]\n","\n","       # Count NOVA 3/4 errors\n","       cm = confusion_matrix(y_true, y_pred)\n","       nova3_4_errors = (cm[3, 4] + cm[4, 3]) if cm.shape[0] > 4 else 0\n","\n","       # Store results\n","       cv_results['fold_accuracies'].append(accuracy)\n","       cv_results['fold_f1_scores'].append(f1_weighted)\n","       cv_results['fold_ece_scores'].append(ece)\n","       cv_results['fold_nova3_4_errors'].append(nova3_4_errors)\n","       cv_results['confusion_matrices'].append(cm)\n","\n","       for i in range(5):\n","           if i < len(f1_per_class):\n","               cv_results['per_class_f1'][i].append(f1_per_class[i])\n","\n","       # Store predictions for error analysis\n","       fold_predictions = pd.DataFrame({\n","           'fold': fold,\n","           'id_gasto': X_val_fold['id_gasto'].values,\n","           'descripcion': X_val_fold['descripcion'].values,\n","           'establecimiento': X_val_fold['establecimiento'].values,\n","           'true_label': y_true,\n","           'predicted_label': y_pred,\n","           'confidence': probs.max(axis=1)\n","       })\n","       all_predictions = pd.concat([all_predictions, fold_predictions])\n","\n","       print(f\"  Fold {fold+1} Results:\")\n","       print(f\"    Accuracy: {accuracy:.4f}\")\n","       print(f\"    F1 Score: {f1_weighted:.4f}\")\n","       print(f\"    ECE: {ece:.4f}\")\n","       print(f\"    NOVA 3/4 errors: {nova3_4_errors}\")\n","\n","       # Clean up to save memory\n","       del model\n","       del trainer\n","       torch.cuda.empty_cache()\n","\n","   # Calculate summary statistics\n","   print(\"\\n\" + \"=\"*70)\n","   print(\"CROSS-VALIDATION SUMMARY\")\n","   print(\"=\"*70)\n","\n","   print(f\"\\nOverall Performance (mean ± std):\")\n","   print(f\"  Accuracy: {np.mean(cv_results['fold_accuracies']):.4f} ± {np.std(cv_results['fold_accuracies']):.4f}\")\n","   print(f\"  F1 Score: {np.mean(cv_results['fold_f1_scores']):.4f} ± {np.std(cv_results['fold_f1_scores']):.4f}\")\n","   print(f\"  ECE:      {np.mean(cv_results['fold_ece_scores']):.4f} ± {np.std(cv_results['fold_ece_scores']):.4f}\")\n","\n","   print(f\"\\nPer-Class F1 Scores (mean ± std):\")\n","   for i in range(5):\n","       if cv_results['per_class_f1'][i]:\n","           mean_f1 = np.mean(cv_results['per_class_f1'][i])\n","           std_f1 = np.std(cv_results['per_class_f1'][i])\n","           print(f\"  NOVA {i}: {mean_f1:.4f} ± {std_f1:.4f}\")\n","\n","   print(f\"\\nNOVA 3/4 Confusion:\")\n","   print(f\"  Mean errors per fold: {np.mean(cv_results['fold_nova3_4_errors']):.1f} ± {np.std(cv_results['fold_nova3_4_errors']):.1f}\")\n","\n","   # Analyze consistent errors across folds\n","   print(\"\\n[ERROR CONSISTENCY ANALYSIS]\")\n","   error_counts = all_predictions[all_predictions['true_label'] != all_predictions['predicted_label']].groupby(\n","       ['descripcion', 'establecimiento', 'true_label', 'predicted_label']\n","   ).size().reset_index(name='error_count')\n","\n","   # Find errors that appear in multiple folds\n","   consistent_errors = error_counts[error_counts['error_count'] >= n_folds * 0.6]  # Errors in 60%+ of folds\n","\n","   if len(consistent_errors) > 0:\n","       print(f\"\\nFound {len(consistent_errors)} consistent errors across folds:\")\n","       print(\"\\nTop 10 most consistent errors:\")\n","       for _, error in consistent_errors.nlargest(10, 'error_count').iterrows():\n","           print(f\"  {error['descripcion']} | {error['establecimiento']}\")\n","           print(f\"    True: NOVA {error['true_label']}, Predicted: NOVA {error['predicted_label']} ({error['error_count']} times)\")\n","\n","   # Save CV results\n","   cv_results['all_predictions'] = all_predictions\n","   cv_results['consistent_errors'] = consistent_errors\n","\n","   # Create visualization\n","   plt.figure(figsize=(12, 4))\n","\n","   # Plot 1: Performance metrics across folds\n","   plt.subplot(1, 3, 1)\n","   folds = range(1, n_folds + 1)\n","   plt.plot(folds, cv_results['fold_accuracies'], 'o-', label='Accuracy')\n","   plt.plot(folds, cv_results['fold_f1_scores'], 's-', label='F1 Score')\n","   plt.axhline(np.mean(cv_results['fold_accuracies']), color='blue', linestyle='--', alpha=0.5)\n","   plt.axhline(np.mean(cv_results['fold_f1_scores']), color='orange', linestyle='--', alpha=0.5)\n","   plt.xlabel('Fold')\n","   plt.ylabel('Score')\n","   plt.title('Performance Across Folds')\n","   plt.legend()\n","   plt.grid(True, alpha=0.3)\n","\n","   # Plot 2: ECE across folds\n","   plt.subplot(1, 3, 2)\n","   plt.plot(folds, cv_results['fold_ece_scores'], 'o-', color='green')\n","   plt.axhline(np.mean(cv_results['fold_ece_scores']), color='green', linestyle='--', alpha=0.5)\n","   plt.xlabel('Fold')\n","   plt.ylabel('ECE')\n","   plt.title('Calibration Error Across Folds')\n","   plt.grid(True, alpha=0.3)\n","\n","   # Plot 3: NOVA 3/4 errors\n","   plt.subplot(1, 3, 3)\n","   plt.bar(folds, cv_results['fold_nova3_4_errors'])\n","   plt.axhline(np.mean(cv_results['fold_nova3_4_errors']), color='red', linestyle='--', alpha=0.5)\n","   plt.xlabel('Fold')\n","   plt.ylabel('Number of Errors')\n","   plt.title('NOVA 3/4 Confusion Across Folds')\n","\n","   plt.tight_layout()\n","   plt.savefig('cross_validation_results.png', dpi=300)\n","   plt.close()\n","\n","   return cv_results\n","\n","# =============================================================================\n","# MAIN TRAINING FUNCTION (Updated to include CV option)\n","# =============================================================================\n","def train_single_model(run_cv=True):\n","   \"\"\"\n","   Main training pipeline for BETO food classification model.\n","\n","   Pipeline:\n","   1. Load and split data (70/15/15)\n","   2. Run cross-validation if requested\n","   3. Initialize BETO model and tokenizer\n","   4. Calculate class weights for imbalanced data\n","   5. Train with early stopping\n","   6. Evaluate calibration metrics\n","   7. Apply temperature scaling if needed\n","   8. Save final model\n","   \"\"\"\n","\n","   print(\"=\"*70)\n","   print(\"BETO TRAINING FOR NOVA FOOD CLASSIFICATION\")\n","   print(\"Chilean Household Food Purchase Analysis\")\n","   print(\"=\"*70)\n","\n","   # -------------------------------------------------------------------------\n","   # 1. SETUP AND DATA LOADING\n","   # -------------------------------------------------------------------------\n","   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","   print(f\"\\n[SETUP] Using device: {device}\")\n","\n","   print(\"\\n[DATA] Loading preprocessed training data...\")\n","   df = pd.read_csv('training_dataset_preprocessed.csv')\n","   print(f\"✓ Loaded {len(df)} labeled samples\")\n","\n","   # Print class distribution\n","   print(\"\\n[DATA] Class distribution:\")\n","   for nova_class, count in df['NOVA'].value_counts().sort_index().items():\n","       percentage = (count / len(df)) * 100\n","       print(f\"  NOVA {nova_class}: {count} samples ({percentage:.1f}%)\")\n","\n","   # -------------------------------------------------------------------------\n","   # 1.5 RUN CROSS-VALIDATION (if requested)\n","   # -------------------------------------------------------------------------\n","   if run_cv:\n","       print(\"\\n[CROSS-VALIDATION] Running 5-fold CV to assess model stability...\")\n","       cv_results = run_cross_validation(df, n_folds=5)\n","\n","       # Save CV results\n","       with open('cv_results.json', 'w') as f:\n","           json.dump({\n","               'fold_accuracies': cv_results['fold_accuracies'],\n","               'fold_f1_scores': cv_results['fold_f1_scores'],\n","               'fold_ece_scores': cv_results['fold_ece_scores'],\n","               'mean_accuracy': float(np.mean(cv_results['fold_accuracies'])),\n","               'std_accuracy': float(np.std(cv_results['fold_accuracies'])),\n","               'mean_f1': float(np.mean(cv_results['fold_f1_scores'])),\n","               'std_f1': float(np.std(cv_results['fold_f1_scores']))\n","           }, f, indent=2)\n","\n","       cv_results['all_predictions'].to_csv('cv_all_predictions.csv', index=False)\n","       cv_results['consistent_errors'].to_csv('cv_consistent_errors.csv', index=False)\n","\n","       print(\"\\n✓ Cross-validation complete. Results saved.\")\n","       print(\"\\n[CONTINUING] Now training final model on full dataset...\")\n","\n","   # -------------------------------------------------------------------------\n","   # 2. TRAIN/VALIDATION/TEST SPLIT\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[SPLIT] Creating train/validation/test splits...\")\n","\n","   # Prepare features and labels\n","   X = df[['combined_text', 'descripcion', 'establecimiento', 'id_gasto']]\n","   y = df['NOVA']\n","\n","   # First split: separate test set (15%)\n","   X_temp, X_test, y_temp, y_test = train_test_split(\n","       X, y,\n","       test_size=0.15,\n","       stratify=y,\n","       random_state=RANDOM_SEED\n","   )\n","\n","   # Save test set separately - DO NOT TOUCH UNTIL FINAL EVALUATION\n","   print(\"\\n[SPLIT] Saving test set for final evaluation...\")\n","   test_df = pd.DataFrame({\n","       'id_gasto': X_test['id_gasto'],\n","       'combined_text': X_test['combined_text'],\n","       'descripcion': X_test['descripcion'],\n","       'establecimiento': X_test['establecimiento'],\n","       'NOVA': y_test\n","   })\n","   test_df.to_csv('TEST_SET_DO_NOT_TOUCH.csv', index=False)\n","   print(f\"✓ Test set saved: {len(test_df)} samples\")\n","\n","   # Second split: divide remaining into train and validation\n","   X_train, X_val, y_train, y_val = train_test_split(\n","       X_temp, y_temp,\n","       test_size=0.176,  # This gives us ~15% of total data for validation\n","       stratify=y_temp,\n","       random_state=RANDOM_SEED\n","   )\n","\n","   print(f\"\\n[SPLIT] Final splits:\")\n","   print(f\"  Train: {len(X_train)} samples ({len(X_train)/len(df)*100:.1f}%)\")\n","   print(f\"  Val:   {len(X_val)} samples ({len(X_val)/len(df)*100:.1f}%)\")\n","   print(f\"  Test:  {len(X_test)} samples ({len(X_test)/len(df)*100:.1f}%)\")\n","\n","   # -------------------------------------------------------------------------\n","   # 3. MODEL INITIALIZATION\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[MODEL] Loading BETO (Spanish BERT)...\")\n","   model_name = 'dccuchile/bert-base-spanish-wwm-uncased'\n","\n","   tokenizer = AutoTokenizer.from_pretrained(model_name)\n","   model = AutoModelForSequenceClassification.from_pretrained(\n","       model_name,\n","       num_labels=5,  # NOVA categories 0-4\n","       problem_type=\"single_label_classification\"\n","   )\n","   model.to(device)\n","   print(f\"✓ Model loaded with {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")\n","\n","   # -------------------------------------------------------------------------\n","   # 4. CLASS WEIGHTS CALCULATION\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[WEIGHTS] Computing class weights for imbalanced data...\")\n","   class_weights = compute_class_weight(\n","       'balanced',\n","       classes=np.unique(y_train),\n","       y=y_train\n","   )\n","   class_weights = torch.tensor(class_weights, dtype=torch.float)\n","\n","   print(\"Class weights:\")\n","   for i, weight in enumerate(class_weights):\n","       print(f\"  NOVA {i}: {weight:.2f}\")\n","\n","   # -------------------------------------------------------------------------\n","   # 5. DATASET PREPARATION\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[DATASET] Creating PyTorch datasets...\")\n","   train_dataset = NOVADataset(X_train['combined_text'], y_train, tokenizer)\n","   val_dataset = NOVADataset(X_val['combined_text'], y_val, tokenizer)\n","   print(f\"✓ Datasets created with max_length=128 tokens\")\n","\n","   # -------------------------------------------------------------------------\n","   # 6. TRAINING CONFIGURATION\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[CONFIG] Setting up training arguments...\")\n","\n","   # Calculate total training steps for warmup\n","   total_steps = (len(train_dataset) // 32) * 5  # batch_size * epochs\n","   warmup_steps = int(0.1 * total_steps)  # 10% warmup\n","\n","   training_args_dict = {\n","       'output_dir': './beto-nova-best',\n","       'num_train_epochs': 5,\n","       'per_device_train_batch_size': 32,\n","       'per_device_eval_batch_size': 64,\n","       'warmup_steps': warmup_steps,  # Reduced from 500\n","       'weight_decay': 0.01,\n","       'learning_rate': 5e-5,\n","       'logging_dir': './logs',\n","       'logging_steps': 50,\n","       'eval_steps': 100,\n","       'save_strategy': \"steps\",\n","       'save_steps': 500,\n","       'load_best_model_at_end': True,\n","       'metric_for_best_model': \"eval_loss\",  # Changed from f1_weighted for better calibration\n","       'greater_is_better': False,  # Loss should decrease\n","       'save_total_limit': 2,\n","       'fp16': torch.cuda.is_available(),\n","       'report_to': \"none\",\n","       'seed': RANDOM_SEED,\n","   }\n","\n","   # Handle different transformers versions\n","   if hasattr(TrainingArguments, 'eval_strategy'):\n","       training_args_dict['eval_strategy'] = \"steps\"\n","   else:\n","       training_args_dict['evaluation_strategy'] = \"steps\"\n","\n","   training_args = TrainingArguments(**training_args_dict)\n","\n","   print(f\"✓ Training configuration set:\")\n","   print(f\"  Warmup steps: {warmup_steps}\")\n","   print(f\"  Total steps: ~{total_steps}\")\n","   print(f\"  Effective batch size: {training_args.per_device_train_batch_size}\")\n","\n","   # -------------------------------------------------------------------------\n","   # 7. TRAINER INITIALIZATION\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[TRAINER] Initializing weighted trainer...\")\n","   trainer = WeightedTrainer(\n","       class_weights=class_weights,\n","       model=model,\n","       args=training_args,\n","       train_dataset=train_dataset,\n","       eval_dataset=val_dataset,\n","       compute_metrics=compute_metrics,\n","       tokenizer=tokenizer,\n","   )\n","\n","   # -------------------------------------------------------------------------\n","   # 8. TRAINING\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[TRAINING] Starting fine-tuning...\")\n","   print(\"=\"*70)\n","   trainer.train()\n","   print(\"=\"*70)\n","\n","   # -------------------------------------------------------------------------\n","   # 9. VALIDATION EVALUATION\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[EVALUATION] Running comprehensive validation analysis...\")\n","\n","   # Get predictions\n","   val_predictions = trainer.predict(val_dataset)\n","   logits = val_predictions.predictions\n","   y_pred_val = np.argmax(logits, axis=1)\n","   y_val_array = y_val.values if hasattr(y_val, 'values') else y_val\n","\n","   # Basic metrics\n","   accuracy = accuracy_score(y_val_array, y_pred_val)\n","   print(f\"\\n✓ Validation Accuracy: {accuracy:.4f}\")\n","\n","   # Detailed classification report\n","   print(\"\\n[EVALUATION] Classification Report:\")\n","   print(classification_report(y_val_array, y_pred_val,\n","                             target_names=[f'NOVA {i}' for i in range(5)]))\n","\n","   # -------------------------------------------------------------------------\n","   # 10. CALIBRATION ANALYSIS (LOSS-BASED)\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[CALIBRATION] Analyzing model calibration...\")\n","\n","   # Calculate probabilities\n","   probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n","\n","   # Calculate loss-based cross-entropy (for temperature scaling)\n","   loss_cross_entropies = []\n","   for i, prob_dist in enumerate(probs):\n","       # Use TRUE label for loss calculation\n","       true_label = y_val_array[i]\n","       ce = -np.log(prob_dist[true_label] + 1e-10)\n","       loss_cross_entropies.append(ce)\n","\n","   loss_ce_array = np.array(loss_cross_entropies)\n","   print(f\"\\nLoss Cross-entropy statistics (for calibration):\")\n","   print(f\"  Mean: {loss_ce_array.mean():.4f}\")\n","   print(f\"  Median: {np.median(loss_ce_array):.4f}\")\n","   print(f\"  Std: {loss_ce_array.std():.4f}\")\n","\n","   # Calculate ECE\n","   ece, bin_accs, bin_confs, bin_counts = calculate_calibration_metrics(\n","       y_val_array, probs\n","   )\n","   print(f\"\\nExpected Calibration Error (ECE): {ece:.4f}\")\n","\n","   # -------------------------------------------------------------------------\n","   # 10.5 UNCERTAINTY ANALYSIS FOR PRODUCTION USE\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[UNCERTAINTY] Analyzing prediction uncertainty for production flagging...\")\n","\n","   # Calculate uncertainty metrics\n","   uncertainty_scores = []\n","   for i, prob_dist in enumerate(probs):\n","       # Uncertainty: -log(P(predicted class))\n","       uncertainty_scores.append(-np.log(prob_dist[y_pred_val[i]] + 1e-10))\n","\n","   uncertainty_array = np.array(uncertainty_scores)\n","\n","   print(f\"\\nPrediction Uncertainty Statistics:\")\n","   print(f\"  Mean: {uncertainty_array.mean():.4f}\")\n","   print(f\"  Median: {np.median(uncertainty_array):.4f}\")\n","   print(f\"  95th percentile: {np.percentile(uncertainty_array, 95):.4f}\")\n","   print(f\"  99th percentile: {np.percentile(uncertainty_array, 99):.4f}\")\n","\n","   # Analyze uncertainty by correctness\n","   correct_mask = y_val_array == y_pred_val\n","   print(f\"\\nUncertainty by prediction correctness:\")\n","   print(f\"  Correct predictions: mean={uncertainty_array[correct_mask].mean():.4f}\")\n","   print(f\"  Incorrect predictions: mean={uncertainty_array[~correct_mask].mean():.4f}\")\n","\n","   # Per-class calibration analysis\n","   print(\"\\n[CALIBRATION] Per-class ECE:\")\n","   for nova_class in range(5):\n","       mask = y_val_array == nova_class\n","       if mask.sum() > 10:  # Enough samples\n","           class_ece, _, _, _ = calculate_calibration_metrics(\n","               y_val_array[mask], probs[mask]\n","           )\n","           print(f\"  NOVA {nova_class}: ECE = {class_ece:.4f} (n={mask.sum()})\")\n","\n","   # Save detailed predictions for error analysis\n","   val_results = pd.DataFrame({\n","       'id_gasto': X_val['id_gasto'].values,\n","       'descripcion': X_val['descripcion'].values,\n","       'establecimiento': X_val['establecimiento'].values,\n","       'true_label': y_val_array,\n","       'predicted_label': y_pred_val,\n","       'confidence': probs.max(axis=1),\n","       'uncertainty_score': uncertainty_scores,\n","       'correct': correct_mask,\n","       'nova_0_prob': probs[:, 0],\n","       'nova_1_prob': probs[:, 1],\n","       'nova_2_prob': probs[:, 2],\n","       'nova_3_prob': probs[:, 3],\n","       'nova_4_prob': probs[:, 4]\n","   })\n","   val_results.to_csv('validation_predictions_with_uncertainty.csv', index=False)\n","   print(f\"\\n✓ Saved detailed predictions to 'validation_predictions_with_uncertainty.csv'\")\n","\n","   # Suggested threshold\n","   threshold_95 = np.percentile(uncertainty_array, 95)\n","   threshold_99 = np.percentile(uncertainty_array, 99)\n","   print(f\"\\n✓ Suggested uncertainty thresholds for review:\")\n","   print(f\"  Conservative (5% flagged): {threshold_95:.4f}\")\n","   print(f\"  Strict (1% flagged): {threshold_99:.4f}\")\n","\n","   # -------------------------------------------------------------------------\n","   # 11. TEMPERATURE SCALING\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[CALIBRATION] Applying temperature scaling...\")\n","   temp_scaler = TemperatureScaling()\n","   temp_scaler.fit(logits, y_val_array)\n","   print(f\"✓ Optimal temperature: {temp_scaler.temperature:.3f}\")\n","\n","   # Recalculate metrics with temperature scaling\n","   scaled_logits = temp_scaler.transform(logits)\n","   scaled_probs = torch.softmax(torch.tensor(scaled_logits), dim=-1).numpy()\n","\n","   # Recalculate ECE after scaling\n","   ece_scaled, _, _, _ = calculate_calibration_metrics(y_val_array, scaled_probs)\n","   print(f\"✓ ECE after temperature scaling: {ece_scaled:.4f}\")\n","\n","   # -------------------------------------------------------------------------\n","   # 12. VISUALIZATIONS\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[VISUALIZATION] Creating diagnostic plots...\")\n","\n","   # Confusion Matrix\n","   cm = confusion_matrix(y_val_array, y_pred_val)\n","   plt.figure(figsize=(8, 6))\n","   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","               xticklabels=[f'NOVA {i}' for i in range(5)],\n","               yticklabels=[f'NOVA {i}' for i in range(5)])\n","   plt.title('Validation Confusion Matrix')\n","   plt.xlabel('Predicted')\n","   plt.ylabel('True')\n","   plt.tight_layout()\n","   plt.savefig('validation_confusion_matrix_final.png', dpi=300)\n","   plt.close()\n","\n","   # Reliability Diagram\n","   plt.figure(figsize=(8, 6))\n","   bin_centers = [(lower + upper) / 2 for lower, upper in\n","                  zip(np.linspace(0, 1, 11)[:-1], np.linspace(0, 1, 11)[1:])]\n","\n","   # Before calibration\n","   plt.plot(bin_centers, bin_accs, 'o-', label='Before calibration', markersize=8)\n","   # Perfect calibration line\n","   plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n","\n","   plt.xlabel('Mean Predicted Confidence')\n","   plt.ylabel('Actual Accuracy')\n","   plt.title('Reliability Diagram')\n","   plt.legend()\n","   plt.grid(True, alpha=0.3)\n","   plt.tight_layout()\n","   plt.savefig('reliability_diagram.png', dpi=300)\n","   plt.close()\n","\n","   # Uncertainty Distribution Plot\n","   plt.figure(figsize=(10, 6))\n","\n","   # Plot for correct vs incorrect predictions\n","   plt.subplot(1, 2, 1)\n","   plt.hist(uncertainty_array[correct_mask], bins=30, alpha=0.6, label='Correct', density=True)\n","   plt.hist(uncertainty_array[~correct_mask], bins=30, alpha=0.6, label='Incorrect', density=True)\n","   plt.xlabel('Uncertainty Score')\n","   plt.ylabel('Density')\n","   plt.title('Uncertainty Distribution by Correctness')\n","   plt.legend()\n","   plt.axvline(threshold_95, color='red', linestyle='--', label=f'95% threshold: {threshold_95:.3f}')\n","\n","   # Plot for each NOVA class\n","   plt.subplot(1, 2, 2)\n","   for nova_class in range(5):\n","       mask = y_pred_val == nova_class\n","       if mask.sum() > 0:\n","           plt.hist(uncertainty_array[mask], bins=20, alpha=0.5, label=f'NOVA {nova_class}', density=True)\n","   plt.xlabel('Uncertainty Score')\n","   plt.ylabel('Density')\n","   plt.title('Uncertainty Distribution by Predicted Class')\n","   plt.legend()\n","\n","   plt.tight_layout()\n","   plt.savefig('uncertainty_distribution.png', dpi=300)\n","   plt.close()\n","\n","   print(\"✓ All plots saved\")\n","\n","   # -------------------------------------------------------------------------\n","   # 13. SAVE FINAL MODEL AND CALIBRATION\n","   # -------------------------------------------------------------------------\n","   print(\"\\n[SAVE] Saving model and calibration parameters...\")\n","\n","   # Save model and tokenizer\n","   model.save_pretrained('./beto-nova-final')\n","   tokenizer.save_pretrained('./beto-nova-final')\n","\n","   # Save comprehensive calibration parameters\n","   calibration_params = {\n","       'temperature': float(temp_scaler.temperature),\n","       'ece_before': float(ece),\n","       'ece_after': float(ece_scaled),\n","       'loss_cross_entropy_mean': float(loss_ce_array.mean()),\n","       'loss_cross_entropy_std': float(loss_ce_array.std()),\n","       'uncertainty_mean': float(uncertainty_array.mean()),\n","       'uncertainty_std': float(uncertainty_array.std()),\n","       'uncertainty_threshold_95': float(threshold_95),\n","       'uncertainty_threshold_99': float(threshold_99),\n","       'validation_accuracy': float(accuracy),\n","       'total_validation_samples': len(y_val_array),\n","       'incorrect_predictions': int((~correct_mask).sum()),\n","       'class_distribution': {\n","           f'nova_{i}': int((y_val_array == i).sum())\n","           for i in range(5)\n","       }\n","   }\n","\n","   with open('./beto-nova-final/calibration_params.json', 'w') as f:\n","       json.dump(calibration_params, f, indent=2)\n","\n","   print(f\"✓ Model saved to './beto-nova-final'\")\n","   print(f\"✓ Calibration parameters saved\")\n","\n","   # -------------------------------------------------------------------------\n","   # 14. FINAL SUMMARY\n","   # -------------------------------------------------------------------------\n","   print(\"\\n\" + \"=\"*70)\n","   print(\"TRAINING COMPLETE - SUMMARY\")\n","   print(\"=\"*70)\n","   print(f\"Validation Accuracy: {accuracy:.4f}\")\n","   print(f\"Loss Cross-entropy (mean): {loss_ce_array.mean():.4f}\")\n","   print(f\"Uncertainty score (mean): {uncertainty_array.mean():.4f}\")\n","   print(f\"ECE (before calibration): {ece:.4f}\")\n","   print(f\"ECE (after calibration): {ece_scaled:.4f}\")\n","   print(f\"Temperature scaling factor: {temp_scaler.temperature:.3f}\")\n","   print(f\"Uncertainty threshold (95%): {threshold_95:.4f}\")\n","   print(\"=\"*70)\n","\n","   return trainer, X_val, y_val, temp_scaler\n","\n","# =============================================================================\n","# MAIN EXECUTION\n","# =============================================================================\n","if __name__ == \"__main__\":\n","   # Run training with cross-validation\n","   trainer, X_val, y_val, temp_scaler = train_single_model(run_cv=True)\n","\n","   print(\"\\n✓ All training steps completed successfully!\")\n","   print(\"\\nNext steps:\")\n","   print(\"1. Review cross-validation results in 'cv_results.json'\")\n","   print(\"2. Analyze consistent errors in 'cv_consistent_errors.csv'\")\n","   print(\"3. Analyze validation errors using 'validation_predictions_with_uncertainty.csv'\")\n","   print(\"4. Only touch test set for final evaluation\")\n","   print(\"5. Prepare production pipeline with uncertainty thresholds\")\n","   print(\"6. Apply temperature scaling in production for calibrated probabilities\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"N5txRITEuKgF","executionInfo":{"status":"ok","timestamp":1753618888189,"user_tz":-60,"elapsed":630751,"user":{"displayName":"Alfredo Heufemann Peña","userId":"01230962574111998829"}},"outputId":"385b6710-b5d2-469a-b2d2-aaa308cf5915"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","BETO TRAINING FOR NOVA FOOD CLASSIFICATION\n","Chilean Household Food Purchase Analysis\n","======================================================================\n","\n","[SETUP] Using device: cuda\n","\n","[DATA] Loading preprocessed training data...\n","✓ Loaded 6000 labeled samples\n","\n","[DATA] Class distribution:\n","  NOVA 0: 44 samples (0.7%)\n","  NOVA 1: 1248 samples (20.8%)\n","  NOVA 2: 243 samples (4.0%)\n","  NOVA 3: 976 samples (16.3%)\n","  NOVA 4: 3489 samples (58.1%)\n","\n","[CROSS-VALIDATION] Running 5-fold CV to assess model stability...\n","\n","======================================================================\n","STARTING 5-FOLD CROSS-VALIDATION\n","======================================================================\n","\n","[FOLD 1/5]\n","  Train: 4800 samples\n","  Val:   1200 samples\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["  Training fold 1...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [750/750 01:42, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Weighted</th>\n","      <th>Precision Weighted</th>\n","      <th>Recall Weighted</th>\n","      <th>F1 Nova 0</th>\n","      <th>F1 Nova 1</th>\n","      <th>F1 Nova 2</th>\n","      <th>F1 Nova 3</th>\n","      <th>F1 Nova 4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.639500</td>\n","      <td>0.489882</td>\n","      <td>0.852500</td>\n","      <td>0.859165</td>\n","      <td>0.883167</td>\n","      <td>0.852500</td>\n","      <td>0.875000</td>\n","      <td>0.844444</td>\n","      <td>0.940000</td>\n","      <td>0.719192</td>\n","      <td>0.897685</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.156600</td>\n","      <td>0.558200</td>\n","      <td>0.903333</td>\n","      <td>0.904494</td>\n","      <td>0.910152</td>\n","      <td>0.903333</td>\n","      <td>0.857143</td>\n","      <td>0.892704</td>\n","      <td>0.957447</td>\n","      <td>0.822430</td>\n","      <td>0.928469</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.164600</td>\n","      <td>0.427946</td>\n","      <td>0.911667</td>\n","      <td>0.913782</td>\n","      <td>0.921818</td>\n","      <td>0.911667</td>\n","      <td>0.933333</td>\n","      <td>0.916179</td>\n","      <td>0.948454</td>\n","      <td>0.833713</td>\n","      <td>0.932635</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.106100</td>\n","      <td>0.452687</td>\n","      <td>0.935000</td>\n","      <td>0.934452</td>\n","      <td>0.934789</td>\n","      <td>0.935000</td>\n","      <td>0.933333</td>\n","      <td>0.933071</td>\n","      <td>0.959184</td>\n","      <td>0.870968</td>\n","      <td>0.950959</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.030400</td>\n","      <td>0.522383</td>\n","      <td>0.944167</td>\n","      <td>0.944043</td>\n","      <td>0.944224</td>\n","      <td>0.944167</td>\n","      <td>0.933333</td>\n","      <td>0.940945</td>\n","      <td>0.968421</td>\n","      <td>0.885417</td>\n","      <td>0.959943</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.044100</td>\n","      <td>0.453113</td>\n","      <td>0.949167</td>\n","      <td>0.949207</td>\n","      <td>0.949430</td>\n","      <td>0.949167</td>\n","      <td>0.933333</td>\n","      <td>0.948819</td>\n","      <td>0.969072</td>\n","      <td>0.892308</td>\n","      <td>0.964029</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.029300</td>\n","      <td>0.461029</td>\n","      <td>0.947500</td>\n","      <td>0.947590</td>\n","      <td>0.948067</td>\n","      <td>0.947500</td>\n","      <td>0.933333</td>\n","      <td>0.943470</td>\n","      <td>0.969072</td>\n","      <td>0.897436</td>\n","      <td>0.961733</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Fold 1 Results:\n","    Accuracy: 0.9475\n","    F1 Score: 0.9475\n","    ECE: 0.0438\n","    NOVA 3/4 errors: 35\n","\n","[FOLD 2/5]\n","  Train: 4800 samples\n","  Val:   1200 samples\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["  Training fold 2...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [750/750 01:42, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Weighted</th>\n","      <th>Precision Weighted</th>\n","      <th>Recall Weighted</th>\n","      <th>F1 Nova 0</th>\n","      <th>F1 Nova 1</th>\n","      <th>F1 Nova 2</th>\n","      <th>F1 Nova 3</th>\n","      <th>F1 Nova 4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.601500</td>\n","      <td>0.401054</td>\n","      <td>0.865833</td>\n","      <td>0.870332</td>\n","      <td>0.891557</td>\n","      <td>0.865833</td>\n","      <td>0.947368</td>\n","      <td>0.873563</td>\n","      <td>0.921348</td>\n","      <td>0.775000</td>\n","      <td>0.891473</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.199400</td>\n","      <td>0.210051</td>\n","      <td>0.923333</td>\n","      <td>0.923808</td>\n","      <td>0.925195</td>\n","      <td>0.923333</td>\n","      <td>1.000000</td>\n","      <td>0.923395</td>\n","      <td>0.989691</td>\n","      <td>0.848039</td>\n","      <td>0.939742</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.181700</td>\n","      <td>0.164993</td>\n","      <td>0.951667</td>\n","      <td>0.951716</td>\n","      <td>0.951775</td>\n","      <td>0.951667</td>\n","      <td>1.000000</td>\n","      <td>0.953908</td>\n","      <td>1.000000</td>\n","      <td>0.898477</td>\n","      <td>0.961953</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.087300</td>\n","      <td>0.178097</td>\n","      <td>0.950833</td>\n","      <td>0.950896</td>\n","      <td>0.951066</td>\n","      <td>0.950833</td>\n","      <td>1.000000</td>\n","      <td>0.961771</td>\n","      <td>0.969697</td>\n","      <td>0.901266</td>\n","      <td>0.959022</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.029600</td>\n","      <td>0.224832</td>\n","      <td>0.952500</td>\n","      <td>0.952337</td>\n","      <td>0.952361</td>\n","      <td>0.952500</td>\n","      <td>1.000000</td>\n","      <td>0.952569</td>\n","      <td>0.979167</td>\n","      <td>0.906250</td>\n","      <td>0.962751</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.027600</td>\n","      <td>0.240944</td>\n","      <td>0.960000</td>\n","      <td>0.959850</td>\n","      <td>0.959890</td>\n","      <td>0.960000</td>\n","      <td>1.000000</td>\n","      <td>0.965795</td>\n","      <td>0.989474</td>\n","      <td>0.914286</td>\n","      <td>0.967972</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.010500</td>\n","      <td>0.211991</td>\n","      <td>0.962500</td>\n","      <td>0.962539</td>\n","      <td>0.962601</td>\n","      <td>0.962500</td>\n","      <td>1.000000</td>\n","      <td>0.964143</td>\n","      <td>0.979167</td>\n","      <td>0.928934</td>\n","      <td>0.969784</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Fold 2 Results:\n","    Accuracy: 0.9633\n","    F1 Score: 0.9634\n","    ECE: 0.0281\n","    NOVA 3/4 errors: 24\n","\n","[FOLD 3/5]\n","  Train: 4800 samples\n","  Val:   1200 samples\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["  Training fold 3...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [750/750 01:42, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Weighted</th>\n","      <th>Precision Weighted</th>\n","      <th>Recall Weighted</th>\n","      <th>F1 Nova 0</th>\n","      <th>F1 Nova 1</th>\n","      <th>F1 Nova 2</th>\n","      <th>F1 Nova 3</th>\n","      <th>F1 Nova 4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.627500</td>\n","      <td>0.446308</td>\n","      <td>0.835833</td>\n","      <td>0.844881</td>\n","      <td>0.892037</td>\n","      <td>0.835833</td>\n","      <td>0.875000</td>\n","      <td>0.911197</td>\n","      <td>0.978723</td>\n","      <td>0.702403</td>\n","      <td>0.851340</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.166300</td>\n","      <td>0.221071</td>\n","      <td>0.889167</td>\n","      <td>0.892782</td>\n","      <td>0.914403</td>\n","      <td>0.889167</td>\n","      <td>0.947368</td>\n","      <td>0.923372</td>\n","      <td>0.905660</td>\n","      <td>0.796610</td>\n","      <td>0.907104</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.147700</td>\n","      <td>0.177172</td>\n","      <td>0.934167</td>\n","      <td>0.934904</td>\n","      <td>0.937714</td>\n","      <td>0.934167</td>\n","      <td>1.000000</td>\n","      <td>0.936605</td>\n","      <td>0.897196</td>\n","      <td>0.888350</td>\n","      <td>0.949054</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.086200</td>\n","      <td>0.147340</td>\n","      <td>0.951667</td>\n","      <td>0.951817</td>\n","      <td>0.952514</td>\n","      <td>0.951667</td>\n","      <td>0.947368</td>\n","      <td>0.943026</td>\n","      <td>0.960000</td>\n","      <td>0.926582</td>\n","      <td>0.961511</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.025600</td>\n","      <td>0.140469</td>\n","      <td>0.960833</td>\n","      <td>0.960880</td>\n","      <td>0.961102</td>\n","      <td>0.960833</td>\n","      <td>0.947368</td>\n","      <td>0.952569</td>\n","      <td>0.979592</td>\n","      <td>0.933333</td>\n","      <td>0.970440</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.026900</td>\n","      <td>0.133411</td>\n","      <td>0.967500</td>\n","      <td>0.967540</td>\n","      <td>0.967849</td>\n","      <td>0.967500</td>\n","      <td>0.947368</td>\n","      <td>0.960784</td>\n","      <td>0.989691</td>\n","      <td>0.943299</td>\n","      <td>0.975469</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.022300</td>\n","      <td>0.157531</td>\n","      <td>0.965000</td>\n","      <td>0.964944</td>\n","      <td>0.965093</td>\n","      <td>0.965000</td>\n","      <td>0.947368</td>\n","      <td>0.962525</td>\n","      <td>0.989691</td>\n","      <td>0.937500</td>\n","      <td>0.972003</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Fold 3 Results:\n","    Accuracy: 0.9650\n","    F1 Score: 0.9650\n","    ECE: 0.0294\n","    NOVA 3/4 errors: 21\n","\n","[FOLD 4/5]\n","  Train: 4800 samples\n","  Val:   1200 samples\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["  Training fold 4...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [750/750 01:42, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Weighted</th>\n","      <th>Precision Weighted</th>\n","      <th>Recall Weighted</th>\n","      <th>F1 Nova 0</th>\n","      <th>F1 Nova 1</th>\n","      <th>F1 Nova 2</th>\n","      <th>F1 Nova 3</th>\n","      <th>F1 Nova 4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.598000</td>\n","      <td>0.560897</td>\n","      <td>0.842500</td>\n","      <td>0.845539</td>\n","      <td>0.873522</td>\n","      <td>0.842500</td>\n","      <td>0.500000</td>\n","      <td>0.825127</td>\n","      <td>0.937500</td>\n","      <td>0.767123</td>\n","      <td>0.872727</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.182600</td>\n","      <td>0.359194</td>\n","      <td>0.922500</td>\n","      <td>0.921913</td>\n","      <td>0.921765</td>\n","      <td>0.922500</td>\n","      <td>0.823529</td>\n","      <td>0.944664</td>\n","      <td>0.970297</td>\n","      <td>0.833773</td>\n","      <td>0.936292</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.188500</td>\n","      <td>0.274091</td>\n","      <td>0.940833</td>\n","      <td>0.941829</td>\n","      <td>0.945674</td>\n","      <td>0.940833</td>\n","      <td>0.823529</td>\n","      <td>0.949698</td>\n","      <td>0.989899</td>\n","      <td>0.882629</td>\n","      <td>0.953711</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.101500</td>\n","      <td>0.364608</td>\n","      <td>0.943333</td>\n","      <td>0.943814</td>\n","      <td>0.945595</td>\n","      <td>0.943333</td>\n","      <td>0.875000</td>\n","      <td>0.950100</td>\n","      <td>0.989899</td>\n","      <td>0.890511</td>\n","      <td>0.954115</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.070500</td>\n","      <td>0.390351</td>\n","      <td>0.950833</td>\n","      <td>0.951256</td>\n","      <td>0.952369</td>\n","      <td>0.950833</td>\n","      <td>0.875000</td>\n","      <td>0.963415</td>\n","      <td>1.000000</td>\n","      <td>0.883951</td>\n","      <td>0.963283</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.039200</td>\n","      <td>0.415530</td>\n","      <td>0.945833</td>\n","      <td>0.946280</td>\n","      <td>0.947842</td>\n","      <td>0.945833</td>\n","      <td>0.875000</td>\n","      <td>0.948413</td>\n","      <td>1.000000</td>\n","      <td>0.892157</td>\n","      <td>0.957787</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.009400</td>\n","      <td>0.424421</td>\n","      <td>0.950833</td>\n","      <td>0.951051</td>\n","      <td>0.951528</td>\n","      <td>0.950833</td>\n","      <td>0.823529</td>\n","      <td>0.957576</td>\n","      <td>1.000000</td>\n","      <td>0.895000</td>\n","      <td>0.962590</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Fold 4 Results:\n","    Accuracy: 0.9508\n","    F1 Score: 0.9511\n","    ECE: 0.0416\n","    NOVA 3/4 errors: 37\n","\n","[FOLD 5/5]\n","  Train: 4800 samples\n","  Val:   1200 samples\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["  Training fold 5...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [750/750 01:42, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Weighted</th>\n","      <th>Precision Weighted</th>\n","      <th>Recall Weighted</th>\n","      <th>F1 Nova 0</th>\n","      <th>F1 Nova 1</th>\n","      <th>F1 Nova 2</th>\n","      <th>F1 Nova 3</th>\n","      <th>F1 Nova 4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.618100</td>\n","      <td>0.460340</td>\n","      <td>0.891667</td>\n","      <td>0.889010</td>\n","      <td>0.896696</td>\n","      <td>0.891667</td>\n","      <td>0.888889</td>\n","      <td>0.871508</td>\n","      <td>0.872340</td>\n","      <td>0.783133</td>\n","      <td>0.926004</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.152600</td>\n","      <td>0.259310</td>\n","      <td>0.931667</td>\n","      <td>0.931922</td>\n","      <td>0.932441</td>\n","      <td>0.931667</td>\n","      <td>0.900000</td>\n","      <td>0.935223</td>\n","      <td>0.959184</td>\n","      <td>0.866499</td>\n","      <td>0.947520</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.166000</td>\n","      <td>0.180313</td>\n","      <td>0.940833</td>\n","      <td>0.941173</td>\n","      <td>0.942171</td>\n","      <td>0.940833</td>\n","      <td>0.947368</td>\n","      <td>0.942801</td>\n","      <td>0.980000</td>\n","      <td>0.880000</td>\n","      <td>0.954876</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.077800</td>\n","      <td>0.182416</td>\n","      <td>0.958333</td>\n","      <td>0.958587</td>\n","      <td>0.959233</td>\n","      <td>0.958333</td>\n","      <td>0.947368</td>\n","      <td>0.955285</td>\n","      <td>0.989691</td>\n","      <td>0.915423</td>\n","      <td>0.969784</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.026600</td>\n","      <td>0.211083</td>\n","      <td>0.954167</td>\n","      <td>0.954184</td>\n","      <td>0.954263</td>\n","      <td>0.954167</td>\n","      <td>0.947368</td>\n","      <td>0.958084</td>\n","      <td>1.000000</td>\n","      <td>0.897436</td>\n","      <td>0.965517</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.041700</td>\n","      <td>0.168363</td>\n","      <td>0.955000</td>\n","      <td>0.955403</td>\n","      <td>0.956613</td>\n","      <td>0.955000</td>\n","      <td>0.947368</td>\n","      <td>0.960474</td>\n","      <td>1.000000</td>\n","      <td>0.900990</td>\n","      <td>0.965768</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.025200</td>\n","      <td>0.186118</td>\n","      <td>0.960000</td>\n","      <td>0.960092</td>\n","      <td>0.960304</td>\n","      <td>0.960000</td>\n","      <td>0.947368</td>\n","      <td>0.964143</td>\n","      <td>1.000000</td>\n","      <td>0.910941</td>\n","      <td>0.969741</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Fold 5 Results:\n","    Accuracy: 0.9583\n","    F1 Score: 0.9585\n","    ECE: 0.0328\n","    NOVA 3/4 errors: 31\n","\n","======================================================================\n","CROSS-VALIDATION SUMMARY\n","======================================================================\n","\n","Overall Performance (mean ± std):\n","  Accuracy: 0.9570 ± 0.0068\n","  F1 Score: 0.9571 ± 0.0068\n","  ECE:      0.0351 ± 0.0064\n","\n","Per-Class F1 Scores (mean ± std):\n","  NOVA 0: 0.9303 ± 0.0581\n","  NOVA 1: 0.9594 ± 0.0056\n","  NOVA 2: 0.9855 ± 0.0105\n","  NOVA 3: 0.9123 ± 0.0203\n","  NOVA 4: 0.9672 ± 0.0042\n","\n","NOVA 3/4 Confusion:\n","  Mean errors per fold: 29.6 ± 6.2\n","\n","[ERROR CONSISTENCY ANALYSIS]\n","\n","✓ Cross-validation complete. Results saved.\n","\n","[CONTINUING] Now training final model on full dataset...\n","\n","[SPLIT] Creating train/validation/test splits...\n","\n","[SPLIT] Saving test set for final evaluation...\n","✓ Test set saved: 900 samples\n","\n","[SPLIT] Final splits:\n","  Train: 4202 samples (70.0%)\n","  Val:   898 samples (15.0%)\n","  Test:  900 samples (15.0%)\n","\n","[MODEL] Loading BETO (Spanish BERT)...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✓ Model loaded with 109.9M parameters\n","\n","[WEIGHTS] Computing class weights for imbalanced data...\n","Class weights:\n","  NOVA 0: 28.01\n","  NOVA 1: 0.96\n","  NOVA 2: 4.94\n","  NOVA 3: 1.23\n","  NOVA 4: 0.34\n","\n","[DATASET] Creating PyTorch datasets...\n","✓ Datasets created with max_length=128 tokens\n","\n","[CONFIG] Setting up training arguments...\n","✓ Training configuration set:\n","  Warmup steps: 65\n","  Total steps: ~655\n","  Effective batch size: 32\n","\n","[TRAINER] Initializing weighted trainer...\n","\n","[TRAINING] Starting fine-tuning...\n","======================================================================\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='660' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [660/660 01:36, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Weighted</th>\n","      <th>Precision Weighted</th>\n","      <th>Recall Weighted</th>\n","      <th>F1 Nova 0</th>\n","      <th>F1 Nova 1</th>\n","      <th>F1 Nova 2</th>\n","      <th>F1 Nova 3</th>\n","      <th>F1 Nova 4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.480000</td>\n","      <td>0.271731</td>\n","      <td>0.918708</td>\n","      <td>0.917654</td>\n","      <td>0.918455</td>\n","      <td>0.918708</td>\n","      <td>1.000000</td>\n","      <td>0.913514</td>\n","      <td>0.958904</td>\n","      <td>0.842491</td>\n","      <td>0.936210</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.168600</td>\n","      <td>0.162707</td>\n","      <td>0.933185</td>\n","      <td>0.934318</td>\n","      <td>0.938837</td>\n","      <td>0.933185</td>\n","      <td>1.000000</td>\n","      <td>0.929348</td>\n","      <td>0.985915</td>\n","      <td>0.879257</td>\n","      <td>0.947059</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.183900</td>\n","      <td>0.117755</td>\n","      <td>0.949889</td>\n","      <td>0.950384</td>\n","      <td>0.952023</td>\n","      <td>0.949889</td>\n","      <td>1.000000</td>\n","      <td>0.946237</td>\n","      <td>1.000000</td>\n","      <td>0.906149</td>\n","      <td>0.960155</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.077100</td>\n","      <td>0.124054</td>\n","      <td>0.952116</td>\n","      <td>0.952579</td>\n","      <td>0.954312</td>\n","      <td>0.952116</td>\n","      <td>1.000000</td>\n","      <td>0.942779</td>\n","      <td>1.000000</td>\n","      <td>0.909677</td>\n","      <td>0.964182</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.019600</td>\n","      <td>0.142640</td>\n","      <td>0.953229</td>\n","      <td>0.953609</td>\n","      <td>0.954942</td>\n","      <td>0.953229</td>\n","      <td>1.000000</td>\n","      <td>0.941176</td>\n","      <td>1.000000</td>\n","      <td>0.918567</td>\n","      <td>0.964043</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.013800</td>\n","      <td>0.136915</td>\n","      <td>0.963252</td>\n","      <td>0.963294</td>\n","      <td>0.963394</td>\n","      <td>0.963252</td>\n","      <td>1.000000</td>\n","      <td>0.954178</td>\n","      <td>1.000000</td>\n","      <td>0.928814</td>\n","      <td>0.973180</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["======================================================================\n","\n","[EVALUATION] Running comprehensive validation analysis...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","✓ Validation Accuracy: 0.9633\n","\n","[EVALUATION] Classification Report:\n","              precision    recall  f1-score   support\n","\n","      NOVA 0       1.00      1.00      1.00         7\n","      NOVA 1       0.95      0.95      0.95       187\n","      NOVA 2       1.00      1.00      1.00        36\n","      NOVA 3       0.93      0.95      0.94       146\n","      NOVA 4       0.97      0.97      0.97       522\n","\n","    accuracy                           0.96       898\n","   macro avg       0.97      0.97      0.97       898\n","weighted avg       0.96      0.96      0.96       898\n","\n","\n","[CALIBRATION] Analyzing model calibration...\n","\n","Loss Cross-entropy statistics (for calibration):\n","  Mean: 0.1671\n","  Median: 0.0016\n","  Std: 0.9308\n","\n","Expected Calibration Error (ECE): 0.0295\n","\n","[UNCERTAINTY] Analyzing prediction uncertainty for production flagging...\n","\n","Prediction Uncertainty Statistics:\n","  Mean: 0.0107\n","  Median: 0.0016\n","  95th percentile: 0.0150\n","  99th percentile: 0.3303\n","\n","Uncertainty by prediction correctness:\n","  Correct predictions: mean=0.0072\n","  Incorrect predictions: mean=0.1037\n","\n","[CALIBRATION] Per-class ECE:\n","  NOVA 1: ECE = 0.0466 (n=187)\n","  NOVA 2: ECE = 0.0005 (n=36)\n","  NOVA 3: ECE = 0.0529 (n=146)\n","  NOVA 4: ECE = 0.0228 (n=522)\n","\n","✓ Saved detailed predictions to 'validation_predictions_with_uncertainty.csv'\n","\n","✓ Suggested uncertainty thresholds for review:\n","  Conservative (5% flagged): 0.0150\n","  Strict (1% flagged): 0.3303\n","\n","[CALIBRATION] Applying temperature scaling...\n","✓ Optimal temperature: 1.466\n","✓ ECE after temperature scaling: 0.0095\n","\n","[VISUALIZATION] Creating diagnostic plots...\n","✓ All plots saved\n","\n","[SAVE] Saving model and calibration parameters...\n","✓ Model saved to './beto-nova-final'\n","✓ Calibration parameters saved\n","\n","======================================================================\n","TRAINING COMPLETE - SUMMARY\n","======================================================================\n","Validation Accuracy: 0.9633\n","Loss Cross-entropy (mean): 0.1671\n","Uncertainty score (mean): 0.0107\n","ECE (before calibration): 0.0295\n","ECE (after calibration): 0.0095\n","Temperature scaling factor: 1.466\n","Uncertainty threshold (95%): 0.0150\n","======================================================================\n","\n","✓ All training steps completed successfully!\n","\n","Next steps:\n","1. Review cross-validation results in 'cv_results.json'\n","2. Analyze consistent errors in 'cv_consistent_errors.csv'\n","3. Analyze validation errors using 'validation_predictions_with_uncertainty.csv'\n","4. Only touch test set for final evaluation\n","5. Prepare production pipeline with uncertainty thresholds\n","6. Apply temperature scaling in production for calibrated probabilities\n"]}]},{"cell_type":"code","source":["import shutil\n","import os\n","\n","# Copy the model to the correct Google Drive\n","source = './beto-nova-final'\n","destination = '/content/drive/MyDrive/beto-nova-final'\n","\n","if os.path.exists(source):\n","    print(f\"✅ Found local model at: {source}\")\n","\n","    # Copy to Drive\n","    print(\"📤 Copying model to correct Google Drive...\")\n","    shutil.copytree(source, destination)\n","\n","    # Verify copy\n","    if os.path.exists(destination):\n","        files = os.listdir(destination)\n","        print(f\"✅ Model copied successfully!\")\n","        print(f\"📁 Files in Drive: {files}\")\n","    else:\n","        print(\"❌ Copy failed\")\n","\n","else:\n","    print(\"❌ Local model not found\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alcKC9WQASKM","executionInfo":{"status":"ok","timestamp":1753622865056,"user_tz":-60,"elapsed":934,"user":{"displayName":"Alfredo Heufemann Peña","userId":"01230962574111998829"}},"outputId":"bef3010a-6d00-4c80-d486-302f3e55f6c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Found local model at: ./beto-nova-final\n","📤 Copying model to correct Google Drive...\n","✅ Model copied successfully!\n","📁 Files in Drive: ['model.safetensors', 'vocab.txt', 'tokenizer.json', 'config.json', 'calibration_params.json', 'special_tokens_map.json', 'tokenizer_config.json']\n"]}]},{"cell_type":"code","source":["# Update the configuration with correct paths and parameters\n","class Config:\n","    \"\"\"Production pipeline configuration \"\"\"\n","\n","    # Model paths (now correct)\n","    MODEL_PATH = '/content/drive/MyDrive/beto-nova-final'\n","    CALIBRATION_PATH = '/content/drive/MyDrive/beto-nova-final/calibration_params.json'\n","\n","    # Data paths - you still need to upload EPF data\n","    EPF_DATA_PATH = 'base-cantidades-quintilizada-ix-epf-(stata).dta'\n","    OUTPUT_DIR = './production_results/'\n","\n","    # Processing parameters (updated from your training)\n","    BATCH_SIZE = 32  # Same as training\n","    MAX_LENGTH = 128\n","    UNCERTAINTY_THRESHOLD = 0.015  # From your training (95th percentile)\n","    TEMPERATURE_SCALING = 1.4663553695303932  # From your training\n","\n","    # Checkpointing\n","    CHECKPOINT_EVERY = 50000\n","\n","    # Required columns from EPF\n","    EPF_COLUMNS = ['id_gasto', 'folio', 'ccif', 'quintil', 'descripcion_gasto', 'establecimiento']\n","\n","# Test the configuration\n","config = Config()\n","\n","print(\"✅ Configuration updated:\")\n","print(f\"   Model path: {config.MODEL_PATH}\")\n","print(f\"   Model exists: {os.path.exists(config.MODEL_PATH)}\")\n","print(f\"   Calibration exists: {os.path.exists(config.CALIBRATION_PATH)}\")\n","print(f\"   Temperature: {config.TEMPERATURE_SCALING}\")\n","print(f\"   Uncertainty threshold: {config.UNCERTAINTY_THRESHOLD}\")\n","print(f\"   Batch size: {config.BATCH_SIZE}\")\n","\n","print(f\"\\n🎯 Ready to proceed with EPF data upload!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Akdx5jupGOSL","executionInfo":{"status":"ok","timestamp":1753623359393,"user_tz":-60,"elapsed":40,"user":{"displayName":"Alfredo Heufemann Peña","userId":"01230962574111998829"}},"outputId":"ae52639d-be6f-4e8f-d448-c102e1622fe8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Configuration updated:\n","   Model path: /content/drive/MyDrive/beto-nova-final\n","   Model exists: True\n","   Calibration exists: True\n","   Temperature: 1.4663553695303932\n","   Uncertainty threshold: 0.015\n","   Batch size: 32\n","\n","🎯 Ready to proceed with EPF data upload!\n"]}]},{"cell_type":"code","source":["\"\"\"\n","NOVA ERROR ANALYSIS\n","Show all specific examples to understand patterns across ALL NOVA categories\n","\"\"\"\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","\n","# ==============================================================================\n","# LOAD AND ANALYZE ALL NOVA ERRORS\n","# ==============================================================================\n","print(\"=\"*70)\n","print(\"COMPREHENSIVE NOVA ERROR ANALYSIS\")\n","print(\"=\"*70)\n","\n","# Load predictions\n","val_results = pd.read_csv('validation_predictions_with_uncertainty.csv')\n","\n","# Overall error analysis\n","total_errors = val_results[val_results['true_label'] != val_results['predicted_label']]\n","total_correct = val_results[val_results['true_label'] == val_results['predicted_label']]\n","\n","print(f\"Total samples: {len(val_results)}\")\n","print(f\"Total correct: {len(total_correct)} ({len(total_correct)/len(val_results)*100:.1f}%)\")\n","print(f\"Total errors: {len(total_errors)} ({len(total_errors)/len(val_results)*100:.1f}%)\")\n","\n","\n","# ==============================================================================\n","# NOVA 1 ERROR ANALYSIS\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"NOVA 1 ERROR ANALYSIS\")\n","print(\"=\"*70)\n","\n","nova1_errors = val_results[(val_results['true_label'] == 0) &\n","                          (val_results['predicted_label'] != 0)]\n","\n","nova1_total = (val_results['true_label'] == 0).sum()\n","print(f\"Total NOVA 1 errors: {len(nova1_errors)}\")\n","print(f\"NOVA 1 total items: {nova1_total}\")\n","print(f\"Error rate: {len(nova1_errors) / nova1_total * 100:.1f}%\")\n","\n","if len(nova1_errors) > 0:\n","    # Break down by prediction\n","    nova1_to_others = nova1_errors.groupby('predicted_label').size().sort_index()\n","    print(f\"\\nNOVA 1 error breakdown:\")\n","    for pred_nova, count in nova1_to_others.items():\n","        print(f\"  NOVA 1 → NOVA {int(pred_nova)}: {count} errors\")\n","\n","    print(f\"\\nTop NOVA 1 errors (highest confidence):\")\n","    for i, (_, row) in enumerate(nova1_errors.sort_values('confidence', ascending=False).head(10).iterrows(), 1):\n","        desc = row['descripcion']\n","        est = row['establecimiento']\n","        conf = row['confidence']\n","        pred = int(row['predicted_label'])\n","        print(f\"{i:2d}. {desc} | {est}\")\n","        print(f\"    Confidence: {conf:.1%} → NOVA {pred}\")\n","        print()\n","\n","# ==============================================================================\n","# NOVA 2 ERROR ANALYSIS\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"NOVA 2 ERROR ANALYSIS\")\n","print(\"=\"*70)\n","\n","nova2_errors = val_results[(val_results['true_label'] == 1) &\n","                          (val_results['predicted_label'] != 1)]\n","\n","nova2_total = (val_results['true_label'] == 1).sum()\n","print(f\"Total NOVA 2 errors: {len(nova2_errors)}\")\n","print(f\"NOVA 2 total items: {nova2_total}\")\n","print(f\"Error rate: {len(nova2_errors) / nova2_total * 100:.1f}%\")\n","\n","if len(nova2_errors) > 0:\n","    # Break down by prediction\n","    nova2_to_others = nova2_errors.groupby('predicted_label').size().sort_index()\n","    print(f\"\\nNOVA 2 error breakdown:\")\n","    for pred_nova, count in nova2_to_others.items():\n","        print(f\"  NOVA 2 → NOVA {int(pred_nova)}: {count} errors\")\n","\n","    print(f\"\\nAll NOVA 2 errors:\")\n","    for i, (_, row) in enumerate(nova2_errors.sort_values('confidence', ascending=False).iterrows(), 1):\n","        desc = row['descripcion']\n","        est = row['establecimiento']\n","        conf = row['confidence']\n","        pred = int(row['predicted_label'])\n","        print(f\"{i:2d}. {desc} | {est}\")\n","        print(f\"    Confidence: {conf:.1%} → NOVA {pred}\")\n","        print()\n","\n","# ==============================================================================\n","# NOVA 3 ERROR ANALYSIS (ENHANCED)\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"NOVA 3 ERROR ANALYSIS\")\n","print(\"=\"*70)\n","\n","nova3_errors = val_results[(val_results['true_label'] == 2) &\n","                          (val_results['predicted_label'] != 2)]\n","\n","nova3_total = (val_results['true_label'] == 2).sum()\n","print(f\"Total NOVA 3 errors: {len(nova3_errors)}\")\n","print(f\"NOVA 3 total items: {nova3_total}\")\n","print(f\"Error rate: {len(nova3_errors) / nova3_total * 100:.1f}%\")\n","\n","# Break down by prediction type\n","nova3_to_others = nova3_errors.groupby('predicted_label').size().sort_index()\n","print(f\"\\nNOVA 3 error breakdown:\")\n","for pred_nova, count in nova3_to_others.items():\n","    print(f\"  NOVA 3 → NOVA {int(pred_nova)}: {count} errors\")\n","\n","# NOVA 3 → NOVA 4 errors (most common)\n","nova3_to_4 = nova3_errors[nova3_errors['predicted_label'] == 3].sort_values('confidence', ascending=False)\n","if len(nova3_to_4) > 0:\n","    print(f\"\\nNOVA 3 → NOVA 4 ERRORS ({len(nova3_to_4)} items):\")\n","    print(\"Format: DESCRIPCION | ESTABLECIMIENTO (confidence)\")\n","    print(\"-\" * 70)\n","\n","    for i, (_, row) in enumerate(nova3_to_4.iterrows(), 1):\n","        desc = row['descripcion']\n","        est = row['establecimiento']\n","        conf = row['confidence']\n","        print(f\"{i:2d}. {desc} | {est}\")\n","        print(f\"    Confidence: {conf:.1%}\")\n","        print()\n","\n","# NOVA 3 → NOVA 1 errors\n","nova3_to_1 = nova3_errors[nova3_errors['predicted_label'] == 0].sort_values('confidence', ascending=False)\n","if len(nova3_to_1) > 0:\n","    print(f\"\\nNOVA 3 → NOVA 1 ERRORS ({len(nova3_to_1)} items):\")\n","    for i, (_, row) in enumerate(nova3_to_1.iterrows(), 1):\n","        desc = row['descripcion']\n","        est = row['establecimiento']\n","        conf = row['confidence']\n","        print(f\"{i}. {desc} | {est}\")\n","        print(f\"   Confidence: {conf:.1%}\")\n","        print()\n","\n","# ==============================================================================\n","# NOVA 4 ERROR ANALYSIS\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"NOVA 4 ERROR ANALYSIS\")\n","print(\"=\"*70)\n","\n","nova4_errors = val_results[(val_results['true_label'] == 3) &\n","                          (val_results['predicted_label']!= 3)]\n","\n","nova4_total = (val_results['true_label'] == 3).sum()\n","print(f\"Total NOVA 4 errors: {len(nova4_errors)}\")\n","print(f\"NOVA 4 total items: {nova4_total}\")\n","print(f\"Error rate: {len(nova4_errors) / nova4_total * 100:.1f}%\")\n","\n","if len(nova4_errors) > 0:\n","    # Break down by prediction\n","    nova4_to_others = nova4_errors.groupby('predicted_label').size().sort_index()\n","    print(f\"\\nNOVA 4 error breakdown:\")\n","    for pred_nova, count in nova4_to_others.items():\n","        print(f\"  NOVA 4 → NOVA {int(pred_nova)}: {count} errors\")\n","\n","    print(f\"\\nTop NOVA 4 errors (highest confidence):\")\n","    for i, (_, row) in enumerate(nova4_errors.sort_values('confidence', ascending=False).head(15).iterrows(), 1):\n","        desc = row['descripcion']\n","        est = row['establecimiento']\n","        conf = row['confidence']\n","        pred = int(row['predicted_label'])\n","        print(f\"{i:2d}. {desc} | {est}\")\n","        print(f\"    Confidence: {conf:.1%} → NOVA {pred}\")\n","        print()\n","\n","# ==============================================================================\n","# CROSS-CATEGORY CONFUSION ANALYSIS\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"CONFUSION MATRIX ANALYSIS\")\n","print(\"=\"*70)\n","\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","\n","# Create confusion matrix\n","cm = confusion_matrix(val_results['true_label'], val_results['predicted_label'])\n","cm_df = pd.DataFrame(cm,\n","                    index=[f'True NOVA {i}' for i in range(5)],\n","                    columns=[f'Pred NOVA {i}' for i in range(5)])\n","\n","print(\"Confusion Matrix:\")\n","print(cm_df)\n","\n","# Most problematic confusions\n","print(f\"\\nMost problematic confusions (off-diagonal):\")\n","confusions = []\n","for i in range(4):\n","    for j in range(4):\n","        if i != j and cm[i,j] > 0:\n","            confusions.append((cm[i,j], f\"NOVA {i} → NOVA {j}\"))\n","\n","confusions.sort(reverse=True)\n","for count, confusion_type in confusions[:10]:\n","    print(f\"  {confusion_type}: {count} errors\")\n","\n","# ==============================================================================\n","# HIGH CONFIDENCE ERROR SUMMARY\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"HIGH CONFIDENCE ERRORS SUMMARY (>95%)\")\n","print(\"=\"*70)\n","\n","high_conf_errors = total_errors[total_errors['confidence'] > 0.95]\n","print(f\"Total high confidence errors: {len(high_conf_errors)}\")\n","\n","if len(high_conf_errors) > 0:\n","    print(f\"\\nHigh confidence errors by type:\")\n","    error_types = high_conf_errors.apply(lambda row: f\"NOVA {int(row['true_label'])} → NOVA {int(row['predicted_label'])}\", axis=1)\n","    error_type_counts = error_types.value_counts()\n","\n","    for error_type, count in error_type_counts.items():\n","        print(f\"  {error_type}: {count} errors\")\n","\n","    print(f\"\\nTop 10 high confidence errors (systematic mistakes):\")\n","    for i, (_, row) in enumerate(high_conf_errors.sort_values('confidence', ascending=False).head(10).iterrows(), 1):\n","        desc = row['descripcion']\n","        est = row['establecimiento']\n","        conf = row['confidence']\n","        true_label = int(row['true_label'])\n","        pred_label = int(row['predicted_label'])\n","        print(f\"{i:2d}. {desc} | {est}\")\n","        print(f\"    NOVA {true_label} → NOVA {pred_label} (Confidence: {conf:.1%})\")\n","        print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zg9vPjWOuOB_","executionInfo":{"status":"ok","timestamp":1753619283973,"user_tz":-60,"elapsed":152,"user":{"displayName":"Alfredo Heufemann Peña","userId":"01230962574111998829"}},"outputId":"24156a44-260b-443b-c396-0539068b0481"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","COMPREHENSIVE NOVA ERROR ANALYSIS\n","======================================================================\n","Total samples: 898\n","Total correct: 865 (96.3%)\n","Total errors: 33 (3.7%)\n","\n","======================================================================\n","NOVA 1 ERROR ANALYSIS\n","======================================================================\n","Total NOVA 1 errors: 0\n","NOVA 1 total items: 7\n","Error rate: 0.0%\n","\n","======================================================================\n","NOVA 2 ERROR ANALYSIS\n","======================================================================\n","Total NOVA 2 errors: 10\n","NOVA 2 total items: 187\n","Error rate: 5.3%\n","\n","NOVA 2 error breakdown:\n","  NOVA 2 → NOVA 3: 3 errors\n","  NOVA 2 → NOVA 4: 7 errors\n","\n","All NOVA 2 errors:\n"," 1. LIMON | SUPERM S.I.\n","    Confidence: 99.8% → NOVA 4\n","\n"," 2. CHAMPIÑONES | SANTA ISABEL\n","    Confidence: 99.8% → NOVA 4\n","\n"," 3. CHAMPIÑONES LAMINADOS 200 GR | LIDER\n","    Confidence: 99.6% → NOVA 4\n","\n"," 4. LENTEJAS ENCUESADA | SUPUNIMARC\n","    Confidence: 98.9% → NOVA 4\n","\n"," 5. BOLSA HABAS CONGELADAS | SUPERMERCADO LIDER\n","    Confidence: 98.7% → NOVA 4\n","\n"," 6. FILETES DE SALMON CONGELADOS | COMERCIALIZADORA RIVAMAR\n","    Confidence: 98.0% → NOVA 3\n","\n"," 7. LECHE PARA EL CAFE | CAFETERIA\n","    Confidence: 96.0% → NOVA 4\n","\n"," 8. NESCAFE TRADICION PARA SERVIR | SHELL\n","    Confidence: 88.7% → NOVA 4\n","\n"," 9. PESCADO | FERIA\n","    Confidence: 72.7% → NOVA 3\n","\n","10. PEPINO ENSALADA FRESCO | SUPERMERCADO\n","    Confidence: 50.0% → NOVA 3\n","\n","\n","======================================================================\n","NOVA 3 ERROR ANALYSIS\n","======================================================================\n","Total NOVA 3 errors: 0\n","NOVA 3 total items: 36\n","Error rate: 0.0%\n","\n","NOVA 3 error breakdown:\n","\n","======================================================================\n","NOVA 4 ERROR ANALYSIS\n","======================================================================\n","Total NOVA 4 errors: 8\n","NOVA 4 total items: 146\n","Error rate: 5.5%\n","\n","NOVA 4 error breakdown:\n","  NOVA 4 → NOVA 1: 2 errors\n","  NOVA 4 → NOVA 4: 6 errors\n","\n","Top NOVA 4 errors (highest confidence):\n"," 1. CARPACHO FRIO | RESTAURANT KURANEPE\n","    Confidence: 99.7% → NOVA 1\n","\n"," 2. PAN CON QUESILLO, LISTO PARA CONSUMIR | SANGUCHERIA\n","    Confidence: 99.7% → NOVA 4\n","\n"," 3. QUESO FRESCO VACA TROZO | SUPERMERCADO\n","    Confidence: 99.6% → NOVA 1\n","\n"," 4. ALMUERXO X3 | LOTUS FLAVIOR RESTAURANT\n","    Confidence: 99.1% → NOVA 4\n","\n"," 5. ALMUERZO | EBERZER\n","    Confidence: 95.5% → NOVA 4\n","\n"," 6. TRAGOS MAS TABLA DE EMPANADAS LISTO PARA SERVIR | FERIA RESTAURANT\n","    Confidence: 87.6% → NOVA 4\n","\n"," 7. TRAGOS MAS TABLA DE EMPANADAS LISTO PARA SERVIR | FERIA RESTAURANT\n","    Confidence: 87.6% → NOVA 4\n","\n"," 8. COMIDA PARA SERVIR | BANANA ROCKPAPE\n","    Confidence: 65.6% → NOVA 4\n","\n","\n","======================================================================\n","CONFUSION MATRIX ANALYSIS\n","======================================================================\n","Confusion Matrix:\n","             Pred NOVA 0  Pred NOVA 1  Pred NOVA 2  Pred NOVA 3  Pred NOVA 4\n","True NOVA 0            7            0            0            0            0\n","True NOVA 1            0          177            0            3            7\n","True NOVA 2            0            0           36            0            0\n","True NOVA 3            0            2            0          138            6\n","True NOVA 4            0            7            0            8          507\n","\n","Most problematic confusions (off-diagonal):\n","  NOVA 1 → NOVA 3: 3 errors\n","  NOVA 3 → NOVA 1: 2 errors\n","\n","======================================================================\n","HIGH CONFIDENCE ERRORS SUMMARY (>95%)\n","======================================================================\n","Total high confidence errors: 22\n","\n","High confidence errors by type:\n","  NOVA 1 → NOVA 4: 6 errors\n","  NOVA 4 → NOVA 3: 5 errors\n","  NOVA 4 → NOVA 1: 5 errors\n","  NOVA 3 → NOVA 4: 3 errors\n","  NOVA 3 → NOVA 1: 2 errors\n","  NOVA 1 → NOVA 3: 1 errors\n","\n","Top 10 high confidence errors (systematic mistakes):\n"," 1. AGUA MINERAL CON GAS (NARANJA) | SUPER LIDER\n","    NOVA 4 → NOVA 1 (Confidence: 99.9%)\n","\n"," 2. FANTA | LIDER\n","    NOVA 4 → NOVA 1 (Confidence: 99.9%)\n","\n"," 3. CAFE DESCAFEINADO (INSTANTANEO EN POLVO) | SUPERMERCADO ACUENTA\n","    NOVA 4 → NOVA 1 (Confidence: 99.9%)\n","\n"," 4. LIMON | SUPERM S.I.\n","    NOVA 1 → NOVA 4 (Confidence: 99.8%)\n","\n"," 5. CAFE LATE | ESTACION DE SERVICIO SHELL\n","    NOVA 4 → NOVA 1 (Confidence: 99.8%)\n","\n"," 6. ALMUERZO (SOPA+ENSALADA+JUGO+PLATO DE FONDO+ POSTRE) | CASINO\n","    NOVA 4 → NOVA 3 (Confidence: 99.8%)\n","\n"," 7. CHAMPIÑONES | SANTA ISABEL\n","    NOVA 1 → NOVA 4 (Confidence: 99.8%)\n","\n"," 8. CHURRASCO+BEBIDA | JUAN MAESTRO\n","    NOVA 4 → NOVA 3 (Confidence: 99.7%)\n","\n"," 9. CARPACHO FRIO | RESTAURANT KURANEPE\n","    NOVA 3 → NOVA 1 (Confidence: 99.7%)\n","\n","10. PAN CON QUESILLO, LISTO PARA CONSUMIR | SANGUCHERIA\n","    NOVA 3 → NOVA 4 (Confidence: 99.7%)\n","\n"]}]},{"cell_type":"code","source":["\"\"\"\n","=============================================================================\n","TEST SET EVALUATION - FINAL MODEL PERFORMANCE\n","BETO NOVA Classification - Chilean Food Dataset\n","\n","This script evaluates the trained model on the held-out test set to provide\n","unbiased performance estimates for thesis reporting.\n","=============================================================================\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import json\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from torch.utils.data import Dataset, DataLoader\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =============================================================================\n","# DATASET CLASS (Same as training)\n","# =============================================================================\n","class NOVADataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=128):\n","        self.texts = texts.values if hasattr(texts, 'values') else texts\n","        self.labels = labels.values if hasattr(labels, 'values') else labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer(\n","            text,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# =============================================================================\n","# CALIBRATION FUNCTIONS (Same as training)\n","# =============================================================================\n","def calculate_calibration_metrics(y_true, y_pred_proba, n_bins=10):\n","    \"\"\"Calculate Expected Calibration Error (ECE)\"\"\"\n","    y_pred = np.argmax(y_pred_proba, axis=1)\n","    confidences = np.max(y_pred_proba, axis=1)\n","    accuracies = (y_pred == y_true).astype(float)\n","\n","    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n","    bin_lowers = bin_boundaries[:-1]\n","    bin_uppers = bin_boundaries[1:]\n","\n","    ece = 0\n","    bin_accuracies = []\n","    bin_confidences = []\n","    bin_counts = []\n","\n","    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n","        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n","        prop_in_bin = in_bin.mean()\n","\n","        if prop_in_bin > 0:\n","            accuracy_in_bin = accuracies[in_bin].mean()\n","            avg_confidence_in_bin = confidences[in_bin].mean()\n","            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n","\n","            bin_accuracies.append(accuracy_in_bin)\n","            bin_confidences.append(avg_confidence_in_bin)\n","            bin_counts.append(in_bin.sum())\n","        else:\n","            bin_accuracies.append(0)\n","            bin_confidences.append(0)\n","            bin_counts.append(0)\n","\n","    return ece, bin_accuracies, bin_confidences, bin_counts\n","\n","def analyze_test_errors(df, top_n=10):\n","    \"\"\"Analyze test set errors in detail\"\"\"\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"COMPREHENSIVE TEST SET ERROR ANALYSIS\")\n","    print(\"=\"*70)\n","\n","    errors = df[df['correct'] == False].copy()\n","\n","    print(f\"Total samples: {len(df)}\")\n","    print(f\"Total correct: {(df['correct'] == True).sum()} ({(df['correct'] == True).mean()*100:.1f}%)\")\n","    print(f\"Total errors: {len(errors)} ({len(errors)/len(df)*100:.1f}%)\")\n","\n","    if len(errors) == 0:\n","        print(\"\\n🎉 PERFECT PERFORMANCE - NO ERRORS!\")\n","        return\n","\n","    # Error breakdown by true class\n","    for nova_class in sorted(df['true_label'].unique()):\n","        class_data = df[df['true_label'] == nova_class]\n","        class_errors = errors[errors['true_label'] == nova_class]\n","        error_rate = len(class_errors) / len(class_data) * 100 if len(class_data) > 0 else 0\n","\n","        print(f\"\\n\" + \"=\"*70)\n","        print(f\"NOVA {nova_class} ERROR ANALYSIS\")\n","        print(\"=\"*70)\n","        print(f\"Total NOVA {nova_class} errors: {len(class_errors)}\")\n","        print(f\"NOVA {nova_class} total items: {len(class_data)}\")\n","        print(f\"Error rate: {error_rate:.1f}%\")\n","\n","        if len(class_errors) > 0:\n","            # Error breakdown by predicted class\n","            error_breakdown = class_errors['predicted_label'].value_counts()\n","            print(f\"\\nNOVA {nova_class} error breakdown:\")\n","            for pred_class, count in error_breakdown.items():\n","                print(f\"  NOVA {nova_class} → NOVA {pred_class}: {count} errors\")\n","\n","            # Show specific errors\n","            if len(class_errors) <= 15:  # Show all if few errors\n","                print(f\"\\nAll NOVA {nova_class} errors:\")\n","                for idx, (_, row) in enumerate(class_errors.iterrows(), 1):\n","                    print(f\"{idx:2d}. {row['descripcion']} | {row['establecimiento']}\")\n","                    print(f\"    Confidence: {row['confidence']*100:.1f}% → NOVA {row['predicted_label']}\")\n","            else:  # Show top confidence errors\n","                top_errors = class_errors.nlargest(top_n, 'confidence')\n","                print(f\"\\nTop {top_n} NOVA {nova_class} errors (highest confidence):\")\n","                for idx, (_, row) in enumerate(top_errors.iterrows(), 1):\n","                    print(f\"{idx:2d}. {row['descripcion']} | {row['establecimiento']}\")\n","                    print(f\"    Confidence: {row['confidence']*100:.1f}% → NOVA {row['predicted_label']}\")\n","\n","    # Confusion matrix analysis\n","    print(f\"\\n\" + \"=\"*70)\n","    print(\"CONFUSION MATRIX ANALYSIS\")\n","    print(\"=\"*70)\n","    cm = confusion_matrix(df['true_label'], df['predicted_label'])\n","    print(\"Confusion Matrix:\")\n","\n","    # Create labeled confusion matrix\n","    classes = sorted(df['true_label'].unique())\n","    cm_df = pd.DataFrame(cm,\n","                        index=[f'True NOVA {i}' for i in classes],\n","                        columns=[f'Pred NOVA {i}' for i in classes])\n","    print(cm_df)\n","\n","    # Most problematic confusions\n","    print(f\"\\nMost problematic confusions (off-diagonal):\")\n","    for i in range(len(classes)):\n","        for j in range(len(classes)):\n","            if i != j and cm[i,j] > 0:\n","                print(f\"  NOVA {classes[i]} → NOVA {classes[j]}: {cm[i,j]} errors\")\n","\n","    # High confidence errors\n","    high_conf_errors = errors[errors['confidence'] > 0.95]\n","    print(f\"\\n\" + \"=\"*70)\n","    print(f\"HIGH CONFIDENCE ERRORS SUMMARY (>95%)\")\n","    print(\"=\"*70)\n","    print(f\"Total high confidence errors: {len(high_conf_errors)}\")\n","\n","    if len(high_conf_errors) > 0:\n","        # Breakdown by error type\n","        high_conf_breakdown = high_conf_errors.groupby(['true_label', 'predicted_label']).size()\n","        print(f\"\\nHigh confidence errors by type:\")\n","        for (true_label, pred_label), count in high_conf_breakdown.items():\n","            print(f\"  NOVA {true_label} → NOVA {pred_label}: {count} errors\")\n","\n","        # Show top high confidence errors\n","        top_high_conf = high_conf_errors.nlargest(min(10, len(high_conf_errors)), 'confidence')\n","        print(f\"\\nTop {len(top_high_conf)} high confidence errors (systematic mistakes):\")\n","        for idx, (_, row) in enumerate(top_high_conf.iterrows(), 1):\n","            print(f\"{idx:2d}. {row['descripcion']} | {row['establecimiento']}\")\n","            print(f\"    NOVA {row['true_label']} → NOVA {row['predicted_label']} (Confidence: {row['confidence']*100:.1f}%)\")\n","\n","# =============================================================================\n","# MAIN TEST SET EVALUATION FUNCTION\n","# =============================================================================\n","def evaluate_test_set():\n","    \"\"\"\n","    Comprehensive test set evaluation with calibrated model\n","    \"\"\"\n","    print(\"=\"*70)\n","    print(\"TEST SET EVALUATION - FINAL MODEL PERFORMANCE\")\n","    print(\"=\"*70)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"\\n[SETUP] Using device: {device}\")\n","\n","    # -------------------------------------------------------------------------\n","    # 1. LOAD TEST SET\n","    # -------------------------------------------------------------------------\n","    print(\"\\n[DATA] Loading test set...\")\n","    test_df = pd.read_csv('TEST_SET_DO_NOT_TOUCH.csv')\n","    print(f\"✓ Loaded {len(test_df)} test samples\")\n","\n","    # Show test set distribution\n","    print(f\"\\n[DATA] Test set class distribution:\")\n","    for nova_class, count in test_df['NOVA'].value_counts().sort_index().items():\n","        percentage = (count / len(test_df)) * 100\n","        print(f\"  NOVA {nova_class}: {count} samples ({percentage:.1f}%)\")\n","\n","    # -------------------------------------------------------------------------\n","    # 2. LOAD TRAINED MODEL AND CALIBRATION PARAMETERS\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[MODEL] Loading trained model and calibration parameters...\")\n","\n","    # Load model and tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained('./beto-nova-final')\n","    model = AutoModelForSequenceClassification.from_pretrained('./beto-nova-final')\n","    model.to(device)\n","    model.eval()\n","    print(f\"✓ Model loaded\")\n","\n","    # Load calibration parameters\n","    with open('./beto-nova-final/calibration_params.json', 'r') as f:\n","        calibration_params = json.load(f)\n","\n","    temperature = calibration_params['temperature']\n","    val_uncertainty_threshold = calibration_params['uncertainty_threshold_95']\n","\n","    print(f\"✓ Calibration parameters loaded:\")\n","    print(f\"  Temperature: {temperature:.3f}\")\n","    print(f\"  Validation uncertainty threshold (95%): {val_uncertainty_threshold:.4f}\")\n","\n","    # -------------------------------------------------------------------------\n","    # 3. CREATE TEST DATASET AND MAKE PREDICTIONS\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[PREDICTION] Running inference on test set...\")\n","\n","    # Prepare data\n","    X_test = test_df['combined_text']\n","    y_test = test_df['NOVA']\n","\n","    # Create dataset\n","    test_dataset = NOVADataset(X_test, y_test, tokenizer)\n","    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","    # Make predictions\n","    all_logits = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels']\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","\n","            all_logits.append(logits.cpu())\n","            all_labels.append(labels)\n","\n","    # Combine results\n","    all_logits = torch.cat(all_logits, dim=0).numpy()\n","    all_labels = torch.cat(all_labels, dim=0).numpy()\n","\n","    print(f\"✓ Predictions complete\")\n","\n","    # -------------------------------------------------------------------------\n","    # 4. APPLY TEMPERATURE SCALING (CALIBRATION)\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[CALIBRATION] Applying temperature scaling...\")\n","\n","    # Apply temperature scaling\n","    scaled_logits = all_logits / temperature\n","\n","    # Calculate probabilities\n","    raw_probs = torch.softmax(torch.tensor(all_logits), dim=-1).numpy()\n","    calibrated_probs = torch.softmax(torch.tensor(scaled_logits), dim=-1).numpy()\n","\n","    # Get predictions\n","    y_pred = np.argmax(calibrated_probs, axis=1)\n","    confidences = np.max(calibrated_probs, axis=1)\n","\n","    print(f\"✓ Temperature scaling applied\")\n","\n","    # -------------------------------------------------------------------------\n","    # 5. CALCULATE PERFORMANCE METRICS\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[EVALUATION] Calculating performance metrics...\")\n","\n","    # Basic metrics\n","    accuracy = accuracy_score(all_labels, y_pred)\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        all_labels, y_pred, average='weighted', zero_division=0\n","    )\n","\n","    # Per-class metrics\n","    f1_per_class = precision_recall_fscore_support(\n","        all_labels, y_pred, average=None, zero_division=0\n","    )[2]\n","\n","    print(f\"\\n✓ TEST SET PERFORMANCE:\")\n","    print(f\"  Accuracy: {accuracy:.4f}\")\n","    print(f\"  Weighted F1: {f1:.4f}\")\n","    print(f\"  Weighted Precision: {precision:.4f}\")\n","    print(f\"  Weighted Recall: {recall:.4f}\")\n","\n","    print(f\"\\n✓ PER-CLASS F1 SCORES:\")\n","    for i, f1_score in enumerate(f1_per_class):\n","        print(f\"  NOVA {i}: {f1_score:.4f}\")\n","\n","    # -------------------------------------------------------------------------\n","    # 6. CALIBRATION ANALYSIS\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[CALIBRATION] Analyzing calibration on test set...\")\n","\n","    # Calculate ECE before and after calibration\n","    ece_raw, _, _, _ = calculate_calibration_metrics(all_labels, raw_probs)\n","    ece_calibrated, bin_accs, bin_confs, bin_counts = calculate_calibration_metrics(\n","        all_labels, calibrated_probs\n","    )\n","\n","    print(f\"  ECE (before calibration): {ece_raw:.4f}\")\n","    print(f\"  ECE (after calibration): {ece_calibrated:.4f}\")\n","    print(f\"  Calibration improvement: {ece_raw - ece_calibrated:.4f}\")\n","\n","    # -------------------------------------------------------------------------\n","    # 7. UNCERTAINTY ANALYSIS\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[UNCERTAINTY] Analyzing prediction uncertainty...\")\n","\n","    # Calculate uncertainty scores\n","    uncertainty_scores = -np.log(confidences + 1e-10)\n","\n","    # Uncertainty statistics\n","    correct_mask = (all_labels == y_pred)\n","\n","    print(f\"  Mean uncertainty: {uncertainty_scores.mean():.4f}\")\n","    print(f\"  Median uncertainty: {np.median(uncertainty_scores):.4f}\")\n","    print(f\"  95th percentile: {np.percentile(uncertainty_scores, 95):.4f}\")\n","    print(f\"  99th percentile: {np.percentile(uncertainty_scores, 99):.4f}\")\n","\n","    print(f\"\\n  Uncertainty by correctness:\")\n","    print(f\"    Correct predictions: {uncertainty_scores[correct_mask].mean():.4f}\")\n","    print(f\"    Incorrect predictions: {uncertainty_scores[~correct_mask].mean():.4f}\")\n","\n","    # Check if validation threshold works on test set\n","    test_threshold_95 = np.percentile(uncertainty_scores, 95)\n","    flagged_by_val_threshold = (uncertainty_scores > val_uncertainty_threshold).sum()\n","\n","    print(f\"\\n  Threshold validation:\")\n","    print(f\"    Validation threshold (95%): {val_uncertainty_threshold:.4f}\")\n","    print(f\"    Test set threshold (95%): {test_threshold_95:.4f}\")\n","    print(f\"    Items flagged by val threshold: {flagged_by_val_threshold} ({flagged_by_val_threshold/len(test_df)*100:.1f}%)\")\n","\n","    # -------------------------------------------------------------------------\n","    # 8. DETAILED CLASSIFICATION REPORT\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[EVALUATION] Detailed classification report:\")\n","    print(\"\\n\" + classification_report(\n","        all_labels, y_pred,\n","        target_names=[f'NOVA {i}' for i in range(5)],\n","        digits=4\n","    ))\n","\n","    # -------------------------------------------------------------------------\n","    # 9. SAVE DETAILED RESULTS\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[SAVE] Saving detailed test results...\")\n","\n","    # Create detailed results dataframe\n","    test_results = pd.DataFrame({\n","        'id_gasto': test_df['id_gasto'].values,\n","        'descripcion': test_df['descripcion'].values,\n","        'establecimiento': test_df['establecimiento'].values,\n","        'combined_text': test_df['combined_text'].values,\n","        'true_label': all_labels,\n","        'predicted_label': y_pred,\n","        'confidence': confidences,\n","        'uncertainty_score': uncertainty_scores,\n","        'correct': correct_mask,\n","        'nova_0_prob': calibrated_probs[:, 0],\n","        'nova_1_prob': calibrated_probs[:, 1],\n","        'nova_2_prob': calibrated_probs[:, 2],\n","        'nova_3_prob': calibrated_probs[:, 3],\n","        'nova_4_prob': calibrated_probs[:, 4],\n","        'flagged_by_val_threshold': uncertainty_scores > val_uncertainty_threshold\n","    })\n","\n","    # Save results\n","    test_results.to_csv('TEST_SET_FINAL_RESULTS.csv', index=False)\n","    print(f\"✓ Detailed results saved to 'TEST_SET_FINAL_RESULTS.csv'\")\n","\n","    # -------------------------------------------------------------------------\n","    # 10. PERFORMANCE COMPARISON WITH VALIDATION\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[COMPARISON] Comparing with validation performance...\")\n","\n","    val_accuracy = calibration_params['validation_accuracy']\n","    val_ece_after = calibration_params['ece_after']\n","\n","    print(f\"  Validation accuracy: {val_accuracy:.4f}\")\n","    print(f\"  Test accuracy: {accuracy:.4f}\")\n","    print(f\"  Accuracy difference: {accuracy - val_accuracy:.4f}\")\n","\n","    print(f\"  Validation ECE (calibrated): {val_ece_after:.4f}\")\n","    print(f\"  Test ECE (calibrated): {ece_calibrated:.4f}\")\n","    print(f\"  ECE difference: {ece_calibrated - val_ece_after:.4f}\")\n","\n","    if abs(accuracy - val_accuracy) < 0.02:\n","        print(f\"  ✅ Performance is consistent (difference < 2%)\")\n","    else:\n","        print(f\"  ⚠️  Performance difference > 2% - investigate further\")\n","\n","    # -------------------------------------------------------------------------\n","    # 11. VISUALIZATIONS\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[VISUALIZATION] Creating test set diagnostic plots...\")\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(all_labels, y_pred)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=[f'NOVA {i}' for i in range(5)],\n","                yticklabels=[f'NOVA {i}' for i in range(5)])\n","    plt.title('Test Set Confusion Matrix', fontsize=16)\n","    plt.xlabel('Predicted', fontsize=14)\n","    plt.ylabel('True', fontsize=14)\n","    plt.tight_layout()\n","    plt.savefig('test_set_confusion_matrix.png', dpi=300, bbox_inches='tight')\n","    plt.close()\n","\n","    # Performance comparison plot\n","    plt.figure(figsize=(12, 5))\n","\n","    # Plot 1: Accuracy comparison\n","    plt.subplot(1, 2, 1)\n","    metrics = ['Validation', 'Test']\n","    accuracies = [val_accuracy, accuracy]\n","    bars = plt.bar(metrics, accuracies, color=['skyblue', 'lightcoral'])\n","    plt.ylabel('Accuracy')\n","    plt.title('Validation vs Test Accuracy')\n","    plt.ylim(0.9, 1.0)\n","\n","    # Add value labels\n","    for bar, acc in zip(bars, accuracies):\n","        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n","                f'{acc:.4f}', ha='center', va='bottom', fontsize=12)\n","\n","    # Plot 2: ECE comparison\n","    plt.subplot(1, 2, 2)\n","    eces = [val_ece_after, ece_calibrated]\n","    bars = plt.bar(metrics, eces, color=['lightgreen', 'orange'])\n","    plt.ylabel('Expected Calibration Error (ECE)')\n","    plt.title('Validation vs Test Calibration')\n","\n","    # Add value labels\n","    for bar, ece in zip(bars, eces):\n","        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0005,\n","                f'{ece:.4f}', ha='center', va='bottom', fontsize=12)\n","\n","    plt.tight_layout()\n","    plt.savefig('validation_vs_test_performance.png', dpi=300, bbox_inches='tight')\n","    plt.close()\n","\n","    print(f\"✓ Plots saved: 'test_set_confusion_matrix.png', 'validation_vs_test_performance.png'\")\n","\n","    # -------------------------------------------------------------------------\n","    # 12. COMPREHENSIVE ERROR ANALYSIS\n","    # -------------------------------------------------------------------------\n","    analyze_test_errors(test_results)\n","\n","    # -------------------------------------------------------------------------\n","    # 13. SAVE FINAL SUMMARY\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n[SAVE] Saving final performance summary...\")\n","\n","    final_summary = {\n","        'test_set_size': len(test_df),\n","        'test_accuracy': float(accuracy),\n","        'test_f1_weighted': float(f1),\n","        'test_precision_weighted': float(precision),\n","        'test_recall_weighted': float(recall),\n","        'test_ece_raw': float(ece_raw),\n","        'test_ece_calibrated': float(ece_calibrated),\n","        'test_uncertainty_mean': float(uncertainty_scores.mean()),\n","        'test_uncertainty_95th': float(np.percentile(uncertainty_scores, 95)),\n","        'per_class_f1': {f'nova_{i}': float(f1_per_class[i]) for i in range(len(f1_per_class))},\n","        'validation_comparison': {\n","            'val_accuracy': val_accuracy,\n","            'test_accuracy': float(accuracy),\n","            'accuracy_difference': float(accuracy - val_accuracy),\n","            'val_ece_calibrated': val_ece_after,\n","            'test_ece_calibrated': float(ece_calibrated),\n","            'ece_difference': float(ece_calibrated - val_ece_after)\n","        },\n","        'calibration_settings': {\n","            'temperature': temperature,\n","            'validation_threshold_95': val_uncertainty_threshold,\n","            'test_threshold_95': float(test_threshold_95)\n","        },\n","        'error_summary': {\n","            'total_errors': int((~correct_mask).sum()),\n","            'error_rate': float((~correct_mask).mean()),\n","            'high_confidence_errors': int((test_results['confidence'] > 0.95)[~correct_mask].sum())\n","        }\n","    }\n","\n","    with open('TEST_SET_FINAL_SUMMARY.json', 'w') as f:\n","        json.dump(final_summary, f, indent=2)\n","\n","    print(f\"✓ Final summary saved to 'TEST_SET_FINAL_SUMMARY.json'\")\n","\n","    # -------------------------------------------------------------------------\n","    # 14. FINAL SUMMARY\n","    # -------------------------------------------------------------------------\n","    print(f\"\\n\" + \"=\"*70)\n","    print(\"TEST SET EVALUATION COMPLETE - FINAL SUMMARY\")\n","    print(\"=\"*70)\n","    print(f\"Test Set Size: {len(test_df)} samples\")\n","    print(f\"Final Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n","    print(f\"Final Test F1 (Weighted): {f1:.4f}\")\n","    print(f\"Final Test ECE (Calibrated): {ece_calibrated:.4f}\")\n","    print(f\"Temperature Scaling Factor: {temperature:.3f}\")\n","    print(f\"Uncertainty Threshold (95%): {test_threshold_95:.4f}\")\n","    print(f\"Total Errors: {(~correct_mask).sum()}/{len(test_df)} ({(~correct_mask).mean()*100:.1f}%)\")\n","\n","    print(f\"\\nValidation vs Test Comparison:\")\n","    print(f\"  Accuracy difference: {accuracy - val_accuracy:+.4f}\")\n","    print(f\"  ECE difference: {ece_calibrated - val_ece_after:+.4f}\")\n","\n","    if abs(accuracy - val_accuracy) < 0.02 and abs(ece_calibrated - val_ece_after) < 0.01:\n","        print(f\"\\n✅ MODEL PERFORMANCE IS STABLE AND RELIABLE\")\n","        print(f\"✅ READY FOR PRODUCTION PIPELINE ON 900K EPF RECORDS\")\n","    else:\n","        print(f\"\\n⚠️  Performance differences detected - review before production\")\n","\n","    print(\"=\"*70)\n","\n","    return test_results, final_summary\n","\n","# =============================================================================\n","# MAIN EXECUTION\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    # Run comprehensive test set evaluation\n","    test_results, summary = evaluate_test_set()\n","\n","    print(f\"\\n✅ Test set evaluation complete!\")\n","    print(f\"\\nFiles generated:\")\n","    print(f\"  1. TEST_SET_FINAL_RESULTS.csv - Detailed predictions\")\n","    print(f\"  2. TEST_SET_FINAL_SUMMARY.json - Performance summary\")\n","    print(f\"  3. test_set_confusion_matrix.png - Confusion matrix\")\n","    print(f\"  4. validation_vs_test_performance.png - Performance comparison\")\n","\n","    print(f\"\\nNext steps:\")\n","    print(f\"  1. Review test set results and error analysis\")\n","    print(f\"  2. Document final performance in thesis\")\n","    print(f\"  3. If performance is stable, proceed to production pipeline\")\n","    print(f\"  4. Apply model to 900K EPF records with uncertainty flagging\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JUwSes8v2jxi","executionInfo":{"status":"ok","timestamp":1753619316221,"user_tz":-60,"elapsed":4540,"user":{"displayName":"Alfredo Heufemann Peña","userId":"01230962574111998829"}},"outputId":"4559d263-5755-47e0-d87a-60a0f229dda8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","TEST SET EVALUATION - FINAL MODEL PERFORMANCE\n","======================================================================\n","\n","[SETUP] Using device: cuda\n","\n","[DATA] Loading test set...\n","✓ Loaded 900 test samples\n","\n","[DATA] Test set class distribution:\n","  NOVA 0: 7 samples (0.8%)\n","  NOVA 1: 187 samples (20.8%)\n","  NOVA 2: 37 samples (4.1%)\n","  NOVA 3: 146 samples (16.2%)\n","  NOVA 4: 523 samples (58.1%)\n","\n","[MODEL] Loading trained model and calibration parameters...\n","✓ Model loaded\n","✓ Calibration parameters loaded:\n","  Temperature: 1.466\n","  Validation uncertainty threshold (95%): 0.0150\n","\n","[PREDICTION] Running inference on test set...\n","✓ Predictions complete\n","\n","[CALIBRATION] Applying temperature scaling...\n","✓ Temperature scaling applied\n","\n","[EVALUATION] Calculating performance metrics...\n","\n","✓ TEST SET PERFORMANCE:\n","  Accuracy: 0.9544\n","  Weighted F1: 0.9546\n","  Weighted Precision: 0.9550\n","  Weighted Recall: 0.9544\n","\n","✓ PER-CLASS F1 SCORES:\n","  NOVA 0: 0.9333\n","  NOVA 1: 0.9396\n","  NOVA 2: 1.0000\n","  NOVA 3: 0.9320\n","  NOVA 4: 0.9633\n","\n","[CALIBRATION] Analyzing calibration on test set...\n","  ECE (before calibration): 0.0359\n","  ECE (after calibration): 0.0170\n","  Calibration improvement: 0.0189\n","\n","[UNCERTAINTY] Analyzing prediction uncertainty...\n","  Mean uncertainty: 0.0314\n","  Median uncertainty: 0.0186\n","  95th percentile: 0.0631\n","  99th percentile: 0.5371\n","\n","  Uncertainty by correctness:\n","    Correct predictions: 0.0236\n","    Incorrect predictions: 0.1953\n","\n","  Threshold validation:\n","    Validation threshold (95%): 0.0150\n","    Test set threshold (95%): 0.0631\n","    Items flagged by val threshold: 694 (77.1%)\n","\n","[EVALUATION] Detailed classification report:\n","\n","              precision    recall  f1-score   support\n","\n","      NOVA 0     0.8750    1.0000    0.9333         7\n","      NOVA 1     0.9227    0.9572    0.9396       187\n","      NOVA 2     1.0000    1.0000    1.0000        37\n","      NOVA 3     0.9257    0.9384    0.9320       146\n","      NOVA 4     0.9727    0.9541    0.9633       523\n","\n","    accuracy                         0.9544       900\n","   macro avg     0.9392    0.9699    0.9537       900\n","weighted avg     0.9550    0.9544    0.9546       900\n","\n","\n","[SAVE] Saving detailed test results...\n","✓ Detailed results saved to 'TEST_SET_FINAL_RESULTS.csv'\n","\n","[COMPARISON] Comparing with validation performance...\n","  Validation accuracy: 0.9633\n","  Test accuracy: 0.9544\n","  Accuracy difference: -0.0088\n","  Validation ECE (calibrated): 0.0095\n","  Test ECE (calibrated): 0.0170\n","  ECE difference: 0.0075\n","  ✅ Performance is consistent (difference < 2%)\n","\n","[VISUALIZATION] Creating test set diagnostic plots...\n","✓ Plots saved: 'test_set_confusion_matrix.png', 'validation_vs_test_performance.png'\n","\n","======================================================================\n","COMPREHENSIVE TEST SET ERROR ANALYSIS\n","======================================================================\n","Total samples: 900\n","Total correct: 859 (95.4%)\n","Total errors: 41 (4.6%)\n","\n","======================================================================\n","NOVA 0 ERROR ANALYSIS\n","======================================================================\n","Total NOVA 0 errors: 0\n","NOVA 0 total items: 7\n","Error rate: 0.0%\n","\n","======================================================================\n","NOVA 1 ERROR ANALYSIS\n","======================================================================\n","Total NOVA 1 errors: 8\n","NOVA 1 total items: 187\n","Error rate: 4.3%\n","\n","NOVA 1 error breakdown:\n","  NOVA 1 → NOVA 4: 8 errors\n","\n","All NOVA 1 errors:\n"," 1. ARANDANOS | FERIA LIBRE\n","    Confidence: 50.9% → NOVA 4\n"," 2. CAFE+PROPINA | CAFETERIA CHOSE Y COFFEE\n","    Confidence: 97.5% → NOVA 4\n"," 3. LECHE DE VACA SEMI DESCREMADA | JUMBO\n","    Confidence: 92.0% → NOVA 4\n"," 4. LECHE SEMI DESCREMADA COLUN | SANTA ISABEL\n","    Confidence: 98.1% → NOVA 4\n"," 5. PASTA CHOCLO CONGELADA | LIDER\n","    Confidence: 95.0% → NOVA 4\n"," 6. LECHE LIQUIDA EN CAJA BLANCA | SUPERMERCADO ACUENTA\n","    Confidence: 97.8% → NOVA 4\n"," 7. CAFES Y JUGOS LISTO PARA SERVIR | CAFETERIA MOSQUETO\n","    Confidence: 93.1% → NOVA 4\n"," 8. MANZANA | SUP UNIMARC\n","    Confidence: 53.6% → NOVA 4\n","\n","======================================================================\n","NOVA 2 ERROR ANALYSIS\n","======================================================================\n","Total NOVA 2 errors: 0\n","NOVA 2 total items: 37\n","Error rate: 0.0%\n","\n","======================================================================\n","NOVA 3 ERROR ANALYSIS\n","======================================================================\n","Total NOVA 3 errors: 9\n","NOVA 3 total items: 146\n","Error rate: 6.2%\n","\n","NOVA 3 error breakdown:\n","  NOVA 3 → NOVA 4: 6 errors\n","  NOVA 3 → NOVA 1: 2 errors\n","  NOVA 3 → NOVA 0: 1 errors\n","\n","All NOVA 3 errors:\n"," 1. COMIDA LICENCIATURA CON PROPINA | RESTAURANTE\n","    Confidence: 96.2% → NOVA 0\n"," 2. AGREGADO PALTA | FUENTE SODA\n","    Confidence: 97.9% → NOVA 4\n"," 3. PAPAS FRITAS | RESTORANT EL KIKA\"\n","    Confidence: 97.9% → NOVA 4\n"," 4. QUEQUE TRADICIONAL | PARTICULAR\n","    Confidence: 95.6% → NOVA 4\n"," 5. QUESO A GRANEL | ALMACEN\n","    Confidence: 92.3% → NOVA 1\n"," 6. ONCE | CAFETERIA ANAYAK\n","    Confidence: 92.5% → NOVA 4\n"," 7. ALCACHOFAS CON JAMON SERRANO | RESTAURANTE\n","    Confidence: 97.8% → NOVA 4\n"," 8. PCH. ITALIANO, CHURRASCO PROMOCION | COMIDA RAPIDA\n","    Confidence: 93.8% → NOVA 4\n"," 9. PAN DOBLABA FRESCO CORRIENTE GRANEL | UNIMARC\n","    Confidence: 51.5% → NOVA 1\n","\n","======================================================================\n","NOVA 4 ERROR ANALYSIS\n","======================================================================\n","Total NOVA 4 errors: 24\n","NOVA 4 total items: 523\n","Error rate: 4.6%\n","\n","NOVA 4 error breakdown:\n","  NOVA 4 → NOVA 1: 13 errors\n","  NOVA 4 → NOVA 3: 11 errors\n","\n","Top 10 NOVA 4 errors (highest confidence):\n"," 1. LONGANIZA DE CERDO | ALMACEN\n","    Confidence: 98.4% → NOVA 1\n"," 2. STARBUCKS DE CAFE MEDIANO | STARBUCKS\n","    Confidence: 98.1% → NOVA 1\n"," 3. PESTO GENOVERSE | TOTTUS\n","    Confidence: 98.1% → NOVA 1\n"," 4. EMPANADAS (4 NAPOLITANA, 4 PINO) LISTO PARA SERVIR | LOCAL DE EMPANADAS CASA DE LA EMPANADA\n","    Confidence: 98.0% → NOVA 3\n"," 5. ALMUERZO | CASINO\n","    Confidence: 98.0% → NOVA 3\n"," 6. SUSHI QUESO CREMA + POLLO | RESTAURANTE KIRU\n","    Confidence: 97.9% → NOVA 3\n"," 7. CHURRASCO + BEBIDA LISTO PARA SERVIR | LOCAL COMIDA RAPIDO\n","    Confidence: 96.1% → NOVA 3\n"," 8. JUGOS DE FRUTA NATURAL | MINIMARKET\n","    Confidence: 95.2% → NOVA 1\n"," 9. DESAYUNO ( CAFE + SELLADITO) | JUAN MAESTRO\n","    Confidence: 95.1% → NOVA 3\n","10. JARABE DE AGAVE (ENDULZANTE NATURAL) | TIENDA ESPECIALIZADA\n","    Confidence: 93.0% → NOVA 1\n","\n","======================================================================\n","CONFUSION MATRIX ANALYSIS\n","======================================================================\n","Confusion Matrix:\n","             Pred NOVA 0  Pred NOVA 1  Pred NOVA 2  Pred NOVA 3  Pred NOVA 4\n","True NOVA 0            7            0            0            0            0\n","True NOVA 1            0          179            0            0            8\n","True NOVA 2            0            0           37            0            0\n","True NOVA 3            1            2            0          137            6\n","True NOVA 4            0           13            0           11          499\n","\n","Most problematic confusions (off-diagonal):\n","  NOVA 1 → NOVA 4: 8 errors\n","  NOVA 3 → NOVA 0: 1 errors\n","  NOVA 3 → NOVA 1: 2 errors\n","  NOVA 3 → NOVA 4: 6 errors\n","  NOVA 4 → NOVA 1: 13 errors\n","  NOVA 4 → NOVA 3: 11 errors\n","\n","======================================================================\n","HIGH CONFIDENCE ERRORS SUMMARY (>95%)\n","======================================================================\n","Total high confidence errors: 17\n","\n","High confidence errors by type:\n","  NOVA 1 → NOVA 4: 3 errors\n","  NOVA 3 → NOVA 0: 1 errors\n","  NOVA 3 → NOVA 4: 4 errors\n","  NOVA 4 → NOVA 1: 4 errors\n","  NOVA 4 → NOVA 3: 5 errors\n","\n","Top 10 high confidence errors (systematic mistakes):\n"," 1. LONGANIZA DE CERDO | ALMACEN\n","    NOVA 4 → NOVA 1 (Confidence: 98.4%)\n"," 2. STARBUCKS DE CAFE MEDIANO | STARBUCKS\n","    NOVA 4 → NOVA 1 (Confidence: 98.1%)\n"," 3. LECHE SEMI DESCREMADA COLUN | SANTA ISABEL\n","    NOVA 1 → NOVA 4 (Confidence: 98.1%)\n"," 4. PESTO GENOVERSE | TOTTUS\n","    NOVA 4 → NOVA 1 (Confidence: 98.1%)\n"," 5. EMPANADAS (4 NAPOLITANA, 4 PINO) LISTO PARA SERVIR | LOCAL DE EMPANADAS CASA DE LA EMPANADA\n","    NOVA 4 → NOVA 3 (Confidence: 98.0%)\n"," 6. ALMUERZO | CASINO\n","    NOVA 4 → NOVA 3 (Confidence: 98.0%)\n"," 7. SUSHI QUESO CREMA + POLLO | RESTAURANTE KIRU\n","    NOVA 4 → NOVA 3 (Confidence: 97.9%)\n"," 8. PAPAS FRITAS | RESTORANT EL KIKA\"\n","    NOVA 3 → NOVA 4 (Confidence: 97.9%)\n"," 9. AGREGADO PALTA | FUENTE SODA\n","    NOVA 3 → NOVA 4 (Confidence: 97.9%)\n","10. LECHE LIQUIDA EN CAJA BLANCA | SUPERMERCADO ACUENTA\n","    NOVA 1 → NOVA 4 (Confidence: 97.8%)\n","\n","[SAVE] Saving final performance summary...\n","✓ Final summary saved to 'TEST_SET_FINAL_SUMMARY.json'\n","\n","======================================================================\n","TEST SET EVALUATION COMPLETE - FINAL SUMMARY\n","======================================================================\n","Test Set Size: 900 samples\n","Final Test Accuracy: 0.9544 (95.44%)\n","Final Test F1 (Weighted): 0.9546\n","Final Test ECE (Calibrated): 0.0170\n","Temperature Scaling Factor: 1.466\n","Uncertainty Threshold (95%): 0.0631\n","Total Errors: 41/900 (4.6%)\n","\n","Validation vs Test Comparison:\n","  Accuracy difference: -0.0088\n","  ECE difference: +0.0075\n","\n","✅ MODEL PERFORMANCE IS STABLE AND RELIABLE\n","✅ READY FOR PRODUCTION PIPELINE ON 900K EPF RECORDS\n","======================================================================\n","\n","✅ Test set evaluation complete!\n","\n","Files generated:\n","  1. TEST_SET_FINAL_RESULTS.csv - Detailed predictions\n","  2. TEST_SET_FINAL_SUMMARY.json - Performance summary\n","  3. test_set_confusion_matrix.png - Confusion matrix\n","  4. validation_vs_test_performance.png - Performance comparison\n","\n","Next steps:\n","  1. Review test set results and error analysis\n","  2. Document final performance in thesis\n","  3. If performance is stable, proceed to production pipeline\n","  4. Apply model to 900K EPF records with uncertainty flagging\n"]}]}]}
