{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING FOR DUPLICATES IN ORIGINAL DATA ===\n",
      "Original training data: 5000 records\n",
      "Unique id_gasto in training: 5000\n",
      "\n",
      "=== EXTRACTING AUGMENTATION SAMPLES ===\n",
      "\n",
      "Existing id_gasto values in training data: 5000\n",
      "Full dataset after removing existing id_gastos: 954626 records\n",
      "\n",
      "NOVA 1 - Extracting 700 samples...\n",
      "Found 76783 total NOVA 1 candidates (excluding existing)\n",
      "  01.1.4.08.01 (HUEVOS DE GALLINA, FRESCOS Y CON CÁSCARA): 16559 records\n",
      "  01.1.6.02.02 (NARANJAS FRESCAS): 4340 records\n",
      "  01.1.7.02.02 (ZAPALLOS FRESCOS O REFRIGERADOS): 7921 records\n",
      "  01.1.7.02.07 (TOMATES FRESCOS O REFRIGERADOS): 20054 records\n",
      "  01.1.7.04.05 (CEBOLLAS FRESCAS O REFRIGERADAS): 12402 records\n",
      "  01.1.7.05.01 (PAPAS Y OTROS TUBÉRCULOS): 15507 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gr1kzxzs5nj68r2ry0ykrmzc0000gp/T/ipykernel_22682/1824994998.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  nova1_sample = nova1_data.groupby('ccif', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOVA 2 - Extracting 200 samples...\n",
      "Found 8203 total NOVA 2 candidates (excluding existing)\n",
      "  01.1.5.01.01 (ACEITES VEGETALES, DE FRUTA O SEMILLAS): 7601 records\n",
      "  01.1.9.03.07 (VINAGRES): 602 records\n",
      "\n",
      "NOVA 3 - Extracting 100 samples...\n",
      "Found 119770 total NOVA 3 candidates (excluding existing)\n",
      "  01.1.1.03.01 (PAN CORRIENTE A GRANEL): 113433 records\n",
      "  01.1.3.03.02 (ATÚN EN CONSERVA): 4792 records\n",
      "  01.1.3.03.03 (JUREL EN CONSERVA): 1545 records\n",
      "\n",
      "=== AUGMENTATION SUMMARY ===\n",
      "Total new samples: 1000\n",
      "\n",
      "Distribution by NOVA:\n",
      "NOVA\n",
      "1    700\n",
      "2    200\n",
      "3    100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== SAMPLE OF AUGMENTATION DATA ===\n",
      "\n",
      "NOVA 1 examples:\n",
      "  01.1.7.05.01: PAPAS FRESCAS EN MALLA | LIDER (id: 12242-1-65)\n",
      "  01.1.6.02.02: NARANJAS FRESCAS | FERIA (id: 10229-1-54)\n",
      "  01.1.7.02.07: TOMATES FRESCOS | FERIA (id: 9035-1-97)\n",
      "\n",
      "NOVA 2 examples:\n",
      "  01.1.5.01.01: ACEITE VEG. CAMPO | SUPERMERCADO AHORRA MAS (id: 13694-1-23)\n",
      "  01.1.5.01.01: ACEITE VEGETAL | FERIA (id: 13844-1-16)\n",
      "  01.1.5.01.01: ACEITE 04 MARAVILLA | FERIA ITINERANTE (id: 8134-1-4)\n",
      "\n",
      "NOVA 3 examples:\n",
      "  01.1.3.03.02: ATUN EN ACEITE | TOTTUS (id: 982-1-22)\n",
      "  01.1.1.03.01: PAN MARRAQUETA GRANEL | ALMACEN EMELY (id: 12831-1-64)\n",
      "  01.1.3.03.02: ATUN EN AGUA EN CONSERVA 3 TARROS | ALMACEN TRADICIONAL (id: 2856-1-113)\n",
      "\n",
      "Saved augmentation to 'augmentation_1000_samples.csv'\n",
      "\n",
      "=== COMBINING WITH TRAINING DATA ===\n",
      "\n",
      "=== FINAL DUPLICATE CHECK ===\n",
      "✓ No duplicate id_gasto values found!\n",
      "\n",
      "Original training: 5000 samples\n",
      "Augmentation: 1000 samples\n",
      "Combined total: 6000 samples\n",
      "Unique id_gasto values: 6000\n",
      "\n",
      "=== FINAL DISTRIBUTION ===\n",
      "NOVA 0: 43 samples (0.7%)\n",
      "NOVA 1: 1249 samples (20.8%)\n",
      "NOVA 2: 243 samples (4.0%)\n",
      "NOVA 3: 978 samples (16.3%)\n",
      "NOVA 4: 3487 samples (58.1%)\n",
      "\n",
      "Saved final training dataset to 'training_dataset_6000.csv'\n",
      "\n",
      "✓ Summary saved to 'augmentation_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gr1kzxzs5nj68r2ry0ykrmzc0000gp/T/ipykernel_22682/1824994998.py:110: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  nova3_sample = nova3_data.groupby('ccif', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Augment dataset using YOUR specific CCIF codes and NOVA assignments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "df_train = pd.read_excel('beto_training_sample_5000_alfredo.xlsx')\n",
    "df_full = pd.read_parquet('epf_nova_merged.parquet')\n",
    "\n",
    "print(\"=== CHECKING FOR DUPLICATES IN ORIGINAL DATA ===\")\n",
    "print(f\"Original training data: {len(df_train)} records\")\n",
    "print(f\"Unique id_gasto in training: {df_train['id_gasto'].nunique()}\")\n",
    "\n",
    "# Check if training data already has duplicates\n",
    "train_duplicates = df_train[df_train.duplicated(subset=['id_gasto'], keep=False)]\n",
    "if len(train_duplicates) > 0:\n",
    "    print(f\"⚠️  WARNING: Training data has {len(train_duplicates)} duplicate id_gasto values!\")\n",
    "    print(train_duplicates[['id_gasto', 'descripcion', 'NOVA']].head())\n",
    "\n",
    "print(\"\\n=== EXTRACTING AUGMENTATION SAMPLES ===\")\n",
    "\n",
    "# YOUR SPECIFIC MAPPINGS\n",
    "# NOVA 1 - Fresh produce, eggs\n",
    "nova1_mapping = {\n",
    "    '01.1.4.08.01': 'HUEVOS DE GALLINA, FRESCOS Y CON CÁSCARA',\n",
    "    '01.1.6.02.02': 'NARANJAS FRESCAS',\n",
    "    '01.1.7.02.02': 'ZAPALLOS FRESCOS O REFRIGERADOS',\n",
    "    '01.1.7.02.07': 'TOMATES FRESCOS O REFRIGERADOS',\n",
    "    '01.1.7.04.05': 'CEBOLLAS FRESCAS O REFRIGERADAS',\n",
    "    '01.1.7.05.01': 'PAPAS Y OTROS TUBÉRCULOS'\n",
    "}\n",
    "\n",
    "# NOVA 2 - Basic ingredients\n",
    "nova2_mapping = {\n",
    "    '01.1.5.01.01': 'ACEITES VEGETALES, DE FRUTA O SEMILLAS',\n",
    "    '01.1.9.03.07': 'VINAGRES'\n",
    "}\n",
    "\n",
    "# NOVA 3 - Processed foods\n",
    "nova3_mapping = {\n",
    "    '01.1.1.03.01': 'PAN CORRIENTE A GRANEL',\n",
    "    '01.1.3.03.02': 'ATÚN EN CONSERVA',\n",
    "    '01.1.3.03.03': 'JUREL EN CONSERVA'\n",
    "}\n",
    "\n",
    "# IMPORTANT: Get all id_gasto values from training data to avoid duplicates\n",
    "existing_id_gastos = set(df_train['id_gasto'].values)\n",
    "print(f\"\\nExisting id_gasto values in training data: {len(existing_id_gastos)}\")\n",
    "\n",
    "# Filter out any records from full dataset that already exist in training\n",
    "df_full_filtered = df_full[~df_full['id_gasto'].isin(existing_id_gastos)].copy()\n",
    "print(f\"Full dataset after removing existing id_gastos: {len(df_full_filtered)} records\")\n",
    "\n",
    "# Extract samples for each NOVA category\n",
    "augmentation_samples = []\n",
    "\n",
    "# NOVA 1 - Extract 700 samples\n",
    "print(\"\\nNOVA 1 - Extracting 700 samples...\")\n",
    "nova1_data = df_full_filtered[df_full_filtered['ccif'].isin(nova1_mapping.keys())].copy()\n",
    "print(f\"Found {len(nova1_data)} total NOVA 1 candidates (excluding existing)\")\n",
    "\n",
    "# Show distribution by CCIF\n",
    "for ccif, glosa in nova1_mapping.items():\n",
    "    count = len(nova1_data[nova1_data['ccif'] == ccif])\n",
    "    print(f\"  {ccif} ({glosa}): {count} records\")\n",
    "\n",
    "# Sample 700\n",
    "if len(nova1_data) >= 700:\n",
    "    # Stratified sampling to get variety\n",
    "    nova1_sample = nova1_data.groupby('ccif', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), 120), random_state=42)  # ~120 per category\n",
    "    ).sample(700, random_state=42)\n",
    "else:\n",
    "    nova1_sample = nova1_data\n",
    "    print(f\"  ⚠️  Only {len(nova1_sample)} samples available for NOVA 1\")\n",
    "\n",
    "nova1_sample['NOVA'] = 1\n",
    "augmentation_samples.append(nova1_sample)\n",
    "\n",
    "# NOVA 2 - Extract 200 samples  \n",
    "print(\"\\nNOVA 2 - Extracting 200 samples...\")\n",
    "nova2_data = df_full_filtered[df_full_filtered['ccif'].isin(nova2_mapping.keys())].copy()\n",
    "print(f\"Found {len(nova2_data)} total NOVA 2 candidates (excluding existing)\")\n",
    "\n",
    "for ccif, glosa in nova2_mapping.items():\n",
    "    count = len(nova2_data[nova2_data['ccif'] == ccif])\n",
    "    print(f\"  {ccif} ({glosa}): {count} records\")\n",
    "\n",
    "# Sample 200\n",
    "if len(nova2_data) >= 200:\n",
    "    nova2_sample = nova2_data.sample(200, random_state=42)\n",
    "else:\n",
    "    nova2_sample = nova2_data\n",
    "    print(f\"  ⚠️  Only {len(nova2_sample)} samples available for NOVA 2\")\n",
    "    \n",
    "nova2_sample['NOVA'] = 2\n",
    "augmentation_samples.append(nova2_sample)\n",
    "\n",
    "# NOVA 3 - Extract 100 samples\n",
    "print(\"\\nNOVA 3 - Extracting 100 samples...\")\n",
    "nova3_data = df_full_filtered[df_full_filtered['ccif'].isin(nova3_mapping.keys())].copy()\n",
    "print(f\"Found {len(nova3_data)} total NOVA 3 candidates (excluding existing)\")\n",
    "\n",
    "for ccif, glosa in nova3_mapping.items():\n",
    "    count = len(nova3_data[nova3_data['ccif'] == ccif])\n",
    "    print(f\"  {ccif} ({glosa}): {count} records\")\n",
    "\n",
    "# Sample 100\n",
    "if len(nova3_data) >= 100:\n",
    "    # Get variety across the 3 types\n",
    "    nova3_sample = nova3_data.groupby('ccif', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), 34), random_state=42)  # ~33 per category\n",
    "    ).sample(100, random_state=42)\n",
    "else:\n",
    "    nova3_sample = nova3_data\n",
    "    print(f\"  ⚠️  Only {len(nova3_sample)} samples available for NOVA 3\")\n",
    "\n",
    "nova3_sample['NOVA'] = 3\n",
    "augmentation_samples.append(nova3_sample)\n",
    "\n",
    "# Combine all augmentation\n",
    "df_augment = pd.concat(augmentation_samples, ignore_index=True)\n",
    "\n",
    "# VERIFY NO DUPLICATES IN AUGMENTATION\n",
    "augment_duplicates = df_augment[df_augment.duplicated(subset=['id_gasto'], keep=False)]\n",
    "if len(augment_duplicates) > 0:\n",
    "    print(f\"\\n⚠️  WARNING: Augmentation has {len(augment_duplicates)} duplicate id_gasto values!\")\n",
    "    # Remove duplicates, keeping first occurrence\n",
    "    df_augment = df_augment.drop_duplicates(subset=['id_gasto'], keep='first')\n",
    "    print(f\"Removed duplicates. New augmentation size: {len(df_augment)}\")\n",
    "\n",
    "print(f\"\\n=== AUGMENTATION SUMMARY ===\")\n",
    "print(f\"Total new samples: {len(df_augment)}\")\n",
    "print(\"\\nDistribution by NOVA:\")\n",
    "print(df_augment['NOVA'].value_counts().sort_index())\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n=== SAMPLE OF AUGMENTATION DATA ===\")\n",
    "for nova in [1, 2, 3]:\n",
    "    nova_samples = df_augment[df_augment['NOVA'] == nova]\n",
    "    if len(nova_samples) > 0:\n",
    "        print(f\"\\nNOVA {nova} examples:\")\n",
    "        examples = nova_samples.sample(min(3, len(nova_samples)))\n",
    "        for _, row in examples.iterrows():\n",
    "            print(f\"  {row['ccif']}: {row['descripcion_gasto']} | {row['establecimiento']} (id: {row['id_gasto']})\")\n",
    "\n",
    "# Prepare for merging - match column names with training data\n",
    "df_augment_final = pd.DataFrame({\n",
    "    'id_gasto': df_augment['id_gasto'],\n",
    "    'folio': df_augment['folio'],\n",
    "    'descripcion': df_augment['descripcion_gasto'],\n",
    "    'establecimiento': df_augment['establecimiento'],\n",
    "    'ccif': df_augment['ccif'],\n",
    "    'NOVA': df_augment['NOVA'].astype(int),\n",
    "    'glosa': df_augment['glosa_ccif']\n",
    "})\n",
    "\n",
    "# Save augmentation samples\n",
    "df_augment_final.to_csv('augmentation_1000_samples.csv', index=False)\n",
    "print(f\"\\nSaved augmentation to 'augmentation_1000_samples.csv'\")\n",
    "\n",
    "# Combine with original training data\n",
    "print(\"\\n=== COMBINING WITH TRAINING DATA ===\")\n",
    "\n",
    "# Select relevant columns from training data\n",
    "df_train_clean = df_train[['id_gasto','folio','descripcion', 'establecimiento', 'ccif', 'NOVA', 'glosa']].copy()\n",
    "\n",
    "# Combine\n",
    "df_combined = pd.concat([df_train_clean, df_augment_final], ignore_index=True)\n",
    "\n",
    "# FINAL VERIFICATION - Check for duplicates in combined dataset\n",
    "print(\"\\n=== FINAL DUPLICATE CHECK ===\")\n",
    "final_duplicates = df_combined[df_combined.duplicated(subset=['id_gasto'], keep=False)]\n",
    "if len(final_duplicates) > 0:\n",
    "    print(f\"⚠️  ERROR: Combined dataset has {len(final_duplicates)} duplicate id_gasto values!\")\n",
    "    print(\"Sample duplicates:\")\n",
    "    print(final_duplicates[['id_gasto', 'descripcion', 'NOVA']].head(10))\n",
    "    \n",
    "    # Remove duplicates, keeping the one from training data (first occurrence)\n",
    "    df_combined = df_combined.drop_duplicates(subset=['id_gasto'], keep='first')\n",
    "    print(f\"\\nRemoved duplicates. Final size: {len(df_combined)}\")\n",
    "else:\n",
    "    print(\"✓ No duplicate id_gasto values found!\")\n",
    "\n",
    "print(f\"\\nOriginal training: {len(df_train_clean)} samples\")\n",
    "print(f\"Augmentation: {len(df_augment_final)} samples\")\n",
    "print(f\"Combined total: {len(df_combined)} samples\")\n",
    "print(f\"Unique id_gasto values: {df_combined['id_gasto'].nunique()}\")\n",
    "\n",
    "# Final distribution\n",
    "print(\"\\n=== FINAL DISTRIBUTION ===\")\n",
    "final_dist = df_combined['NOVA'].value_counts().sort_index()\n",
    "for nova, count in final_dist.items():\n",
    "    print(f\"NOVA {nova}: {count} samples ({count/len(df_combined)*100:.1f}%)\")\n",
    "\n",
    "# Save final training dataset\n",
    "df_combined.to_csv('training_dataset_6000.csv', index=False)\n",
    "print(f\"\\nSaved final training dataset to 'training_dataset_6000.csv'\")\n",
    "\n",
    "# Save a summary of what we did\n",
    "with open('augmentation_summary.txt', 'w') as f:\n",
    "    f.write(\"AUGMENTATION SUMMARY\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    f.write(f\"Original training samples: {len(df_train_clean)}\\n\")\n",
    "    f.write(f\"Augmentation samples added: {len(df_augment_final)}\\n\")\n",
    "    f.write(f\"Final total samples: {len(df_combined)}\\n\")\n",
    "    f.write(f\"Unique id_gasto values: {df_combined['id_gasto'].nunique()}\\n\")\n",
    "    f.write(\"\\nFinal NOVA distribution:\\n\")\n",
    "    for nova, count in final_dist.items():\n",
    "        f.write(f\"  NOVA {nova}: {count} ({count/len(df_combined)*100:.1f}%)\\n\")\n",
    "\n",
    "print(\"\\n✓ Summary saved to 'augmentation_summary.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
